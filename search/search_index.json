{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"autodiff \u00b6 autodiff is a C++17 library that uses modern and advanced programming techniques to enable automatic computation of derivatives in an efficient, easy, and intuitive way. We welcome you to use autodiff and recommend us any improvements you think it is necessary. You may want to do so by chatting with us on our Gitter Community Channel and/or by making proposals by creating a GitHub issue . Attention There are breaking changes in autodiff v0.6! Please check the updated Tutorials page to learn how to correctly include the header files, the slightly changed API, and the new autodiff type real designed for efficient higher-order directional derivatives . This is in contrast to the dual type, which is designed for higher-order cross derivatives. Demonstration \u00b6 Consider the following function f(x, y, z) : double f ( double x , double y , double z ) { return ( x + y + z ) * exp ( x * y * z ); } which we use use to evaluate the variable u = f(x, y, z) : double x = 1.0 ; double y = 2.0 ; double z = 3.0 ; double u = f ( x , y , z ); How can we minimally transform this code so that not only u , but also its derivatives \u2202u/\u2202x , \u2202u/\u2202y , and \u2202u/\u2202z , can be computed? The next two sections present how this can be achieved using two automatic differentiation algorithms implemented in autodiff : forward mode and reverse mode . Forward mode \u00b6 In a forward mode automatic differentiation algorithm, both output variables and one or more of their derivatives are computed together. For example, the function evaluation f(x, y, z) can be transformed in a way that it will not only produce the value of u , the output variable , but also one or more of its derivatives (\u2202u/\u2202x, \u2202u/\u2202y, \u2202u/\u2202z) with respect to the input variables (x, y, z) . Enabling forward automatic differentiation for the calculation of derivatives using autodiff is relatively simple. For our previous function f , we only need to replace the floating-point type double with autodiff::dual for both input and output variables: dual f ( const dual & x , const dual & y , const dual & z ) { return ( x + y + z ) * exp ( x * y * z ); } We can now compute the derivatives \u2202u/\u2202x , \u2202u/\u2202y , and \u2202u/\u2202z as follows: dual x = 1.0 ; dual y = 2.0 ; dual z = 3.0 ; dual u = f ( x , y , z ); double ux = derivative ( f , wrt ( x ), at ( x , y , z )); double uy = derivative ( f , wrt ( y ), at ( x , y , z )); double uz = derivative ( f , wrt ( z ), at ( x , y , z )); The auxiliary function autodiff::wrt , an acronym for with respect to , is used to indicate which input variable (x, y, z) is the selected one to compute the partial derivative of f . The auxiliary function autodiff::at is used to indicate where (at which values of its parameters) the derivative of f is evaluated. Reverse mode \u00b6 In a reverse mode automatic differentiation algorithm, the output variable of a function is evaluated first. During this function evaluation, all mathematical operations between the input variables are \"recorded\" in an expression tree . By traversing this tree from top-level (output variable as the root node) to bottom-level (input variables as the leaf nodes), it is possible to compute the contribution of each branch on the derivatives of the output variable with respect to input variables. Thus, a single pass in a reverse mode calculation computes all derivatives , in contrast with forward mode, which requires one pass for each input variable. Note, however, that it is possible to change the behavior of a forward pass so that many (perhaps even all) derivatives of an output variable are computed simultaneously (e.g., in a single forward pass, \u2202u/\u2202x , \u2202u/\u2202y , and \u2202u/\u2202z are evaluated together with u , in contrast with three forward passes, each one computing the individual derivatives). Similar as before, we can use autodiff to enable reverse automatic differentiation for our function f by simply replacing type double with autodiff::var as follows: var f ( var x , var y , var z ) { return ( x + y + z ) * exp ( x * y * z ); } The code below demonstrates how the derivatives \u2202u/\u2202x , \u2202u/\u2202y , and \u2202u/\u2202z can be calculated: var x = 1.0 ; var y = 2.0 ; var z = 3.0 ; var u = f ( x , y , z ); auto [ ux , uy , uz ] = derivatives ( u , wrt ( x , y , z )); The function autodiff::derivatives will traverse the expression tree stored in variable u and compute all its derivatives with respect to the input variables (x, y, z) , which are then stored in the object dud . The derivative of u with respect to input variable x (i.e., \u2202u/\u2202x ) can then be extracted from dud using dud(x) . The operations dud(x) , dud(y) , dud(z) involve no computations! Just extraction of derivatives previously computed with a call to function autodiff::derivatives . Get in touch! \u00b6 Contact us on Gitter or via a GitHub Discussion if you need support and assistance when using autodiff . If you would like to report a bug, then please create a new GitHub Issue .","title":"Home"},{"location":"#autodiff","text":"autodiff is a C++17 library that uses modern and advanced programming techniques to enable automatic computation of derivatives in an efficient, easy, and intuitive way. We welcome you to use autodiff and recommend us any improvements you think it is necessary. You may want to do so by chatting with us on our Gitter Community Channel and/or by making proposals by creating a GitHub issue . Attention There are breaking changes in autodiff v0.6! Please check the updated Tutorials page to learn how to correctly include the header files, the slightly changed API, and the new autodiff type real designed for efficient higher-order directional derivatives . This is in contrast to the dual type, which is designed for higher-order cross derivatives.","title":"autodiff"},{"location":"#demonstration","text":"Consider the following function f(x, y, z) : double f ( double x , double y , double z ) { return ( x + y + z ) * exp ( x * y * z ); } which we use use to evaluate the variable u = f(x, y, z) : double x = 1.0 ; double y = 2.0 ; double z = 3.0 ; double u = f ( x , y , z ); How can we minimally transform this code so that not only u , but also its derivatives \u2202u/\u2202x , \u2202u/\u2202y , and \u2202u/\u2202z , can be computed? The next two sections present how this can be achieved using two automatic differentiation algorithms implemented in autodiff : forward mode and reverse mode .","title":"Demonstration"},{"location":"#forward-mode","text":"In a forward mode automatic differentiation algorithm, both output variables and one or more of their derivatives are computed together. For example, the function evaluation f(x, y, z) can be transformed in a way that it will not only produce the value of u , the output variable , but also one or more of its derivatives (\u2202u/\u2202x, \u2202u/\u2202y, \u2202u/\u2202z) with respect to the input variables (x, y, z) . Enabling forward automatic differentiation for the calculation of derivatives using autodiff is relatively simple. For our previous function f , we only need to replace the floating-point type double with autodiff::dual for both input and output variables: dual f ( const dual & x , const dual & y , const dual & z ) { return ( x + y + z ) * exp ( x * y * z ); } We can now compute the derivatives \u2202u/\u2202x , \u2202u/\u2202y , and \u2202u/\u2202z as follows: dual x = 1.0 ; dual y = 2.0 ; dual z = 3.0 ; dual u = f ( x , y , z ); double ux = derivative ( f , wrt ( x ), at ( x , y , z )); double uy = derivative ( f , wrt ( y ), at ( x , y , z )); double uz = derivative ( f , wrt ( z ), at ( x , y , z )); The auxiliary function autodiff::wrt , an acronym for with respect to , is used to indicate which input variable (x, y, z) is the selected one to compute the partial derivative of f . The auxiliary function autodiff::at is used to indicate where (at which values of its parameters) the derivative of f is evaluated.","title":"Forward mode"},{"location":"#reverse-mode","text":"In a reverse mode automatic differentiation algorithm, the output variable of a function is evaluated first. During this function evaluation, all mathematical operations between the input variables are \"recorded\" in an expression tree . By traversing this tree from top-level (output variable as the root node) to bottom-level (input variables as the leaf nodes), it is possible to compute the contribution of each branch on the derivatives of the output variable with respect to input variables. Thus, a single pass in a reverse mode calculation computes all derivatives , in contrast with forward mode, which requires one pass for each input variable. Note, however, that it is possible to change the behavior of a forward pass so that many (perhaps even all) derivatives of an output variable are computed simultaneously (e.g., in a single forward pass, \u2202u/\u2202x , \u2202u/\u2202y , and \u2202u/\u2202z are evaluated together with u , in contrast with three forward passes, each one computing the individual derivatives). Similar as before, we can use autodiff to enable reverse automatic differentiation for our function f by simply replacing type double with autodiff::var as follows: var f ( var x , var y , var z ) { return ( x + y + z ) * exp ( x * y * z ); } The code below demonstrates how the derivatives \u2202u/\u2202x , \u2202u/\u2202y , and \u2202u/\u2202z can be calculated: var x = 1.0 ; var y = 2.0 ; var z = 3.0 ; var u = f ( x , y , z ); auto [ ux , uy , uz ] = derivatives ( u , wrt ( x , y , z )); The function autodiff::derivatives will traverse the expression tree stored in variable u and compute all its derivatives with respect to the input variables (x, y, z) , which are then stored in the object dud . The derivative of u with respect to input variable x (i.e., \u2202u/\u2202x ) can then be extracted from dud using dud(x) . The operations dud(x) , dud(y) , dud(z) involve no computations! Just extraction of derivatives previously computed with a call to function autodiff::derivatives .","title":"Reverse mode"},{"location":"#get-in-touch","text":"Contact us on Gitter or via a GitHub Discussion if you need support and assistance when using autodiff . If you would like to report a bug, then please create a new GitHub Issue .","title":"Get in touch!"},{"location":"about/","text":"About \u00b6 Philosophy \u00b6 autodiff aims to be both efficient and easy to use C++ library for automatic differentiation. If you appreciate how it has been developed so far, and want to contribute, you are most welcome to join us in its development. And if you dislike it, please let us know how we can improve! You can contact us using our Gitter Community Channel or by creating a new GitHub Discussion . If you would like to report a bug, then please create a new GitHub Issue . License \u00b6 MIT License Copyright \u00a9 2018\u20132021 Allan Leal Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.","title":"About"},{"location":"about/#about","text":"","title":"About"},{"location":"about/#philosophy","text":"autodiff aims to be both efficient and easy to use C++ library for automatic differentiation. If you appreciate how it has been developed so far, and want to contribute, you are most welcome to join us in its development. And if you dislike it, please let us know how we can improve! You can contact us using our Gitter Community Channel or by creating a new GitHub Discussion . If you would like to report a bug, then please create a new GitHub Issue .","title":"Philosophy"},{"location":"about/#license","text":"MIT License Copyright \u00a9 2018\u20132021 Allan Leal Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.","title":"License"},{"location":"faq/","text":"Why should I consider automatic differentiation? \u00b6 The following are the reasons why you should consider automatic differentiation in your computational project: your functions are extremely complicated; manually implementing analytical derivatives is a tedious and error-prone task; and computing derivatives using finite differences can be inaccurate and inefficient. What is the difference between reverse and forward modes? \u00b6 Here is a brief, practical, and qualitative discussion on the differences between these two automatic differentiation algorithms. In a forward mode algorithm, each function evaluation produces not only its output value but also its derivative. The evaluation of a vector function, for example, computes both the output vector as well as the derivative of this vector, in general, with respect to one of the input variables (e.g., \u2202u/\u2202x , \u2202u/\u2202y ). However, the forward mode algorithm can also be used to compute directional derivatives. In this case, the derivative of the output vector with respect to a given direction vector is performed. We could say that, fundamentally, the forward mode always computes directional derivatives. The simplest case of a derivative with respect to a single input variable corresponds thus with a directional derivative whose direction vector is a unit vector along the input variable of interest (e.g., the unit vector along x , or along y , and so forth). In a reverse mode algorithm, each function evaluation produces a complete expression tree that contains the sequence of all mathematical operations between the input variables to produce the output variable (a scalar). Once this expression tree is constructed, it is then used to compute the derivatives of the scalar output variable with respect to all input variables. Is one algorithm always faster than another? \u00b6 Even though the reverse mode algorithm requires a single function evaluation and all derivatives are subsequently computed in a single pass, as the constructed expression tree is traversed, the forward algorithm can still be a much more efficient choice for your application , despite its multiple, repeated function evaluations, one for each input variable. This is because the implementation of the forward mode algorithm in autodiff uses template meta-programming techniques to avoid as much as possible temporary memory allocations and to optimize the calculations in certain cases. The reverse mode algorithm, on the other hand, requires the construction of an expression tree at runtime, and, for this, several dynamic memory allocations are needed, which can be costly. We plan to implement alternative versions of this algorithm, in which memory allocation could be done in advance to decrease the number of subsequent allocations. This, however, will require a slightly more complicated usage than it is already provided by the reverse mode algorithm implemented in autodiff . Which automatic differentiation algorithm should I use? \u00b6 Ideally, you should try both algorithms for your specific needs, benchmark them, and then make an informed decision about which one to use. If you're in a hurry, consider: forward mode : if you have a vector function, or a scalar function with not many input variables. reverse mode : if you have a scalar function with many (thousands or more) input variables. Have in mind this is a very simplistic rule, and you should definitely try both algorithms whenever possible, since the forward mode could still be faster than reverse mode even when many input variables are considered for a function of interest. How do I cite autodiff ? \u00b6 We appreciate your intention of citing autodiff in your publications. Please use the following BibTeX reference entry: @misc { autodiff , author = {Leal, Allan M. M.} , title = {autodiff, a modern, fast and expressive {C++} library for automatic differentiation} , url = {https://autodiff.github.io} , howpublished = {\\texttt{https://autodiff.github.io}} , year = {2018} } This should produce a formatted citation that looks more or less the following: Leal, A.M.M. (2018). autodiff, a modern, fast and expressive C++ library for automatic differentiation . https://autodiff.github.io Please ensure the website URL is displayed.","title":"FAQ"},{"location":"faq/#why-should-i-consider-automatic-differentiation","text":"The following are the reasons why you should consider automatic differentiation in your computational project: your functions are extremely complicated; manually implementing analytical derivatives is a tedious and error-prone task; and computing derivatives using finite differences can be inaccurate and inefficient.","title":"Why should I consider automatic differentiation?"},{"location":"faq/#what-is-the-difference-between-reverse-and-forward-modes","text":"Here is a brief, practical, and qualitative discussion on the differences between these two automatic differentiation algorithms. In a forward mode algorithm, each function evaluation produces not only its output value but also its derivative. The evaluation of a vector function, for example, computes both the output vector as well as the derivative of this vector, in general, with respect to one of the input variables (e.g., \u2202u/\u2202x , \u2202u/\u2202y ). However, the forward mode algorithm can also be used to compute directional derivatives. In this case, the derivative of the output vector with respect to a given direction vector is performed. We could say that, fundamentally, the forward mode always computes directional derivatives. The simplest case of a derivative with respect to a single input variable corresponds thus with a directional derivative whose direction vector is a unit vector along the input variable of interest (e.g., the unit vector along x , or along y , and so forth). In a reverse mode algorithm, each function evaluation produces a complete expression tree that contains the sequence of all mathematical operations between the input variables to produce the output variable (a scalar). Once this expression tree is constructed, it is then used to compute the derivatives of the scalar output variable with respect to all input variables.","title":"What is the difference between reverse and forward modes?"},{"location":"faq/#is-one-algorithm-always-faster-than-another","text":"Even though the reverse mode algorithm requires a single function evaluation and all derivatives are subsequently computed in a single pass, as the constructed expression tree is traversed, the forward algorithm can still be a much more efficient choice for your application , despite its multiple, repeated function evaluations, one for each input variable. This is because the implementation of the forward mode algorithm in autodiff uses template meta-programming techniques to avoid as much as possible temporary memory allocations and to optimize the calculations in certain cases. The reverse mode algorithm, on the other hand, requires the construction of an expression tree at runtime, and, for this, several dynamic memory allocations are needed, which can be costly. We plan to implement alternative versions of this algorithm, in which memory allocation could be done in advance to decrease the number of subsequent allocations. This, however, will require a slightly more complicated usage than it is already provided by the reverse mode algorithm implemented in autodiff .","title":"Is one algorithm always faster than another?"},{"location":"faq/#which-automatic-differentiation-algorithm-should-i-use","text":"Ideally, you should try both algorithms for your specific needs, benchmark them, and then make an informed decision about which one to use. If you're in a hurry, consider: forward mode : if you have a vector function, or a scalar function with not many input variables. reverse mode : if you have a scalar function with many (thousands or more) input variables. Have in mind this is a very simplistic rule, and you should definitely try both algorithms whenever possible, since the forward mode could still be faster than reverse mode even when many input variables are considered for a function of interest.","title":"Which automatic differentiation algorithm should I use?"},{"location":"faq/#how-do-i-cite-autodiff","text":"We appreciate your intention of citing autodiff in your publications. Please use the following BibTeX reference entry: @misc { autodiff , author = {Leal, Allan M. M.} , title = {autodiff, a modern, fast and expressive {C++} library for automatic differentiation} , url = {https://autodiff.github.io} , howpublished = {\\texttt{https://autodiff.github.io}} , year = {2018} } This should produce a formatted citation that looks more or less the following: Leal, A.M.M. (2018). autodiff, a modern, fast and expressive C++ library for automatic differentiation . https://autodiff.github.io Please ensure the website URL is displayed.","title":"How do I cite autodiff?"},{"location":"installation/","text":"Installation \u00b6 Installing autodiff is easy, since it is a header-only library . Follow the steps below. Installation using Conda \u00b6 If you have Anaconda or Miniconda installed (with Python 3.7+), you can install autodiff with a single command: conda install conda-forge::autodiff This will install autodiff in the conda environment that is active at the moment (e.g., the default conda environment is named base ). Installation using CMake \u00b6 If you have cmake installed in your system, you can then not only install autodiff but also build its examples and tests. First, you'll need to download it by either git cloning its GitHub repository : git clone https://github.com/autodiff/autodiff or by clicking here to start the download of a zip file, which you should extract to a directory of your choice. Then, execute the following steps (assuming you are in the root of the source code directory of autodiff !): mkdir .build && cd .build cmake .. cmake --build . --target install Attention We assume above that you are in the root of the source code directory, under autodiff ! The build directory will be created at autodiff/.build . We use .build here instead of the more usual build because there is a file called BUILD that provides support to Bazel build system. In operating systems that treats file and directory names as case insensitive, you may not be able to create a build directory. The previous installation commands will require administrative rights in most systems. To install autodiff locally, use: cmake .. -DCMAKE_INSTALL_PREFIX=/some/local/dir Installation by copying \u00b6 Assuming the git cloned repository or the extracted source code resides in a directory named autodiff , you can now copy the sub-directory autodiff/autodiff to somewhere in your project directory and directly use autodiff . Installation failed. What do I do? \u00b6 Discuss with us first your issue on our Gitter Community Channel . We may be able to respond more quickly there on how to sort out your issue. If bug fixes are indeed required, we'll kindly ask you to create a GitHub issue , in which you can provide more details about the issue and keep track of our progress on fixing it. You are also welcome to recommend us installation improvements in the Gitter channel.","title":"Installation"},{"location":"installation/#installation","text":"Installing autodiff is easy, since it is a header-only library . Follow the steps below.","title":"Installation"},{"location":"installation/#installation-using-conda","text":"If you have Anaconda or Miniconda installed (with Python 3.7+), you can install autodiff with a single command: conda install conda-forge::autodiff This will install autodiff in the conda environment that is active at the moment (e.g., the default conda environment is named base ).","title":"Installation using Conda"},{"location":"installation/#installation-using-cmake","text":"If you have cmake installed in your system, you can then not only install autodiff but also build its examples and tests. First, you'll need to download it by either git cloning its GitHub repository : git clone https://github.com/autodiff/autodiff or by clicking here to start the download of a zip file, which you should extract to a directory of your choice. Then, execute the following steps (assuming you are in the root of the source code directory of autodiff !): mkdir .build && cd .build cmake .. cmake --build . --target install Attention We assume above that you are in the root of the source code directory, under autodiff ! The build directory will be created at autodiff/.build . We use .build here instead of the more usual build because there is a file called BUILD that provides support to Bazel build system. In operating systems that treats file and directory names as case insensitive, you may not be able to create a build directory. The previous installation commands will require administrative rights in most systems. To install autodiff locally, use: cmake .. -DCMAKE_INSTALL_PREFIX=/some/local/dir","title":"Installation using CMake"},{"location":"installation/#installation-by-copying","text":"Assuming the git cloned repository or the extracted source code resides in a directory named autodiff , you can now copy the sub-directory autodiff/autodiff to somewhere in your project directory and directly use autodiff .","title":"Installation by copying"},{"location":"installation/#installation-failed-what-do-i-do","text":"Discuss with us first your issue on our Gitter Community Channel . We may be able to respond more quickly there on how to sort out your issue. If bug fixes are indeed required, we'll kindly ask you to create a GitHub issue , in which you can provide more details about the issue and keep track of our progress on fixing it. You are also welcome to recommend us installation improvements in the Gitter channel.","title":"Installation failed. What do I do?"},{"location":"tutorials/","text":"Tutorials \u00b6 We present here some examples demonstrating the use of autodiff for computing different types of derivatives. We welcome any contribution towards improving and expanding this list of examples. We would also love to hear your suggestions on how to better demonstrate the capabilities of autodiff . Forward mode \u00b6 Derivatives of a single-variable function \u00b6 // C++ includes #include <iostream> // autodiff include #include <autodiff/forward/dual.hpp> using namespace autodiff ; // The single-variable function for which derivatives are needed dual f ( dual x ) { return 1 + x + x * x + 1 / x + log ( x ); } int main () { dual x = 2.0 ; // the input variable x dual u = f ( x ); // the output variable u double dudx = derivative ( f , wrt ( x ), at ( x )); // evaluate the derivative du/dx std :: cout << \"u = \" << u << std :: endl ; // print the evaluated output u std :: cout << \"du/dx = \" << dudx << std :: endl ; // print the evaluated derivative du/dx } Derivatives of a single-variable function using a custom scalar (complex) \u00b6 // C++ includes #include <iostream> #include <complex> using namespace std ; // autodiff include #include <autodiff/forward/dual.hpp> using namespace autodiff ; // Specialize isArithmetic for complex to make it compatible with dual namespace autodiff :: detail { template < typename T > struct ArithmeticTraits < complex < T >> : ArithmeticTraits < T > {}; } // autodiff::detail using cxdual = Dual < complex < double > , complex < double >> ; // The single-variable function for which derivatives are needed cxdual f ( cxdual x ) { return 1 + x + x * x + 1 / x + log ( x ); } int main () { cxdual x = 2.0 ; // the input variable x cxdual u = f ( x ); // the output variable u cxdual dudx = derivative ( f , wrt ( x ), at ( x )); // evaluate the derivative du/dx cout << \"u = \" << u << endl ; // print the evaluated output u cout << \"du/dx = \" << dudx << endl ; // print the evaluated derivative du/dx } Derivatives of a multi-variable function \u00b6 // C++ includes #include <iostream> // autodiff include #include <autodiff/forward/dual.hpp> using namespace autodiff ; // The multi-variable function for which derivatives are needed dual f ( dual x , dual y , dual z ) { return 1 + x + y + z + x * y + y * z + x * z + x * y * z + exp ( x / y + y / z ); } int main () { dual x = 1.0 ; dual y = 2.0 ; dual z = 3.0 ; dual u = f ( x , y , z ); double dudx = derivative ( f , wrt ( x ), at ( x , y , z )); double dudy = derivative ( f , wrt ( y ), at ( x , y , z )); double dudz = derivative ( f , wrt ( z ), at ( x , y , z )); std :: cout << \"u = \" << u << std :: endl ; // print the evaluated output u = f(x, y, z) std :: cout << \"du/dx = \" << dudx << std :: endl ; // print the evaluated derivative du/dx std :: cout << \"du/dy = \" << dudy << std :: endl ; // print the evaluated derivative du/dy std :: cout << \"du/dz = \" << dudz << std :: endl ; // print the evaluated derivative du/dz } Derivatives of a multi-variable function with parameters \u00b6 // C++ includes #include <iostream> // autodiff include #include <autodiff/forward/dual.hpp> using namespace autodiff ; // A type defining parameters for a function of interest struct Params { dual a ; dual b ; dual c ; }; // The function that depends on parameters for which derivatives are needed dual f ( dual x , const Params & params ) { return params . a * sin ( x ) + params . b * cos ( x ) + params . c * sin ( x ) * cos ( x ); } int main () { Params params ; // initialize the parameter variables params . a = 1.0 ; // the parameter a of type dual, not double! params . b = 2.0 ; // the parameter b of type dual, not double! params . c = 3.0 ; // the parameter c of type dual, not double! dual x = 0.5 ; // the input variable x dual u = f ( x , params ); // the output variable u double dudx = derivative ( f , wrt ( x ), at ( x , params )); // evaluate the derivative du/dx double duda = derivative ( f , wrt ( params . a ), at ( x , params )); // evaluate the derivative du/da double dudb = derivative ( f , wrt ( params . b ), at ( x , params )); // evaluate the derivative du/db double dudc = derivative ( f , wrt ( params . c ), at ( x , params )); // evaluate the derivative du/dc std :: cout << \"u = \" << u << std :: endl ; // print the evaluated output u std :: cout << \"du/dx = \" << dudx << std :: endl ; // print the evaluated derivative du/dx std :: cout << \"du/da = \" << duda << std :: endl ; // print the evaluated derivative du/da std :: cout << \"du/db = \" << dudb << std :: endl ; // print the evaluated derivative du/db std :: cout << \"du/dc = \" << dudc << std :: endl ; // print the evaluated derivative du/dc } /*------------------------------------------------------------------------------------------------- === Note === --------------------------------------------------------------------------------------------------- This example would also work if real was used instead of dual. Should you need higher-order cross derivatives, however, e.g.,: double d2udxda = derivative(f, wrt(x, params.a), at(x, params)); then higher-order dual types are the right choicesince real types are optimally designed for higher-order directional derivatives. -------------------------------------------------------------------------------------------------*/ Gradient vector of a scalar function \u00b6 // C++ includes #include <iostream> // autodiff include #include <autodiff/forward/real.hpp> #include <autodiff/forward/real/eigen.hpp> using namespace autodiff ; // The scalar function for which the gradient is needed real f ( const ArrayXreal & x ) { return ( x * x . exp ()). sum (); // sum([xi * exp(xi) for i = 1:5]) } int main () { using Eigen :: VectorXd ; ArrayXreal x ( 5 ); // the input array x with 5 variables x << 1 , 2 , 3 , 4 , 5 ; // x = [1, 2, 3, 4, 5] real u ; // the output scalar u = f(x) evaluated together with gradient below VectorXd g = gradient ( f , wrt ( x ), at ( x ), u ); // evaluate the function value u and its gradient vector g = du/dx std :: cout << \"u = \" << u << std :: endl ; // print the evaluated output u std :: cout << \"g = \\n \" << g << std :: endl ; // print the evaluated gradient vector g = du/dx } Gradient vector of a scalar function with parameters \u00b6 // C++ includes #include <iostream> // autodiff include #include <autodiff/forward/real.hpp> #include <autodiff/forward/real/eigen.hpp> using namespace autodiff ; // The scalar function for which the gradient is needed real f ( const ArrayXreal & x , const ArrayXreal & p , const real & q ) { return ( x * x ). sum () * p . sum () * exp ( q ); // sum([xi * xi for i = 1:5]) * sum(p) * exp(q) } int main () { using Eigen :: VectorXd ; ArrayXreal x ( 5 ); // the input vector x with 5 variables x << 1 , 2 , 3 , 4 , 5 ; // x = [1, 2, 3, 4, 5] ArrayXreal p ( 3 ); // the input parameter vector p with 3 variables p << 1 , 2 , 3 ; // p = [1, 2, 3] real q = -2 ; // the input parameter q as a single variable real u ; // the output scalar u = f(x, p, q) evaluated together with gradient below VectorXd gx = gradient ( f , wrt ( x ), at ( x , p , q ), u ); // evaluate the function value u and its gradient vector gx = du/dx VectorXd gp = gradient ( f , wrt ( p ), at ( x , p , q ), u ); // evaluate the function value u and its gradient vector gp = du/dp VectorXd gq = gradient ( f , wrt ( q ), at ( x , p , q ), u ); // evaluate the function value u and its gradient vector gq = du/dq VectorXd gqpx = gradient ( f , wrt ( q , p , x ), at ( x , p , q ), u ); // evaluate the function value u and its gradient vector gqpx = [du/dq, du/dp, du/dx] std :: cout << \"u = \" << u << std :: endl ; // print the evaluated output u std :: cout << \"gx = \\n \" << gx << std :: endl ; // print the evaluated gradient vector gx = du/dx std :: cout << \"gp = \\n \" << gp << std :: endl ; // print the evaluated gradient vector gp = du/dp std :: cout << \"gq = \\n \" << gq << std :: endl ; // print the evaluated gradient vector gq = du/dq std :: cout << \"gqpx = \\n \" << gqpx << std :: endl ; // print the evaluated gradient vector gqpx = [du/dq, du/dp, du/dx] } /*------------------------------------------------------------------------------------------------- === Note === --------------------------------------------------------------------------------------------------- This example would also work if dual was used instead. However, if gradient, Jacobian, and directional derivatives are all you need, then real types are your best option. You want to use dual types when evaluating higher-order cross derivatives, which is not supported for real types. -------------------------------------------------------------------------------------------------*/ Jacobian matrix of a vector function \u00b6 // C++ includes #include <iostream> // autodiff include #include <autodiff/forward/real.hpp> #include <autodiff/forward/real/eigen.hpp> using namespace autodiff ; // The vector function for which the Jacobian is needed VectorXreal f ( const VectorXreal & x ) { return x * x . sum (); } int main () { using Eigen :: MatrixXd ; VectorXreal x ( 5 ); // the input vector x with 5 variables x << 1 , 2 , 3 , 4 , 5 ; // x = [1, 2, 3, 4, 5] VectorXreal F ; // the output vector F = f(x) evaluated together with Jacobian matrix below MatrixXd J = jacobian ( f , wrt ( x ), at ( x ), F ); // evaluate the output vector F and the Jacobian matrix dF/dx std :: cout << \"F = \\n \" << F << std :: endl ; // print the evaluated output vector F std :: cout << \"J = \\n \" << J << std :: endl ; // print the evaluated Jacobian matrix dF/dx } Jacobian matrix of a vector function with parameters \u00b6 // C++ includes #include <iostream> // autodiff include #include <autodiff/forward/real.hpp> #include <autodiff/forward/real/eigen.hpp> using namespace autodiff ; // The vector function with parameters for which the Jacobian is needed VectorXreal f ( const VectorXreal & x , const VectorXreal & p , const real & q ) { return x * p . sum () * exp ( q ); } int main () { using Eigen :: MatrixXd ; VectorXreal x ( 5 ); // the input vector x with 5 variables x << 1 , 2 , 3 , 4 , 5 ; // x = [1, 2, 3, 4, 5] VectorXreal p ( 3 ); // the input parameter vector p with 3 variables p << 1 , 2 , 3 ; // p = [1, 2, 3] real q = -2 ; // the input parameter q as a single variable VectorXreal F ; // the output vector F = f(x, p, q) evaluated together with Jacobian below MatrixXd Jx = jacobian ( f , wrt ( x ), at ( x , p , q ), F ); // evaluate the function and the Jacobian matrix Jx = dF/dx MatrixXd Jp = jacobian ( f , wrt ( p ), at ( x , p , q ), F ); // evaluate the function and the Jacobian matrix Jp = dF/dp MatrixXd Jq = jacobian ( f , wrt ( q ), at ( x , p , q ), F ); // evaluate the function and the Jacobian matrix Jq = dF/dq MatrixXd Jqpx = jacobian ( f , wrt ( q , p , x ), at ( x , p , q ), F ); // evaluate the function and the Jacobian matrix Jqpx = [dF/dq, dF/dp, dF/dx] std :: cout << \"F = \\n \" << F << std :: endl ; // print the evaluated output vector F std :: cout << \"Jx = \\n \" << Jx << std :: endl ; // print the evaluated Jacobian matrix dF/dx std :: cout << \"Jp = \\n \" << Jp << std :: endl ; // print the evaluated Jacobian matrix dF/dp std :: cout << \"Jq = \\n \" << Jq << std :: endl ; // print the evaluated Jacobian matrix dF/dq std :: cout << \"Jqpx = \\n \" << Jqpx << std :: endl ; // print the evaluated Jacobian matrix [dF/dq, dF/dp, dF/dx] } Higher-order cross derivatives of a scalar function \u00b6 // C++ includes #include <iostream> // autodiff include #include <autodiff/forward/dual.hpp> using namespace autodiff ; // The multi-variable function for which higher-order derivatives are needed (up to 4th order) dual4th f ( dual4th x , dual4th y , dual4th z ) { return 1 + x + y + z + x * y + y * z + x * z + x * y * z + exp ( x / y + y / z ); } int main () { dual4th x = 1.0 ; dual4th y = 2.0 ; dual4th z = 3.0 ; auto [ u0 , ux , uxy , uxyx , uxyxz ] = derivatives ( f , wrt ( x , y , x , z ), at ( x , y , z )); std :: cout << \"u0 = \" << u0 << std :: endl ; // print the evaluated value of u = f(x, y, z) std :: cout << \"ux = \" << ux << std :: endl ; // print the evaluated derivative du/dx std :: cout << \"uxy = \" << uxy << std :: endl ; // print the evaluated derivative d\u00b2u/dxdy std :: cout << \"uxyx = \" << uxyx << std :: endl ; // print the evaluated derivative d\u00b3u/dx\u00b2dy std :: cout << \"uxyxz = \" << uxyxz << std :: endl ; // print the evaluated derivative d\u2074u/dx\u00b2dydz } /*------------------------------------------------------------------------------------------------- === Note === --------------------------------------------------------------------------------------------------- In most cases, dual can be replaced by real, as commented in other examples. However, computing higher-order cross derivatives has definitely to be done using higher-order dual types (e.g., dual3rd, dual4th)! This is because real types (e.g., real2nd, real3rd, real4th) are optimally designed for computing higher-order directional derivatives. -------------------------------------------------------------------------------------------------*/ Higher-order directional derivatives of a scalar function \u00b6 // C++ includes #include <iostream> // autodiff include #include <autodiff/forward/real.hpp> using namespace autodiff ; // The multi-variable function for which higher-order derivatives are needed (up to 4th order) real4th f ( real4th x , real4th y , real4th z ) { return sin ( x ) * cos ( y ) * exp ( z ); } int main () { real4th x = 1.0 ; real4th y = 2.0 ; real4th z = 3.0 ; auto dfdv = derivatives ( f , along ( 1.0 , 1.0 , 2.0 ), at ( x , y , z )); // the directional derivatives of f along direction v = (1, 1, 2) at (x, y, z) = (1, 2, 3) std :: cout << \"dfdv[0] = \" << dfdv [ 0 ] << std :: endl ; // print the evaluated 0th order directional derivative of f along v (equivalent to f(x, y, z)) std :: cout << \"dfdv[1] = \" << dfdv [ 1 ] << std :: endl ; // print the evaluated 1st order directional derivative of f along v std :: cout << \"dfdv[2] = \" << dfdv [ 2 ] << std :: endl ; // print the evaluated 2nd order directional derivative of f along v std :: cout << \"dfdv[3] = \" << dfdv [ 3 ] << std :: endl ; // print the evaluated 3rd order directional derivative of f along v std :: cout << \"dfdv[4] = \" << dfdv [ 4 ] << std :: endl ; // print the evaluated 4th order directional derivative of f along v } /*------------------------------------------------------------------------------------------------- === Note === --------------------------------------------------------------------------------------------------- This example would also work if dual was used instead of real. However, real types are your best option for directional derivatives, as they were optimally designed for this kind of derivatives. -------------------------------------------------------------------------------------------------*/ Higher-order directional derivatives of a vector function \u00b6 // C++ includes #include <iostream> // autodiff include #include <autodiff/forward/real.hpp> #include <autodiff/forward/real/eigen.hpp> using namespace autodiff ; // The vector function for which higher-order directional derivatives are needed (up to 4th order). ArrayXreal4th f ( const ArrayXreal4th & x , real4th p ) { return p * x . log (); } int main () { using Eigen :: ArrayXd ; ArrayXreal4th x ( 5 ); // the input vector x x << 1.0 , 2.0 , 3.0 , 4.0 , 5.0 ; real4th p = 7.0 ; // the input parameter p = 1 ArrayXd vx ( 5 ); // the direction vx in the direction vector v = (vx, vp) vx << 1.0 , 1.0 , 1.0 , 1.0 , 1.0 ; double vp = 1.0 ; // the direction vp in the direction vector v = (vx, vp) auto dfdv = derivatives ( f , along ( vx , vp ), at ( x , p )); // the directional derivatives of f along direction v = (vx, vp) at (x, p) std :: cout << std :: scientific << std :: showpos ; std :: cout << \"Directional derivatives of f along v = (vx, vp) up to 4th order:\" << std :: endl ; std :: cout << \"dfdv[0] = \" << dfdv [ 0 ]. transpose () << std :: endl ; // print the evaluated 0th order directional derivative of f along v (equivalent to f(x, p)) std :: cout << \"dfdv[1] = \" << dfdv [ 1 ]. transpose () << std :: endl ; // print the evaluated 1st order directional derivative of f along v std :: cout << \"dfdv[2] = \" << dfdv [ 2 ]. transpose () << std :: endl ; // print the evaluated 2nd order directional derivative of f along v std :: cout << \"dfdv[3] = \" << dfdv [ 3 ]. transpose () << std :: endl ; // print the evaluated 3rd order directional derivative of f along v std :: cout << \"dfdv[4] = \" << dfdv [ 4 ]. transpose () << std :: endl ; // print the evaluated 4th order directional derivative of f along v std :: cout << std :: endl ; double t = 0.1 ; // the step length along direction v = (vx, vp) used to compute 4th order Taylor estimate of f ArrayXreal4th u = f ( x + t * vx , p + t * vp ); // start from (x, p), walk a step length t = 0.1 along direction v = (vx, vp) and evaluate f at this current point ArrayXd utaylor = dfdv [ 0 ] + t * dfdv [ 1 ] + ( t * t / 2.0 ) * dfdv [ 2 ] + ( t * t * t / 6.0 ) * dfdv [ 3 ] + ( t * t * t * t / 24.0 ) * dfdv [ 4 ]; // evaluate the 4th order Taylor estimate of f along direction v = (vx, vp) at a step length of t = 0.1 std :: cout << \"Comparison between exact evaluation and 4th order Taylor estimate:\" << std :: endl ; std :: cout << \"u(exact) = \" << u . transpose () << std :: endl ; std :: cout << \"u(taylor) = \" << utaylor . transpose () << std :: endl ; } /*------------------------------------------------------------------------------------------------- === Output === --------------------------------------------------------------------------------------------------- Directional derivatives of f along v = (vx, vp) up to 4th order: dfdv[0] = +0.000000e+00 +4.852030e+00 +7.690286e+00 +9.704061e+00 +1.126607e+01 dfdv[1] = +7.000000e+00 +4.193147e+00 +3.431946e+00 +3.136294e+00 +3.009438e+00 dfdv[2] = -5.000000e+00 -7.500000e-01 -1.111111e-01 +6.250000e-02 +1.200000e-01 dfdv[3] = +1.100000e+01 +1.000000e+00 +1.851852e-01 +3.125000e-02 -8.000000e-03 dfdv[4] = -3.400000e+01 -1.625000e+00 -2.222222e-01 -3.906250e-02 -3.200000e-03 Comparison between exact evaluation and 4th order Taylor estimate: u(exact) = +6.767023e-01 +5.267755e+00 +8.032955e+00 +1.001801e+01 +1.156761e+01 u(taylor) = +6.766917e-01 +5.267755e+00 +8.032955e+00 +1.001801e+01 +1.156761e+01 -------------------------------------------------------------------------------------------------*/ /*------------------------------------------------------------------------------------------------- === Note === --------------------------------------------------------------------------------------------------- This example would also work if dual was used instead of real. However, real types are your best option for directional derivatives, as they were optimally designed for this kind of derivatives. -------------------------------------------------------------------------------------------------*/ Taylor series of a scalar function along a direction \u00b6 // C++ includes #include <iostream> // autodiff include #include <autodiff/forward/real.hpp> #include <autodiff/forward/real/eigen.hpp> using namespace autodiff ; // The scalar function for which a 4th order directional Taylor series will be computed. real4th f ( const real4th & x , const real4th & y , const real4th & z ) { return sin ( x * y ) * cos ( x * z ) * exp ( z ); } int main () { real4th x = 1.0 ; // the input vector x real4th y = 2.0 ; // the input vector y real4th z = 3.0 ; // the input vector z auto g = taylorseries ( f , along ( 1 , 1 , 2 ), at ( x , y , z )); // the function g(t) as a 4th order Taylor approximation of f(x + t, y + t, z + 2t) double t = 0.1 ; // the step length used to evaluate g(t), the Taylor approximation of f(x + t, y + t, z + 2t) real4th u = f ( x + t , y + t , z + 2 * t ); // the exact value of f(x + t, y + t, z + 2t) double utaylor = g ( t ); // the 4th order Taylor estimate of f(x + t, y + t, z + 2t) std :: cout << std :: fixed ; std :: cout << \"Comparison between exact evaluation and 4th order Taylor estimate of f(x + t, y + t, z + 2t):\" << std :: endl ; std :: cout << \"u(exact) = \" << u << std :: endl ; std :: cout << \"u(taylor) = \" << utaylor << std :: endl ; } /*------------------------------------------------------------------------------------------------- === Output === --------------------------------------------------------------------------------------------------- Comparison between exact evaluation and 4th order Taylor estimate of f(x + t, y + t, z + 2t): u(exact) = -16.847071 u(taylor) = -16.793986 -------------------------------------------------------------------------------------------------*/ Taylor series of a vector function along a direction \u00b6 // C++ includes #include <iostream> // autodiff include #include <autodiff/forward/real.hpp> #include <autodiff/forward/real/eigen.hpp> using namespace autodiff ; // The vector function for which a 4th order directional Taylor series will be computed. ArrayXreal4th f ( const ArrayXreal4th & x ) { return x . sin () / x ; } int main () { using Eigen :: ArrayXd ; ArrayXreal4th x ( 5 ); // the input vector x x << 1.0 , 2.0 , 3.0 , 4.0 , 5.0 ; ArrayXd v ( 5 ); // the direction vector v v << 1.0 , 1.0 , 1.0 , 1.0 , 1.0 ; auto g = taylorseries ( f , along ( v ), at ( x )); // the function g(t) as a 4th order Taylor approximation of f(x + t\u00b7v) double t = 0.1 ; // the step length used to evaluate g(t), the Taylor approximation of f(x + t\u00b7v) ArrayXreal4th u = f ( x + t * v ); // the exact value of f(x + t\u00b7v) ArrayXd utaylor = g ( t ); // the 4th order Taylor estimate of f(x + t\u00b7v) std :: cout << std :: fixed ; std :: cout << \"Comparison between exact evaluation and 4th order Taylor estimate of f(x + t\u00b7v):\" << std :: endl ; std :: cout << \"u(exact) = \" << u . transpose () << std :: endl ; std :: cout << \"u(taylor) = \" << utaylor . transpose () << std :: endl ; } /*------------------------------------------------------------------------------------------------- === Output === --------------------------------------------------------------------------------------------------- Comparison between exact evaluation and 4th order Taylor estimate of f(x + t\u00b7v): u(exact) = 0.810189 0.411052 0.013413 -0.199580 -0.181532 u(taylor) = 0.810189 0.411052 0.013413 -0.199580 -0.181532 -------------------------------------------------------------------------------------------------*/ Reverse mode \u00b6 Single-variable function \u00b6 // C++ includes #include <iostream> using namespace std ; // autodiff include #include <autodiff/reverse/var.hpp> using namespace autodiff ; // The single-variable function for which derivatives are needed var f ( var x ) { return 1 + x + x * x + 1 / x + log ( x ); } int main () { var x = 2.0 ; // the input variable x var u = f ( x ); // the output variable u auto [ ux ] = derivatives ( u , wrt ( x )); // evaluate the derivative of u with respect to x cout << \"u = \" << u << endl ; // print the evaluated output variable u cout << \"ux = \" << ux << endl ; // print the evaluated derivative ux } Multi-variable function \u00b6 // C++ includes #include <iostream> using namespace std ; // autodiff include #include <autodiff/reverse/var.hpp> using namespace autodiff ; // The multi-variable function for which derivatives are needed var f ( var x , var y , var z ) { return 1 + x + y + z + x * y + y * z + x * z + x * y * z + exp ( x / y + y / z ); } int main () { var x = 1.0 ; // the input variable x var y = 2.0 ; // the input variable y var z = 3.0 ; // the input variable z var u = f ( x , y , z ); // the output variable u auto [ ux , uy , uz ] = derivatives ( u , wrt ( x , y , z )); // evaluate the derivatives of u with respect to x, y, z cout << \"u = \" << u << endl ; // print the evaluated output u cout << \"ux = \" << ux << endl ; // print the evaluated derivative ux cout << \"uy = \" << uy << endl ; // print the evaluated derivative uy cout << \"uz = \" << uz << endl ; // print the evaluated derivative uz } Multi-variable function with parameters \u00b6 // C++ includes #include <iostream> using namespace std ; // autodiff include #include <autodiff/reverse/var.hpp> using namespace autodiff ; // A type defining parameters for a function of interest struct Params { var a ; var b ; var c ; }; // The function that depends on parameters for which derivatives are needed var f ( var x , const Params & params ) { return params . a * sin ( x ) + params . b * cos ( x ) + params . c * sin ( x ) * cos ( x ); } int main () { Params params ; // initialize the parameter variables params . a = 1.0 ; // the parameter a of type var, not double! params . b = 2.0 ; // the parameter b of type var, not double! params . c = 3.0 ; // the parameter c of type var, not double! var x = 0.5 ; // the input variable x var u = f ( x , params ); // the output variable u auto [ ux , ua , ub , uc ] = derivatives ( u , wrt ( x , params . a , params . b , params . c )); // evaluate the derivatives of u with respect to x and parameters a, b, c cout << \"u = \" << u << endl ; // print the evaluated output u cout << \"ux = \" << ux << endl ; // print the evaluated derivative du/dx cout << \"ua = \" << ua << endl ; // print the evaluated derivative du/da cout << \"ub = \" << ub << endl ; // print the evaluated derivative du/db cout << \"uc = \" << uc << endl ; // print the evaluated derivative du/dc } Gradient of a scalar function \u00b6 // C++ includes #include <iostream> // autodiff include #include <autodiff/reverse/var.hpp> #include <autodiff/reverse/var/eigen.hpp> using namespace autodiff ; // The scalar function for which the gradient is needed var f ( const ArrayXvar & x ) { return sqrt (( x * x ). sum ()); // sqrt(sum([xi * xi for i = 1:5])) } int main () { using Eigen :: VectorXd ; VectorXvar x ( 5 ); // the input vector x with 5 variables x << 1 , 2 , 3 , 4 , 5 ; // x = [1, 2, 3, 4, 5] var y = f ( x ); // the output variable y VectorXd dydx = gradient ( y , x ); // evaluate the gradient vector dy/dx std :: cout << \"y = \" << y << std :: endl ; // print the evaluated output y std :: cout << \"dy/dx = \\n \" << dydx << std :: endl ; // print the evaluated gradient vector dy/dx } Hessian of a scalar function \u00b6 // C++ includes #include <iostream> using namespace std ; // autodiff include #include <autodiff/reverse/var.hpp> #include <autodiff/reverse/var/eigen.hpp> using namespace autodiff ; // The scalar function for which the gradient is needed var f ( const ArrayXvar & x ) { return sqrt (( x * x ). sum ()); // sqrt(sum([xi * xi for i = 1:5])) } int main () { VectorXvar x ( 5 ); // the input vector x with 5 variables x << 1 , 2 , 3 , 4 , 5 ; // x = [1, 2, 3, 4, 5] var u = f ( x ); // the output variable u Eigen :: VectorXd g ; // the gradient vector to be computed in method `hessian` Eigen :: MatrixXd H = hessian ( u , x , g ); // evaluate the Hessian matrix H and the gradient vector g of u cout << \"u = \" << u << endl ; // print the evaluated output variable u cout << \"g = \\n \" << g << endl ; // print the evaluated gradient vector of u cout << \"H = \\n \" << H << endl ; // print the evaluated Hessian matrix of u } Higher-order derivatives of a single-variable function \u00b6 // C++ includes #include <iostream> using namespace std ; // autodiff include #include <autodiff/reverse/var.hpp> using namespace autodiff ; int main () { var x = 0.5 ; // the input variable x var u = sin ( x ) * cos ( x ); // the output variable u auto [ ux ] = derivativesx ( u , wrt ( x )); // evaluate the first order derivatives of u auto [ uxx ] = derivativesx ( ux , wrt ( x )); // evaluate the second order derivatives of ux cout << \"u = \" << u << endl ; // print the evaluated output variable u cout << \"ux(autodiff) = \" << ux << endl ; // print the evaluated first order derivative ux cout << \"ux(exact) = \" << 1 - 2 * sin ( x ) * sin ( x ) << endl ; // print the exact first order derivative ux cout << \"uxx(autodiff) = \" << uxx << endl ; // print the evaluated second order derivative uxx cout << \"uxx(exact) = \" << -4 * cos ( x ) * sin ( x ) << endl ; // print the exact second order derivative uxx } /*=============================================================================== Output: ================================================================================= u = 0.420735 ux(autodiff) = 0.540302 ux(exact) = 0.540302 uxx(autodiff) = -1.68294 uxx(exact) = -1.68294 ===============================================================================*/ Higher-order derivatives of a multi-variable function \u00b6 // C++ includes #include <iostream> using namespace std ; // autodiff include #include <autodiff/reverse/var.hpp> using namespace autodiff ; int main () { var x = 1.0 ; // the input variable x var y = 0.5 ; // the input variable y var z = 2.0 ; // the input variable z var u = x * log ( y ) * exp ( z ); // the output variable u auto [ ux , uy , uz ] = derivativesx ( u , wrt ( x , y , z )); // evaluate the derivatives of u with respect to x, y, z. auto [ uxx , uxy , uxz ] = derivativesx ( ux , wrt ( x , y , z )); // evaluate the derivatives of ux with respect to x, y, z. auto [ uyx , uyy , uyz ] = derivativesx ( uy , wrt ( x , y , z )); // evaluate the derivatives of uy with respect to x, y, z. auto [ uzx , uzy , uzz ] = derivativesx ( uz , wrt ( x , y , z )); // evaluate the derivatives of uz with respect to x, y, z. cout << \"u = \" << u << endl ; // print the evaluated output variable u cout << \"ux = \" << ux << endl ; // print the evaluated first order derivative ux cout << \"uy = \" << uy << endl ; // print the evaluated first order derivative uy cout << \"uz = \" << uz << endl ; // print the evaluated first order derivative uz cout << \"uxx = \" << uxx << endl ; // print the evaluated second order derivative uxx cout << \"uxy = \" << uxy << endl ; // print the evaluated second order derivative uxy cout << \"uxz = \" << uxz << endl ; // print the evaluated second order derivative uxz cout << \"uyx = \" << uyx << endl ; // print the evaluated second order derivative uyx cout << \"uyy = \" << uyy << endl ; // print the evaluated second order derivative uyy cout << \"uyz = \" << uyz << endl ; // print the evaluated second order derivative uyz cout << \"uzx = \" << uzx << endl ; // print the evaluated second order derivative uzx cout << \"uzy = \" << uzy << endl ; // print the evaluated second order derivative uzy cout << \"uzz = \" << uzz << endl ; // print the evaluated second order derivative uzz } Integration with CMake-based projects \u00b6 Integrating autodiff in a CMake-based project is very simple as shown next. Let's assume our CMake-based project consists of two files: main.cpp and CMakeLists.txt , whose contents are shown below: main.cpp #include <iostream> #include <autodiff/forward/dual.hpp> using namespace autodiff ; dual f ( dual x ) { return 1 + x + x * x + 1 / x + log ( x ); } int main () { dual x = 1.0 ; dual u = f ( x ); double dudx = derivative ( f , wrt ( x ), at ( x )); std :: cout << \"u = \" << u << std :: endl ; std :: cout << \"du/dx = \" << dudx << std :: endl ; } CMakeLists.txt cmake_minimum_required ( VERSION 3.0 ) project ( app ) find_package ( autodiff ) add_executable ( app main.cpp ) target_link_libraries ( app autodiff::autodiff ) In the CMakeLists.txt file, note the use of the command: find_package ( autodiff ) to find the header files of the autodiff library, and the command: target_link_libraries ( app autodiff::autodiff ) to link the executable target app against the autodiff library ( autodiff::autodiff ) using CMake's modern target-based design. To build the application, do: mkdir build && cd build cmake .. -DCMAKE_PREFIX_PATH = /path/to/autodiff/install/dir make Attention If autodiff has been installed system-wide, then the CMake argument CMAKE_PREFIX_PATH should not be needed. Otherwise, you will need to specify where autodiff is installed in your machine. For example: cmake .. -DCMAKE_PREFIX_PATH = $HOME /local assuming directory $HOME/local is where autodiff was installed to, which should then contain the following directory: $HOME/local/include/autodiff/ where the autodiff header files are located. To execute the application, do: ./app","title":"Tutorials"},{"location":"tutorials/#tutorials","text":"We present here some examples demonstrating the use of autodiff for computing different types of derivatives. We welcome any contribution towards improving and expanding this list of examples. We would also love to hear your suggestions on how to better demonstrate the capabilities of autodiff .","title":"Tutorials"},{"location":"tutorials/#forward-mode","text":"","title":"Forward mode"},{"location":"tutorials/#derivatives-of-a-single-variable-function","text":"// C++ includes #include <iostream> // autodiff include #include <autodiff/forward/dual.hpp> using namespace autodiff ; // The single-variable function for which derivatives are needed dual f ( dual x ) { return 1 + x + x * x + 1 / x + log ( x ); } int main () { dual x = 2.0 ; // the input variable x dual u = f ( x ); // the output variable u double dudx = derivative ( f , wrt ( x ), at ( x )); // evaluate the derivative du/dx std :: cout << \"u = \" << u << std :: endl ; // print the evaluated output u std :: cout << \"du/dx = \" << dudx << std :: endl ; // print the evaluated derivative du/dx }","title":"Derivatives of a single-variable function"},{"location":"tutorials/#derivatives-of-a-single-variable-function-using-a-custom-scalar-complex","text":"// C++ includes #include <iostream> #include <complex> using namespace std ; // autodiff include #include <autodiff/forward/dual.hpp> using namespace autodiff ; // Specialize isArithmetic for complex to make it compatible with dual namespace autodiff :: detail { template < typename T > struct ArithmeticTraits < complex < T >> : ArithmeticTraits < T > {}; } // autodiff::detail using cxdual = Dual < complex < double > , complex < double >> ; // The single-variable function for which derivatives are needed cxdual f ( cxdual x ) { return 1 + x + x * x + 1 / x + log ( x ); } int main () { cxdual x = 2.0 ; // the input variable x cxdual u = f ( x ); // the output variable u cxdual dudx = derivative ( f , wrt ( x ), at ( x )); // evaluate the derivative du/dx cout << \"u = \" << u << endl ; // print the evaluated output u cout << \"du/dx = \" << dudx << endl ; // print the evaluated derivative du/dx }","title":"Derivatives of a single-variable function using a custom scalar (complex)"},{"location":"tutorials/#derivatives-of-a-multi-variable-function","text":"// C++ includes #include <iostream> // autodiff include #include <autodiff/forward/dual.hpp> using namespace autodiff ; // The multi-variable function for which derivatives are needed dual f ( dual x , dual y , dual z ) { return 1 + x + y + z + x * y + y * z + x * z + x * y * z + exp ( x / y + y / z ); } int main () { dual x = 1.0 ; dual y = 2.0 ; dual z = 3.0 ; dual u = f ( x , y , z ); double dudx = derivative ( f , wrt ( x ), at ( x , y , z )); double dudy = derivative ( f , wrt ( y ), at ( x , y , z )); double dudz = derivative ( f , wrt ( z ), at ( x , y , z )); std :: cout << \"u = \" << u << std :: endl ; // print the evaluated output u = f(x, y, z) std :: cout << \"du/dx = \" << dudx << std :: endl ; // print the evaluated derivative du/dx std :: cout << \"du/dy = \" << dudy << std :: endl ; // print the evaluated derivative du/dy std :: cout << \"du/dz = \" << dudz << std :: endl ; // print the evaluated derivative du/dz }","title":"Derivatives of a multi-variable function"},{"location":"tutorials/#derivatives-of-a-multi-variable-function-with-parameters","text":"// C++ includes #include <iostream> // autodiff include #include <autodiff/forward/dual.hpp> using namespace autodiff ; // A type defining parameters for a function of interest struct Params { dual a ; dual b ; dual c ; }; // The function that depends on parameters for which derivatives are needed dual f ( dual x , const Params & params ) { return params . a * sin ( x ) + params . b * cos ( x ) + params . c * sin ( x ) * cos ( x ); } int main () { Params params ; // initialize the parameter variables params . a = 1.0 ; // the parameter a of type dual, not double! params . b = 2.0 ; // the parameter b of type dual, not double! params . c = 3.0 ; // the parameter c of type dual, not double! dual x = 0.5 ; // the input variable x dual u = f ( x , params ); // the output variable u double dudx = derivative ( f , wrt ( x ), at ( x , params )); // evaluate the derivative du/dx double duda = derivative ( f , wrt ( params . a ), at ( x , params )); // evaluate the derivative du/da double dudb = derivative ( f , wrt ( params . b ), at ( x , params )); // evaluate the derivative du/db double dudc = derivative ( f , wrt ( params . c ), at ( x , params )); // evaluate the derivative du/dc std :: cout << \"u = \" << u << std :: endl ; // print the evaluated output u std :: cout << \"du/dx = \" << dudx << std :: endl ; // print the evaluated derivative du/dx std :: cout << \"du/da = \" << duda << std :: endl ; // print the evaluated derivative du/da std :: cout << \"du/db = \" << dudb << std :: endl ; // print the evaluated derivative du/db std :: cout << \"du/dc = \" << dudc << std :: endl ; // print the evaluated derivative du/dc } /*------------------------------------------------------------------------------------------------- === Note === --------------------------------------------------------------------------------------------------- This example would also work if real was used instead of dual. Should you need higher-order cross derivatives, however, e.g.,: double d2udxda = derivative(f, wrt(x, params.a), at(x, params)); then higher-order dual types are the right choicesince real types are optimally designed for higher-order directional derivatives. -------------------------------------------------------------------------------------------------*/","title":"Derivatives of a multi-variable function with parameters"},{"location":"tutorials/#gradient-vector-of-a-scalar-function","text":"// C++ includes #include <iostream> // autodiff include #include <autodiff/forward/real.hpp> #include <autodiff/forward/real/eigen.hpp> using namespace autodiff ; // The scalar function for which the gradient is needed real f ( const ArrayXreal & x ) { return ( x * x . exp ()). sum (); // sum([xi * exp(xi) for i = 1:5]) } int main () { using Eigen :: VectorXd ; ArrayXreal x ( 5 ); // the input array x with 5 variables x << 1 , 2 , 3 , 4 , 5 ; // x = [1, 2, 3, 4, 5] real u ; // the output scalar u = f(x) evaluated together with gradient below VectorXd g = gradient ( f , wrt ( x ), at ( x ), u ); // evaluate the function value u and its gradient vector g = du/dx std :: cout << \"u = \" << u << std :: endl ; // print the evaluated output u std :: cout << \"g = \\n \" << g << std :: endl ; // print the evaluated gradient vector g = du/dx }","title":"Gradient vector of a scalar function"},{"location":"tutorials/#gradient-vector-of-a-scalar-function-with-parameters","text":"// C++ includes #include <iostream> // autodiff include #include <autodiff/forward/real.hpp> #include <autodiff/forward/real/eigen.hpp> using namespace autodiff ; // The scalar function for which the gradient is needed real f ( const ArrayXreal & x , const ArrayXreal & p , const real & q ) { return ( x * x ). sum () * p . sum () * exp ( q ); // sum([xi * xi for i = 1:5]) * sum(p) * exp(q) } int main () { using Eigen :: VectorXd ; ArrayXreal x ( 5 ); // the input vector x with 5 variables x << 1 , 2 , 3 , 4 , 5 ; // x = [1, 2, 3, 4, 5] ArrayXreal p ( 3 ); // the input parameter vector p with 3 variables p << 1 , 2 , 3 ; // p = [1, 2, 3] real q = -2 ; // the input parameter q as a single variable real u ; // the output scalar u = f(x, p, q) evaluated together with gradient below VectorXd gx = gradient ( f , wrt ( x ), at ( x , p , q ), u ); // evaluate the function value u and its gradient vector gx = du/dx VectorXd gp = gradient ( f , wrt ( p ), at ( x , p , q ), u ); // evaluate the function value u and its gradient vector gp = du/dp VectorXd gq = gradient ( f , wrt ( q ), at ( x , p , q ), u ); // evaluate the function value u and its gradient vector gq = du/dq VectorXd gqpx = gradient ( f , wrt ( q , p , x ), at ( x , p , q ), u ); // evaluate the function value u and its gradient vector gqpx = [du/dq, du/dp, du/dx] std :: cout << \"u = \" << u << std :: endl ; // print the evaluated output u std :: cout << \"gx = \\n \" << gx << std :: endl ; // print the evaluated gradient vector gx = du/dx std :: cout << \"gp = \\n \" << gp << std :: endl ; // print the evaluated gradient vector gp = du/dp std :: cout << \"gq = \\n \" << gq << std :: endl ; // print the evaluated gradient vector gq = du/dq std :: cout << \"gqpx = \\n \" << gqpx << std :: endl ; // print the evaluated gradient vector gqpx = [du/dq, du/dp, du/dx] } /*------------------------------------------------------------------------------------------------- === Note === --------------------------------------------------------------------------------------------------- This example would also work if dual was used instead. However, if gradient, Jacobian, and directional derivatives are all you need, then real types are your best option. You want to use dual types when evaluating higher-order cross derivatives, which is not supported for real types. -------------------------------------------------------------------------------------------------*/","title":"Gradient vector of a scalar function with parameters"},{"location":"tutorials/#jacobian-matrix-of-a-vector-function","text":"// C++ includes #include <iostream> // autodiff include #include <autodiff/forward/real.hpp> #include <autodiff/forward/real/eigen.hpp> using namespace autodiff ; // The vector function for which the Jacobian is needed VectorXreal f ( const VectorXreal & x ) { return x * x . sum (); } int main () { using Eigen :: MatrixXd ; VectorXreal x ( 5 ); // the input vector x with 5 variables x << 1 , 2 , 3 , 4 , 5 ; // x = [1, 2, 3, 4, 5] VectorXreal F ; // the output vector F = f(x) evaluated together with Jacobian matrix below MatrixXd J = jacobian ( f , wrt ( x ), at ( x ), F ); // evaluate the output vector F and the Jacobian matrix dF/dx std :: cout << \"F = \\n \" << F << std :: endl ; // print the evaluated output vector F std :: cout << \"J = \\n \" << J << std :: endl ; // print the evaluated Jacobian matrix dF/dx }","title":"Jacobian matrix of a vector function"},{"location":"tutorials/#jacobian-matrix-of-a-vector-function-with-parameters","text":"// C++ includes #include <iostream> // autodiff include #include <autodiff/forward/real.hpp> #include <autodiff/forward/real/eigen.hpp> using namespace autodiff ; // The vector function with parameters for which the Jacobian is needed VectorXreal f ( const VectorXreal & x , const VectorXreal & p , const real & q ) { return x * p . sum () * exp ( q ); } int main () { using Eigen :: MatrixXd ; VectorXreal x ( 5 ); // the input vector x with 5 variables x << 1 , 2 , 3 , 4 , 5 ; // x = [1, 2, 3, 4, 5] VectorXreal p ( 3 ); // the input parameter vector p with 3 variables p << 1 , 2 , 3 ; // p = [1, 2, 3] real q = -2 ; // the input parameter q as a single variable VectorXreal F ; // the output vector F = f(x, p, q) evaluated together with Jacobian below MatrixXd Jx = jacobian ( f , wrt ( x ), at ( x , p , q ), F ); // evaluate the function and the Jacobian matrix Jx = dF/dx MatrixXd Jp = jacobian ( f , wrt ( p ), at ( x , p , q ), F ); // evaluate the function and the Jacobian matrix Jp = dF/dp MatrixXd Jq = jacobian ( f , wrt ( q ), at ( x , p , q ), F ); // evaluate the function and the Jacobian matrix Jq = dF/dq MatrixXd Jqpx = jacobian ( f , wrt ( q , p , x ), at ( x , p , q ), F ); // evaluate the function and the Jacobian matrix Jqpx = [dF/dq, dF/dp, dF/dx] std :: cout << \"F = \\n \" << F << std :: endl ; // print the evaluated output vector F std :: cout << \"Jx = \\n \" << Jx << std :: endl ; // print the evaluated Jacobian matrix dF/dx std :: cout << \"Jp = \\n \" << Jp << std :: endl ; // print the evaluated Jacobian matrix dF/dp std :: cout << \"Jq = \\n \" << Jq << std :: endl ; // print the evaluated Jacobian matrix dF/dq std :: cout << \"Jqpx = \\n \" << Jqpx << std :: endl ; // print the evaluated Jacobian matrix [dF/dq, dF/dp, dF/dx] }","title":"Jacobian matrix of a vector function with parameters"},{"location":"tutorials/#higher-order-cross-derivatives-of-a-scalar-function","text":"// C++ includes #include <iostream> // autodiff include #include <autodiff/forward/dual.hpp> using namespace autodiff ; // The multi-variable function for which higher-order derivatives are needed (up to 4th order) dual4th f ( dual4th x , dual4th y , dual4th z ) { return 1 + x + y + z + x * y + y * z + x * z + x * y * z + exp ( x / y + y / z ); } int main () { dual4th x = 1.0 ; dual4th y = 2.0 ; dual4th z = 3.0 ; auto [ u0 , ux , uxy , uxyx , uxyxz ] = derivatives ( f , wrt ( x , y , x , z ), at ( x , y , z )); std :: cout << \"u0 = \" << u0 << std :: endl ; // print the evaluated value of u = f(x, y, z) std :: cout << \"ux = \" << ux << std :: endl ; // print the evaluated derivative du/dx std :: cout << \"uxy = \" << uxy << std :: endl ; // print the evaluated derivative d\u00b2u/dxdy std :: cout << \"uxyx = \" << uxyx << std :: endl ; // print the evaluated derivative d\u00b3u/dx\u00b2dy std :: cout << \"uxyxz = \" << uxyxz << std :: endl ; // print the evaluated derivative d\u2074u/dx\u00b2dydz } /*------------------------------------------------------------------------------------------------- === Note === --------------------------------------------------------------------------------------------------- In most cases, dual can be replaced by real, as commented in other examples. However, computing higher-order cross derivatives has definitely to be done using higher-order dual types (e.g., dual3rd, dual4th)! This is because real types (e.g., real2nd, real3rd, real4th) are optimally designed for computing higher-order directional derivatives. -------------------------------------------------------------------------------------------------*/","title":"Higher-order cross derivatives of a scalar function"},{"location":"tutorials/#higher-order-directional-derivatives-of-a-scalar-function","text":"// C++ includes #include <iostream> // autodiff include #include <autodiff/forward/real.hpp> using namespace autodiff ; // The multi-variable function for which higher-order derivatives are needed (up to 4th order) real4th f ( real4th x , real4th y , real4th z ) { return sin ( x ) * cos ( y ) * exp ( z ); } int main () { real4th x = 1.0 ; real4th y = 2.0 ; real4th z = 3.0 ; auto dfdv = derivatives ( f , along ( 1.0 , 1.0 , 2.0 ), at ( x , y , z )); // the directional derivatives of f along direction v = (1, 1, 2) at (x, y, z) = (1, 2, 3) std :: cout << \"dfdv[0] = \" << dfdv [ 0 ] << std :: endl ; // print the evaluated 0th order directional derivative of f along v (equivalent to f(x, y, z)) std :: cout << \"dfdv[1] = \" << dfdv [ 1 ] << std :: endl ; // print the evaluated 1st order directional derivative of f along v std :: cout << \"dfdv[2] = \" << dfdv [ 2 ] << std :: endl ; // print the evaluated 2nd order directional derivative of f along v std :: cout << \"dfdv[3] = \" << dfdv [ 3 ] << std :: endl ; // print the evaluated 3rd order directional derivative of f along v std :: cout << \"dfdv[4] = \" << dfdv [ 4 ] << std :: endl ; // print the evaluated 4th order directional derivative of f along v } /*------------------------------------------------------------------------------------------------- === Note === --------------------------------------------------------------------------------------------------- This example would also work if dual was used instead of real. However, real types are your best option for directional derivatives, as they were optimally designed for this kind of derivatives. -------------------------------------------------------------------------------------------------*/","title":"Higher-order directional derivatives of a scalar function"},{"location":"tutorials/#higher-order-directional-derivatives-of-a-vector-function","text":"// C++ includes #include <iostream> // autodiff include #include <autodiff/forward/real.hpp> #include <autodiff/forward/real/eigen.hpp> using namespace autodiff ; // The vector function for which higher-order directional derivatives are needed (up to 4th order). ArrayXreal4th f ( const ArrayXreal4th & x , real4th p ) { return p * x . log (); } int main () { using Eigen :: ArrayXd ; ArrayXreal4th x ( 5 ); // the input vector x x << 1.0 , 2.0 , 3.0 , 4.0 , 5.0 ; real4th p = 7.0 ; // the input parameter p = 1 ArrayXd vx ( 5 ); // the direction vx in the direction vector v = (vx, vp) vx << 1.0 , 1.0 , 1.0 , 1.0 , 1.0 ; double vp = 1.0 ; // the direction vp in the direction vector v = (vx, vp) auto dfdv = derivatives ( f , along ( vx , vp ), at ( x , p )); // the directional derivatives of f along direction v = (vx, vp) at (x, p) std :: cout << std :: scientific << std :: showpos ; std :: cout << \"Directional derivatives of f along v = (vx, vp) up to 4th order:\" << std :: endl ; std :: cout << \"dfdv[0] = \" << dfdv [ 0 ]. transpose () << std :: endl ; // print the evaluated 0th order directional derivative of f along v (equivalent to f(x, p)) std :: cout << \"dfdv[1] = \" << dfdv [ 1 ]. transpose () << std :: endl ; // print the evaluated 1st order directional derivative of f along v std :: cout << \"dfdv[2] = \" << dfdv [ 2 ]. transpose () << std :: endl ; // print the evaluated 2nd order directional derivative of f along v std :: cout << \"dfdv[3] = \" << dfdv [ 3 ]. transpose () << std :: endl ; // print the evaluated 3rd order directional derivative of f along v std :: cout << \"dfdv[4] = \" << dfdv [ 4 ]. transpose () << std :: endl ; // print the evaluated 4th order directional derivative of f along v std :: cout << std :: endl ; double t = 0.1 ; // the step length along direction v = (vx, vp) used to compute 4th order Taylor estimate of f ArrayXreal4th u = f ( x + t * vx , p + t * vp ); // start from (x, p), walk a step length t = 0.1 along direction v = (vx, vp) and evaluate f at this current point ArrayXd utaylor = dfdv [ 0 ] + t * dfdv [ 1 ] + ( t * t / 2.0 ) * dfdv [ 2 ] + ( t * t * t / 6.0 ) * dfdv [ 3 ] + ( t * t * t * t / 24.0 ) * dfdv [ 4 ]; // evaluate the 4th order Taylor estimate of f along direction v = (vx, vp) at a step length of t = 0.1 std :: cout << \"Comparison between exact evaluation and 4th order Taylor estimate:\" << std :: endl ; std :: cout << \"u(exact) = \" << u . transpose () << std :: endl ; std :: cout << \"u(taylor) = \" << utaylor . transpose () << std :: endl ; } /*------------------------------------------------------------------------------------------------- === Output === --------------------------------------------------------------------------------------------------- Directional derivatives of f along v = (vx, vp) up to 4th order: dfdv[0] = +0.000000e+00 +4.852030e+00 +7.690286e+00 +9.704061e+00 +1.126607e+01 dfdv[1] = +7.000000e+00 +4.193147e+00 +3.431946e+00 +3.136294e+00 +3.009438e+00 dfdv[2] = -5.000000e+00 -7.500000e-01 -1.111111e-01 +6.250000e-02 +1.200000e-01 dfdv[3] = +1.100000e+01 +1.000000e+00 +1.851852e-01 +3.125000e-02 -8.000000e-03 dfdv[4] = -3.400000e+01 -1.625000e+00 -2.222222e-01 -3.906250e-02 -3.200000e-03 Comparison between exact evaluation and 4th order Taylor estimate: u(exact) = +6.767023e-01 +5.267755e+00 +8.032955e+00 +1.001801e+01 +1.156761e+01 u(taylor) = +6.766917e-01 +5.267755e+00 +8.032955e+00 +1.001801e+01 +1.156761e+01 -------------------------------------------------------------------------------------------------*/ /*------------------------------------------------------------------------------------------------- === Note === --------------------------------------------------------------------------------------------------- This example would also work if dual was used instead of real. However, real types are your best option for directional derivatives, as they were optimally designed for this kind of derivatives. -------------------------------------------------------------------------------------------------*/","title":"Higher-order directional derivatives of a vector function"},{"location":"tutorials/#taylor-series-of-a-scalar-function-along-a-direction","text":"// C++ includes #include <iostream> // autodiff include #include <autodiff/forward/real.hpp> #include <autodiff/forward/real/eigen.hpp> using namespace autodiff ; // The scalar function for which a 4th order directional Taylor series will be computed. real4th f ( const real4th & x , const real4th & y , const real4th & z ) { return sin ( x * y ) * cos ( x * z ) * exp ( z ); } int main () { real4th x = 1.0 ; // the input vector x real4th y = 2.0 ; // the input vector y real4th z = 3.0 ; // the input vector z auto g = taylorseries ( f , along ( 1 , 1 , 2 ), at ( x , y , z )); // the function g(t) as a 4th order Taylor approximation of f(x + t, y + t, z + 2t) double t = 0.1 ; // the step length used to evaluate g(t), the Taylor approximation of f(x + t, y + t, z + 2t) real4th u = f ( x + t , y + t , z + 2 * t ); // the exact value of f(x + t, y + t, z + 2t) double utaylor = g ( t ); // the 4th order Taylor estimate of f(x + t, y + t, z + 2t) std :: cout << std :: fixed ; std :: cout << \"Comparison between exact evaluation and 4th order Taylor estimate of f(x + t, y + t, z + 2t):\" << std :: endl ; std :: cout << \"u(exact) = \" << u << std :: endl ; std :: cout << \"u(taylor) = \" << utaylor << std :: endl ; } /*------------------------------------------------------------------------------------------------- === Output === --------------------------------------------------------------------------------------------------- Comparison between exact evaluation and 4th order Taylor estimate of f(x + t, y + t, z + 2t): u(exact) = -16.847071 u(taylor) = -16.793986 -------------------------------------------------------------------------------------------------*/","title":"Taylor series of a scalar function along a direction"},{"location":"tutorials/#taylor-series-of-a-vector-function-along-a-direction","text":"// C++ includes #include <iostream> // autodiff include #include <autodiff/forward/real.hpp> #include <autodiff/forward/real/eigen.hpp> using namespace autodiff ; // The vector function for which a 4th order directional Taylor series will be computed. ArrayXreal4th f ( const ArrayXreal4th & x ) { return x . sin () / x ; } int main () { using Eigen :: ArrayXd ; ArrayXreal4th x ( 5 ); // the input vector x x << 1.0 , 2.0 , 3.0 , 4.0 , 5.0 ; ArrayXd v ( 5 ); // the direction vector v v << 1.0 , 1.0 , 1.0 , 1.0 , 1.0 ; auto g = taylorseries ( f , along ( v ), at ( x )); // the function g(t) as a 4th order Taylor approximation of f(x + t\u00b7v) double t = 0.1 ; // the step length used to evaluate g(t), the Taylor approximation of f(x + t\u00b7v) ArrayXreal4th u = f ( x + t * v ); // the exact value of f(x + t\u00b7v) ArrayXd utaylor = g ( t ); // the 4th order Taylor estimate of f(x + t\u00b7v) std :: cout << std :: fixed ; std :: cout << \"Comparison between exact evaluation and 4th order Taylor estimate of f(x + t\u00b7v):\" << std :: endl ; std :: cout << \"u(exact) = \" << u . transpose () << std :: endl ; std :: cout << \"u(taylor) = \" << utaylor . transpose () << std :: endl ; } /*------------------------------------------------------------------------------------------------- === Output === --------------------------------------------------------------------------------------------------- Comparison between exact evaluation and 4th order Taylor estimate of f(x + t\u00b7v): u(exact) = 0.810189 0.411052 0.013413 -0.199580 -0.181532 u(taylor) = 0.810189 0.411052 0.013413 -0.199580 -0.181532 -------------------------------------------------------------------------------------------------*/","title":"Taylor series of a vector function along a direction"},{"location":"tutorials/#reverse-mode","text":"","title":"Reverse mode"},{"location":"tutorials/#single-variable-function","text":"// C++ includes #include <iostream> using namespace std ; // autodiff include #include <autodiff/reverse/var.hpp> using namespace autodiff ; // The single-variable function for which derivatives are needed var f ( var x ) { return 1 + x + x * x + 1 / x + log ( x ); } int main () { var x = 2.0 ; // the input variable x var u = f ( x ); // the output variable u auto [ ux ] = derivatives ( u , wrt ( x )); // evaluate the derivative of u with respect to x cout << \"u = \" << u << endl ; // print the evaluated output variable u cout << \"ux = \" << ux << endl ; // print the evaluated derivative ux }","title":"Single-variable function"},{"location":"tutorials/#multi-variable-function","text":"// C++ includes #include <iostream> using namespace std ; // autodiff include #include <autodiff/reverse/var.hpp> using namespace autodiff ; // The multi-variable function for which derivatives are needed var f ( var x , var y , var z ) { return 1 + x + y + z + x * y + y * z + x * z + x * y * z + exp ( x / y + y / z ); } int main () { var x = 1.0 ; // the input variable x var y = 2.0 ; // the input variable y var z = 3.0 ; // the input variable z var u = f ( x , y , z ); // the output variable u auto [ ux , uy , uz ] = derivatives ( u , wrt ( x , y , z )); // evaluate the derivatives of u with respect to x, y, z cout << \"u = \" << u << endl ; // print the evaluated output u cout << \"ux = \" << ux << endl ; // print the evaluated derivative ux cout << \"uy = \" << uy << endl ; // print the evaluated derivative uy cout << \"uz = \" << uz << endl ; // print the evaluated derivative uz }","title":"Multi-variable function"},{"location":"tutorials/#multi-variable-function-with-parameters","text":"// C++ includes #include <iostream> using namespace std ; // autodiff include #include <autodiff/reverse/var.hpp> using namespace autodiff ; // A type defining parameters for a function of interest struct Params { var a ; var b ; var c ; }; // The function that depends on parameters for which derivatives are needed var f ( var x , const Params & params ) { return params . a * sin ( x ) + params . b * cos ( x ) + params . c * sin ( x ) * cos ( x ); } int main () { Params params ; // initialize the parameter variables params . a = 1.0 ; // the parameter a of type var, not double! params . b = 2.0 ; // the parameter b of type var, not double! params . c = 3.0 ; // the parameter c of type var, not double! var x = 0.5 ; // the input variable x var u = f ( x , params ); // the output variable u auto [ ux , ua , ub , uc ] = derivatives ( u , wrt ( x , params . a , params . b , params . c )); // evaluate the derivatives of u with respect to x and parameters a, b, c cout << \"u = \" << u << endl ; // print the evaluated output u cout << \"ux = \" << ux << endl ; // print the evaluated derivative du/dx cout << \"ua = \" << ua << endl ; // print the evaluated derivative du/da cout << \"ub = \" << ub << endl ; // print the evaluated derivative du/db cout << \"uc = \" << uc << endl ; // print the evaluated derivative du/dc }","title":"Multi-variable function with parameters"},{"location":"tutorials/#gradient-of-a-scalar-function","text":"// C++ includes #include <iostream> // autodiff include #include <autodiff/reverse/var.hpp> #include <autodiff/reverse/var/eigen.hpp> using namespace autodiff ; // The scalar function for which the gradient is needed var f ( const ArrayXvar & x ) { return sqrt (( x * x ). sum ()); // sqrt(sum([xi * xi for i = 1:5])) } int main () { using Eigen :: VectorXd ; VectorXvar x ( 5 ); // the input vector x with 5 variables x << 1 , 2 , 3 , 4 , 5 ; // x = [1, 2, 3, 4, 5] var y = f ( x ); // the output variable y VectorXd dydx = gradient ( y , x ); // evaluate the gradient vector dy/dx std :: cout << \"y = \" << y << std :: endl ; // print the evaluated output y std :: cout << \"dy/dx = \\n \" << dydx << std :: endl ; // print the evaluated gradient vector dy/dx }","title":"Gradient of a scalar function"},{"location":"tutorials/#hessian-of-a-scalar-function","text":"// C++ includes #include <iostream> using namespace std ; // autodiff include #include <autodiff/reverse/var.hpp> #include <autodiff/reverse/var/eigen.hpp> using namespace autodiff ; // The scalar function for which the gradient is needed var f ( const ArrayXvar & x ) { return sqrt (( x * x ). sum ()); // sqrt(sum([xi * xi for i = 1:5])) } int main () { VectorXvar x ( 5 ); // the input vector x with 5 variables x << 1 , 2 , 3 , 4 , 5 ; // x = [1, 2, 3, 4, 5] var u = f ( x ); // the output variable u Eigen :: VectorXd g ; // the gradient vector to be computed in method `hessian` Eigen :: MatrixXd H = hessian ( u , x , g ); // evaluate the Hessian matrix H and the gradient vector g of u cout << \"u = \" << u << endl ; // print the evaluated output variable u cout << \"g = \\n \" << g << endl ; // print the evaluated gradient vector of u cout << \"H = \\n \" << H << endl ; // print the evaluated Hessian matrix of u }","title":"Hessian of a scalar function"},{"location":"tutorials/#higher-order-derivatives-of-a-single-variable-function","text":"// C++ includes #include <iostream> using namespace std ; // autodiff include #include <autodiff/reverse/var.hpp> using namespace autodiff ; int main () { var x = 0.5 ; // the input variable x var u = sin ( x ) * cos ( x ); // the output variable u auto [ ux ] = derivativesx ( u , wrt ( x )); // evaluate the first order derivatives of u auto [ uxx ] = derivativesx ( ux , wrt ( x )); // evaluate the second order derivatives of ux cout << \"u = \" << u << endl ; // print the evaluated output variable u cout << \"ux(autodiff) = \" << ux << endl ; // print the evaluated first order derivative ux cout << \"ux(exact) = \" << 1 - 2 * sin ( x ) * sin ( x ) << endl ; // print the exact first order derivative ux cout << \"uxx(autodiff) = \" << uxx << endl ; // print the evaluated second order derivative uxx cout << \"uxx(exact) = \" << -4 * cos ( x ) * sin ( x ) << endl ; // print the exact second order derivative uxx } /*=============================================================================== Output: ================================================================================= u = 0.420735 ux(autodiff) = 0.540302 ux(exact) = 0.540302 uxx(autodiff) = -1.68294 uxx(exact) = -1.68294 ===============================================================================*/","title":"Higher-order derivatives of a single-variable function"},{"location":"tutorials/#higher-order-derivatives-of-a-multi-variable-function","text":"// C++ includes #include <iostream> using namespace std ; // autodiff include #include <autodiff/reverse/var.hpp> using namespace autodiff ; int main () { var x = 1.0 ; // the input variable x var y = 0.5 ; // the input variable y var z = 2.0 ; // the input variable z var u = x * log ( y ) * exp ( z ); // the output variable u auto [ ux , uy , uz ] = derivativesx ( u , wrt ( x , y , z )); // evaluate the derivatives of u with respect to x, y, z. auto [ uxx , uxy , uxz ] = derivativesx ( ux , wrt ( x , y , z )); // evaluate the derivatives of ux with respect to x, y, z. auto [ uyx , uyy , uyz ] = derivativesx ( uy , wrt ( x , y , z )); // evaluate the derivatives of uy with respect to x, y, z. auto [ uzx , uzy , uzz ] = derivativesx ( uz , wrt ( x , y , z )); // evaluate the derivatives of uz with respect to x, y, z. cout << \"u = \" << u << endl ; // print the evaluated output variable u cout << \"ux = \" << ux << endl ; // print the evaluated first order derivative ux cout << \"uy = \" << uy << endl ; // print the evaluated first order derivative uy cout << \"uz = \" << uz << endl ; // print the evaluated first order derivative uz cout << \"uxx = \" << uxx << endl ; // print the evaluated second order derivative uxx cout << \"uxy = \" << uxy << endl ; // print the evaluated second order derivative uxy cout << \"uxz = \" << uxz << endl ; // print the evaluated second order derivative uxz cout << \"uyx = \" << uyx << endl ; // print the evaluated second order derivative uyx cout << \"uyy = \" << uyy << endl ; // print the evaluated second order derivative uyy cout << \"uyz = \" << uyz << endl ; // print the evaluated second order derivative uyz cout << \"uzx = \" << uzx << endl ; // print the evaluated second order derivative uzx cout << \"uzy = \" << uzy << endl ; // print the evaluated second order derivative uzy cout << \"uzz = \" << uzz << endl ; // print the evaluated second order derivative uzz }","title":"Higher-order derivatives of a multi-variable function"},{"location":"tutorials/#integration-with-cmake-based-projects","text":"Integrating autodiff in a CMake-based project is very simple as shown next. Let's assume our CMake-based project consists of two files: main.cpp and CMakeLists.txt , whose contents are shown below: main.cpp #include <iostream> #include <autodiff/forward/dual.hpp> using namespace autodiff ; dual f ( dual x ) { return 1 + x + x * x + 1 / x + log ( x ); } int main () { dual x = 1.0 ; dual u = f ( x ); double dudx = derivative ( f , wrt ( x ), at ( x )); std :: cout << \"u = \" << u << std :: endl ; std :: cout << \"du/dx = \" << dudx << std :: endl ; } CMakeLists.txt cmake_minimum_required ( VERSION 3.0 ) project ( app ) find_package ( autodiff ) add_executable ( app main.cpp ) target_link_libraries ( app autodiff::autodiff ) In the CMakeLists.txt file, note the use of the command: find_package ( autodiff ) to find the header files of the autodiff library, and the command: target_link_libraries ( app autodiff::autodiff ) to link the executable target app against the autodiff library ( autodiff::autodiff ) using CMake's modern target-based design. To build the application, do: mkdir build && cd build cmake .. -DCMAKE_PREFIX_PATH = /path/to/autodiff/install/dir make Attention If autodiff has been installed system-wide, then the CMake argument CMAKE_PREFIX_PATH should not be needed. Otherwise, you will need to specify where autodiff is installed in your machine. For example: cmake .. -DCMAKE_PREFIX_PATH = $HOME /local assuming directory $HOME/local is where autodiff was installed to, which should then contain the following directory: $HOME/local/include/autodiff/ where the autodiff header files are located. To execute the application, do: ./app","title":"Integration with CMake-based projects"}]}