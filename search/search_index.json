{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"autodiff \u00b6 autodiff is a C++17 library that uses modern and advanced programming techniques to enable automatic computation of derivatives in an efficient and easy way. Attention autodiff is planned to be a long-term maintained automatic differentiation project, with many more algorithms being implemented in the future. Please have in mind, however, that autodiff is still in its earlier stages of development. We welcome you to use autodiff and recommend us any improvements you think it is necessary. Demonstration \u00b6 Consider the following function f(x, y, z) : double f ( double x , double y , double z ) { return ( x + y + z ) * exp ( x * y * z ); } which we use use to evaluate variable u = f(x, y, z) : double x = 1.0 ; double y = 2.0 ; double z = 3.0 ; double u = f ( x , y , z ); How can we minimally transform this code so that not only u , but also its derivatives \u2202u/\u2202x , \u2202u/\u2202y , and \u2202u/\u2202z , can be computed? The next two sections present how this can be achieved using two automatic differentiation algorithms implemented in autodiff : forward mode and reverse mode . Forward mode \u00b6 In a forward mode automatic differentiation algorithm, both output variables and one or more of their derivatives are computed together. For example, the function evaluation f(x, y, z) can be transformed in a way that it will not only produce the value of u , the output variable , but also one or more of its derivatives (\u2202u/\u2202x, \u2202u/\u2202y, \u2202u/\u2202z) with respect to the input variables (x, y, z) . Enabling forward automatic differentiation for the calculation of derivatives using autodiff is relatively simple. For our previous function f , we only need to replace the floating-point type double to autodiff::dual for both input and output variables: dual f ( const dual & x , const dual & y , const dual & z ) { return ( x + y + z ) * exp ( x * y * z ); } We can now compute the derivatives \u2202u/\u2202x , \u2202u/\u2202y , and \u2202u/\u2202z as follows: dual x = 1.0 ; dual y = 2.0 ; dual z = 3.0 ; dual u = f ( x , y , z ); double ux = derivative ( f , wrt ( x ), at ( x , y , z )); double uy = derivative ( f , wrt ( y ), at ( x , y , z )); double uz = derivative ( f , wrt ( z ), at ( x , y , z )); The auxiliary function autodiff::wrt , an acronym for with respect to , is used to indicate which input variable (x, y, z) is the selected one to compute the partial derivative of f . The auxiliary function autodiff::at is used to indicate where (at which values of its parameters) the derivative of f is evaluated. Reverse mode \u00b6 In a reverse mode automatic differentiation algorithm, the output variable of a function is evaluated first. During this function evaluation, all mathematical operations between the input variables are \"recorded\" in an expression tree . By traversing this tree from top-level (output variable as the root node) to bottom-level (input variables as the leaf nodes), it is possible to compute the contribution of each branch on the derivatives of the output variable with respect to input variables. Thus, a single pass in a reverse mode calculation computes all derivatives , in contrast with forward mode, which requires one pass for each input variable. Note, however, that it is possible to change the behavior of a forward pass so that many (even all) derivatives of an output variable are computed simultaneously (e.g., in a single forward pass, \u2202u/\u2202x , \u2202u/\u2202y , and \u2202u/\u2202z are evaluated together with u , in contrast with three forward passes, each one computing the individual derivatives). Similar as before, we can use autodiff to enable reverse automatic differentiation for our function f by simply replacing type double by autodiff::var as follows: var f ( var x , var y , var z ) { return ( x + y + z ) * exp ( x * y * z ); } The code below demonstrates how the derivatives \u2202u/\u2202x , \u2202u/\u2202y , and \u2202u/\u2202z can be calculated: var x = 1.0 ; var y = 2.0 ; var z = 3.0 ; var u = f ( x , y , z ); auto [ ux , uy , uz ] = derivatives ( u , wrt ( x , y , z )); The function autodiff::derivatives will traverse the expression tree stored in variable u and compute all its derivatives with respect to the input variables (x, y, z) , which are then stored in the object dud . The derivative of u with respect to input variable x (i.e., \u2202u/\u2202x ) can then be extracted from dud using dud(x) . The operations dud(x) , dud(y) , dud(z) involve no computations! Just extraction of derivatives previously computed with a call to function autodiff::derivatives . Get in touch! \u00b6 Contact us on Gitter if you need support and assistance when using autodiff .","title":"Home"},{"location":"#autodiff","text":"autodiff is a C++17 library that uses modern and advanced programming techniques to enable automatic computation of derivatives in an efficient and easy way. Attention autodiff is planned to be a long-term maintained automatic differentiation project, with many more algorithms being implemented in the future. Please have in mind, however, that autodiff is still in its earlier stages of development. We welcome you to use autodiff and recommend us any improvements you think it is necessary.","title":"autodiff"},{"location":"#demonstration","text":"Consider the following function f(x, y, z) : double f ( double x , double y , double z ) { return ( x + y + z ) * exp ( x * y * z ); } which we use use to evaluate variable u = f(x, y, z) : double x = 1.0 ; double y = 2.0 ; double z = 3.0 ; double u = f ( x , y , z ); How can we minimally transform this code so that not only u , but also its derivatives \u2202u/\u2202x , \u2202u/\u2202y , and \u2202u/\u2202z , can be computed? The next two sections present how this can be achieved using two automatic differentiation algorithms implemented in autodiff : forward mode and reverse mode .","title":"Demonstration"},{"location":"#forward-mode","text":"In a forward mode automatic differentiation algorithm, both output variables and one or more of their derivatives are computed together. For example, the function evaluation f(x, y, z) can be transformed in a way that it will not only produce the value of u , the output variable , but also one or more of its derivatives (\u2202u/\u2202x, \u2202u/\u2202y, \u2202u/\u2202z) with respect to the input variables (x, y, z) . Enabling forward automatic differentiation for the calculation of derivatives using autodiff is relatively simple. For our previous function f , we only need to replace the floating-point type double to autodiff::dual for both input and output variables: dual f ( const dual & x , const dual & y , const dual & z ) { return ( x + y + z ) * exp ( x * y * z ); } We can now compute the derivatives \u2202u/\u2202x , \u2202u/\u2202y , and \u2202u/\u2202z as follows: dual x = 1.0 ; dual y = 2.0 ; dual z = 3.0 ; dual u = f ( x , y , z ); double ux = derivative ( f , wrt ( x ), at ( x , y , z )); double uy = derivative ( f , wrt ( y ), at ( x , y , z )); double uz = derivative ( f , wrt ( z ), at ( x , y , z )); The auxiliary function autodiff::wrt , an acronym for with respect to , is used to indicate which input variable (x, y, z) is the selected one to compute the partial derivative of f . The auxiliary function autodiff::at is used to indicate where (at which values of its parameters) the derivative of f is evaluated.","title":"Forward mode"},{"location":"#reverse-mode","text":"In a reverse mode automatic differentiation algorithm, the output variable of a function is evaluated first. During this function evaluation, all mathematical operations between the input variables are \"recorded\" in an expression tree . By traversing this tree from top-level (output variable as the root node) to bottom-level (input variables as the leaf nodes), it is possible to compute the contribution of each branch on the derivatives of the output variable with respect to input variables. Thus, a single pass in a reverse mode calculation computes all derivatives , in contrast with forward mode, which requires one pass for each input variable. Note, however, that it is possible to change the behavior of a forward pass so that many (even all) derivatives of an output variable are computed simultaneously (e.g., in a single forward pass, \u2202u/\u2202x , \u2202u/\u2202y , and \u2202u/\u2202z are evaluated together with u , in contrast with three forward passes, each one computing the individual derivatives). Similar as before, we can use autodiff to enable reverse automatic differentiation for our function f by simply replacing type double by autodiff::var as follows: var f ( var x , var y , var z ) { return ( x + y + z ) * exp ( x * y * z ); } The code below demonstrates how the derivatives \u2202u/\u2202x , \u2202u/\u2202y , and \u2202u/\u2202z can be calculated: var x = 1.0 ; var y = 2.0 ; var z = 3.0 ; var u = f ( x , y , z ); auto [ ux , uy , uz ] = derivatives ( u , wrt ( x , y , z )); The function autodiff::derivatives will traverse the expression tree stored in variable u and compute all its derivatives with respect to the input variables (x, y, z) , which are then stored in the object dud . The derivative of u with respect to input variable x (i.e., \u2202u/\u2202x ) can then be extracted from dud using dud(x) . The operations dud(x) , dud(y) , dud(z) involve no computations! Just extraction of derivatives previously computed with a call to function autodiff::derivatives .","title":"Reverse mode"},{"location":"#get-in-touch","text":"Contact us on Gitter if you need support and assistance when using autodiff .","title":"Get in touch!"},{"location":"about/","text":"About \u00b6 Philosophy \u00b6 autodiff aims to be both efficient and easy to use C++ library for automatic differentiation. If you appreciate how it has been developed so far, and want to contribute, you are most welcome to join us in its development. And if you dislike it, please let us know how we can improve! You can contact us here . License \u00b6 MIT License Copyright \u00a9 2018\u20132021 Allan Leal Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.","title":"About"},{"location":"about/#about","text":"","title":"About"},{"location":"about/#philosophy","text":"autodiff aims to be both efficient and easy to use C++ library for automatic differentiation. If you appreciate how it has been developed so far, and want to contribute, you are most welcome to join us in its development. And if you dislike it, please let us know how we can improve! You can contact us here .","title":"Philosophy"},{"location":"about/#license","text":"MIT License Copyright \u00a9 2018\u20132021 Allan Leal Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.","title":"License"},{"location":"faq/","text":"Why should I consider automatic differentiation? \u00b6 The following are the reasons why you should consider automatic differentiation in your computational project: your functions are extremely complicated; manually implementing analytical derivatives is a tedious and error-prone task; and computing derivatives using finite differences can be inaccurate and inefficient. What is the difference between reverse and forward modes? \u00b6 Here is a brief, practical, and qualitative discussion on the differences between these two automatic differentiation algorithms. In a forward mode algorithm, each function evaluation produces not only its output value but also its derivative. The evaluation of a vector function, for example, computes both the output vector as well as the derivative of this vector, in general, with respect to one of the input variables (e.g., \u2202u/\u2202x , \u2202u/\u2202y ). However, the forward mode algorithm can also be used to compute directional derivatives. In this case, the derivative of the output vector with respect to a given direction vector is performed. We could say that, fundamentally, the forward mode always computes directional derivatives. The simplest case of a derivative with respect to a single input variable corresponds thus with a directional derivative whose direction vector is a unit vector along the input variable of interest (e.g., the unit vector along x , or along y , and so forth). In a reverse mode algorithm, each function evaluation produces a complete expression tree that contains the sequence of all mathematical operations between the input variables to produce the output variable (a scalar). Once this expression tree is constructed, it is then used to compute the derivatives of the scalar output variable with respect to all input variables. Is one algorithm always faster than another? \u00b6 Even though the reverse mode algorithm requires a single function evaluation and all derivatives are subsequently computed in a single pass, as the constructed expression tree is traversed, the forward algorithm can still be a much more efficient choice for your application , despite its multiple, repeated function evaluations, one for each input variable. This is because the implementation of the forward mode algorithm in autodiff uses template meta-programming techniques to avoid as much as possible temporary memory allocations and to optimize the calculations in certain cases. The reverse mode algorithm, on the other hand, requires the construction of an expression tree at runtime, and, for this, several dynamic memory allocations are needed, which can be costly. We plan to implement alternative versions of this algorithm, in which memory allocation could be done in advance to decrease the number of subsequent allocations. This, however, will require a slightly more complicated usage than it is already provided by the reverse mode algorithm implemented in autodiff . Which automatic differentiation algorithm should I use? \u00b6 Ideally, you should try both algorithms for your specific needs, benchmark them, and then make an informed decision about which one to use. If you're in a hurry, consider: forward mode : if you have a vector function, or a scalar function with not many input variables. reverse mode : if you have a scalar function with many (thousands or more) input variables. Have in mind this is a very simplistic rule, and you should definitely try both algorithms whenever possible, since the forward mode could still be faster than reverse mode even when many input variables are considered for a function of interest.","title":"FAQ"},{"location":"faq/#why-should-i-consider-automatic-differentiation","text":"The following are the reasons why you should consider automatic differentiation in your computational project: your functions are extremely complicated; manually implementing analytical derivatives is a tedious and error-prone task; and computing derivatives using finite differences can be inaccurate and inefficient.","title":"Why should I consider automatic differentiation?"},{"location":"faq/#what-is-the-difference-between-reverse-and-forward-modes","text":"Here is a brief, practical, and qualitative discussion on the differences between these two automatic differentiation algorithms. In a forward mode algorithm, each function evaluation produces not only its output value but also its derivative. The evaluation of a vector function, for example, computes both the output vector as well as the derivative of this vector, in general, with respect to one of the input variables (e.g., \u2202u/\u2202x , \u2202u/\u2202y ). However, the forward mode algorithm can also be used to compute directional derivatives. In this case, the derivative of the output vector with respect to a given direction vector is performed. We could say that, fundamentally, the forward mode always computes directional derivatives. The simplest case of a derivative with respect to a single input variable corresponds thus with a directional derivative whose direction vector is a unit vector along the input variable of interest (e.g., the unit vector along x , or along y , and so forth). In a reverse mode algorithm, each function evaluation produces a complete expression tree that contains the sequence of all mathematical operations between the input variables to produce the output variable (a scalar). Once this expression tree is constructed, it is then used to compute the derivatives of the scalar output variable with respect to all input variables.","title":"What is the difference between reverse and forward modes?"},{"location":"faq/#is-one-algorithm-always-faster-than-another","text":"Even though the reverse mode algorithm requires a single function evaluation and all derivatives are subsequently computed in a single pass, as the constructed expression tree is traversed, the forward algorithm can still be a much more efficient choice for your application , despite its multiple, repeated function evaluations, one for each input variable. This is because the implementation of the forward mode algorithm in autodiff uses template meta-programming techniques to avoid as much as possible temporary memory allocations and to optimize the calculations in certain cases. The reverse mode algorithm, on the other hand, requires the construction of an expression tree at runtime, and, for this, several dynamic memory allocations are needed, which can be costly. We plan to implement alternative versions of this algorithm, in which memory allocation could be done in advance to decrease the number of subsequent allocations. This, however, will require a slightly more complicated usage than it is already provided by the reverse mode algorithm implemented in autodiff .","title":"Is one algorithm always faster than another?"},{"location":"faq/#which-automatic-differentiation-algorithm-should-i-use","text":"Ideally, you should try both algorithms for your specific needs, benchmark them, and then make an informed decision about which one to use. If you're in a hurry, consider: forward mode : if you have a vector function, or a scalar function with not many input variables. reverse mode : if you have a scalar function with many (thousands or more) input variables. Have in mind this is a very simplistic rule, and you should definitely try both algorithms whenever possible, since the forward mode could still be faster than reverse mode even when many input variables are considered for a function of interest.","title":"Which automatic differentiation algorithm should I use?"},{"location":"installation/","text":"Installation \u00b6 Installing autodiff is easy, since it is a header-only library . Follow the steps below. Download \u00b6 Download autodiff by either git cloning its GitHub repository : git clone https://github.com/autodiff/autodiff or by clicking here to start the download of a zip file, which you should extract to a directory of your choice. Installation by conda \u00b6 The easiest way of installing autodiff is via conda: conda install conda-forge::autodiff For this, install Miniconda with Python 3.8+. You should be familiar with conda though. Installation by copying \u00b6 Assuming the git cloned repository or extracted source code resides in a directory named autodiff , you can now copy the directory autodiff/autodiff to somewhere in your project directory and directly use autodiff . This quick and dirty solution might suffices in most cases. If this solution bothers you, read the next section! Installation using CMake \u00b6 If you have cmake installed in your system, you can then install autodiff (and also build its tests and examples) as follows: mkdir .build && cd .build cmake .. cmake --build . --target install Attention We assume above that you are in the root of the source code directory, under autodiff ! The build directory will be created at autodiff/.build . We use .build here instead of the more usual build because there is a file called BUILD that provides support to Bazel build system. In operating systems that treats file and directory names as case insensitive, you may not be able to create a build directory. The previous installation commands will require administrative rights in most systems. To install autodiff locally, use: cmake .. -DCMAKE_INSTALL_PREFIX=/some/local/dir Installation failed. What do I do? \u00b6 Create a new issue , and let us know what happened and possibly howe we can improve the installation process of autodiff .","title":"Installation"},{"location":"installation/#installation","text":"Installing autodiff is easy, since it is a header-only library . Follow the steps below.","title":"Installation"},{"location":"installation/#download","text":"Download autodiff by either git cloning its GitHub repository : git clone https://github.com/autodiff/autodiff or by clicking here to start the download of a zip file, which you should extract to a directory of your choice.","title":"Download"},{"location":"installation/#installation-by-conda","text":"The easiest way of installing autodiff is via conda: conda install conda-forge::autodiff For this, install Miniconda with Python 3.8+. You should be familiar with conda though.","title":"Installation by conda"},{"location":"installation/#installation-by-copying","text":"Assuming the git cloned repository or extracted source code resides in a directory named autodiff , you can now copy the directory autodiff/autodiff to somewhere in your project directory and directly use autodiff . This quick and dirty solution might suffices in most cases. If this solution bothers you, read the next section!","title":"Installation by copying"},{"location":"installation/#installation-using-cmake","text":"If you have cmake installed in your system, you can then install autodiff (and also build its tests and examples) as follows: mkdir .build && cd .build cmake .. cmake --build . --target install Attention We assume above that you are in the root of the source code directory, under autodiff ! The build directory will be created at autodiff/.build . We use .build here instead of the more usual build because there is a file called BUILD that provides support to Bazel build system. In operating systems that treats file and directory names as case insensitive, you may not be able to create a build directory. The previous installation commands will require administrative rights in most systems. To install autodiff locally, use: cmake .. -DCMAKE_INSTALL_PREFIX=/some/local/dir","title":"Installation using CMake"},{"location":"installation/#installation-failed-what-do-i-do","text":"Create a new issue , and let us know what happened and possibly howe we can improve the installation process of autodiff .","title":"Installation failed. What do I do?"},{"location":"tutorials/","text":"Tutorials \u00b6 Forward mode \u00b6 Single-variable function \u00b6 // C++ includes #include <iostream> using namespace std ; // autodiff include #include <autodiff/forward.hpp> using namespace autodiff ; // The single-variable function for which derivatives are needed dual f ( dual x ) { return 1 + x + x * x + 1 / x + log ( x ); } int main () { dual x = 2.0 ; // the input variable x dual u = f ( x ); // the output variable u double dudx = derivative ( f , wrt ( x ), at ( x )); // evaluate the derivative du/dx cout << \"u = \" << u << endl ; // print the evaluated output u cout << \"du/dx = \" << dudx << endl ; // print the evaluated derivative du/dx } Multi-variable function \u00b6 // C++ includes #include <iostream> using namespace std ; // autodiff include #include <autodiff/forward.hpp> using namespace autodiff ; // The multi-variable function for which derivatives are needed dual f ( dual x , dual y , dual z ) { return 1 + x + y + z + x * y + y * z + x * z + x * y * z + exp ( x / y + y / z ); } int main () { dual x = 1.0 ; // the input variable x dual y = 2.0 ; // the input variable y dual z = 3.0 ; // the input variable z dual u = f ( x , y , z ); // the output scalar u = f(x, y, z) double dudx = derivative ( f , wrt ( x ), at ( x , y , z )); // evaluate du/dx double dudy = derivative ( f , wrt ( y ), at ( x , y , z )); // evaluate du/dy double dudz = derivative ( f , wrt ( z ), at ( x , y , z )); // evaluate du/dz cout << \"u = \" << u << endl ; // print the evaluated output u cout << \"du/dx = \" << dudx << endl ; // print the evaluated derivative du/dx cout << \"du/dy = \" << dudy << endl ; // print the evaluated derivative du/dy cout << \"du/dz = \" << dudz << endl ; // print the evaluated derivative du/dz } Multi-variable function with parameters \u00b6 // C++ includes #include <iostream> using namespace std ; // autodiff include #include <autodiff/forward.hpp> using namespace autodiff ; // A type defining parameters for a function of interest struct Params { dual a ; dual b ; dual c ; }; // The function that depends on parameters for which derivatives are needed dual f ( dual x , const Params & params ) { return params . a * sin ( x ) + params . b * cos ( x ) + params . c * sin ( x ) * cos ( x ); } int main () { Params params ; // initialize the parameter variables params . a = 1.0 ; // the parameter a of type dual, not double! params . b = 2.0 ; // the parameter b of type dual, not double! params . c = 3.0 ; // the parameter c of type dual, not double! dual x = 0.5 ; // the input variable x dual u = f ( x , params ); // the output variable u double dudx = derivative ( f , wrt ( x ), at ( x , params )); // evaluate the derivative du/dx double duda = derivative ( f , wrt ( params . a ), at ( x , params )); // evaluate the derivative du/da double dudb = derivative ( f , wrt ( params . b ), at ( x , params )); // evaluate the derivative du/db double dudc = derivative ( f , wrt ( params . c ), at ( x , params )); // evaluate the derivative du/dc cout << \"u = \" << u << endl ; // print the evaluated output u cout << \"du/dx = \" << dudx << endl ; // print the evaluated derivative du/dx cout << \"du/da = \" << duda << endl ; // print the evaluated derivative du/da cout << \"du/db = \" << dudb << endl ; // print the evaluated derivative du/db cout << \"du/dc = \" << dudc << endl ; // print the evaluated derivative du/dc } Gradient of a scalar function \u00b6 // C++ includes #include <iostream> using namespace std ; // Eigen includes #include <Eigen/Core> using namespace Eigen ; // autodiff include #include <autodiff/forward.hpp> #include <autodiff/forward/eigen.hpp> using namespace autodiff ; // The scalar function for which the gradient is needed dual f ( const VectorXdual & x ) { return x . cwiseProduct ( x ). sum (); // sum([x(i) * x(i) for i = 1:5]) } int main () { VectorXdual x ( 5 ); // the input vector x with 5 variables x << 1 , 2 , 3 , 4 , 5 ; // x = [1, 2, 3, 4, 5] dual u ; // the output scalar u = f(x) evaluated together with gradient below VectorXd g = gradient ( f , wrt ( x ), at ( x ), u ); // evaluate the function value u and its gradient vector g = du/dx cout << \"u = \" << u << endl ; // print the evaluated output u cout << \"g = \\n \" << g << endl ; // print the evaluated gradient vector g = du/dx } Gradient of a scalar function with parameters \u00b6 // C++ includes #include <iostream> using namespace std ; // Eigen includes #include <Eigen/Core> using namespace Eigen ; // autodiff include #include <autodiff/forward.hpp> #include <autodiff/forward/eigen.hpp> using namespace autodiff ; // The scalar function for which the gradient is needed dual f ( const VectorXdual & x , const VectorXdual & p ) { return x . cwiseProduct ( x ). sum () * exp ( p . sum ()); // sum([x(i) * x(i) for i = 1:5]) * exp(sum(p)) } int main () { VectorXdual x ( 5 ); // the input vector x with 5 variables x << 1 , 2 , 3 , 4 , 5 ; // x = [1, 2, 3, 4, 5] VectorXdual p ( 3 ); // the input parameter vector p with 3 variables p << 1 , 2 , 3 ; // p = [1, 2, 3] dual u ; // the output scalar u = f(x, p) evaluated together with gradient below VectorXd gx = gradient ( f , wrt ( x ), at ( x , p ), u ); // evaluate the function value u and its gradient vector gx = du/dx VectorXd gp = gradient ( f , wrt ( p ), at ( x , p ), u ); // evaluate the function value u and its gradient vector gp = du/dp VectorXd gpx = gradient ( f , wrtpack ( p , x ), at ( x , p ), u ); // evaluate the function value u and its gradient vector gp = [du/dp, du/dx] cout << \"u = \" << u << endl ; // print the evaluated output u cout << \"gx = \\n \" << gx << endl ; // print the evaluated gradient vector gx = du/dx cout << \"gp = \\n \" << gp << endl ; // print the evaluated gradient vector gp = du/dp cout << \"gpx = \\n \" << gpx << endl ; // print the evaluated gradient vector gp = [du/dp, du/dx] } Jacobian of a vector function \u00b6 // C++ includes #include <iostream> using namespace std ; // Eigen includes #include <Eigen/Core> using namespace Eigen ; // autodiff include #include <autodiff/forward.hpp> #include <autodiff/forward/eigen.hpp> using namespace autodiff ; // The vector function for which the Jacobian is needed VectorXdual f ( const VectorXdual & x ) { return x * x . sum (); } int main () { VectorXdual x ( 5 ); // the input vector x with 5 variables x << 1 , 2 , 3 , 4 , 5 ; // x = [1, 2, 3, 4, 5] VectorXdual F ; // the output vector F = f(x) evaluated together with Jacobian matrix below MatrixXd J = jacobian ( f , wrt ( x ), at ( x ), F ); // evaluate the output vector F and the Jacobian matrix dF/dx cout << \"F = \\n \" << F << endl ; // print the evaluated output vector F cout << \"J = \\n \" << J << endl ; // print the evaluated Jacobian matrix dF/dx } Jacobian of a vector function with parameters \u00b6 // C++ includes #include <iostream> using namespace std ; // Eigen includes #include <Eigen/Core> using namespace Eigen ; // autodiff include #include <autodiff/forward.hpp> #include <autodiff/forward/eigen.hpp> using namespace autodiff ; // The vector function with parameters for which the Jacobian is needed VectorXdual f ( const VectorXdual & x , const VectorXdual & p ) { return x * exp ( p . sum ()); } int main () { VectorXdual x ( 5 ); // the input vector x with 5 variables x << 1 , 2 , 3 , 4 , 5 ; // x = [1, 2, 3, 4, 5] VectorXdual p ( 3 ); // the input parameter vector p with 3 variables p << 1 , 2 , 3 ; // p = [1, 2, 3] VectorXdual F ; // the output vector F = f(x, p) evaluated together with Jacobian below MatrixXd Jx = jacobian ( f , wrt ( x ), at ( x , p ), F ); // evaluate the function and the Jacobian matrix dF/dx MatrixXd Jp = jacobian ( f , wrt ( p ), at ( x , p ), F ); // evaluate the function and the Jacobian matrix dF/dp MatrixXd Jpx = jacobian ( f , wrtpack ( p , x ), at ( x , p ), F ); // evaluate the function and the Jacobian matrix [dF/dp, dF/dx] cout << \"F = \\n \" << F << endl ; // print the evaluated output vector F cout << \"Jx = \\n \" << Jx << endl ; // print the evaluated Jacobian matrix dF/dx cout << \"Jp = \\n \" << Jp << endl ; // print the evaluated Jacobian matrix dF/dp cout << \"Jpx = \\n \" << Jpx << endl ; // print the evaluated Jacobian matrix [dF/dp, dF/dx] } Higher-order derivatives of a multi-variable function \u00b6 // C++ includes #include <iostream> using namespace std ; // autodiff include #include <autodiff/forward.hpp> using namespace autodiff ; // Define a 2nd order dual type using HigherOrderDual<N> construct. using dual2nd = HigherOrderDual < 2 > ; // The single-variable function for which derivatives are needed dual2nd f ( dual2nd x , dual2nd y ) { return 1 + x + y + x * y + y / x + log ( x / y ); } int main () { dual2nd x = 2.0 ; // the input variable x dual2nd y = 1.0 ; // the input variable y dual2nd u = f ( x , y ); // the output variable u dual ux = derivative ( f , wrt ( x ), at ( x , y )); // evaluate the derivative du/dx dual uy = derivative ( f , wrt ( y ), at ( x , y )); // evaluate the derivative du/dy double uxx = derivative ( f , wrt < 2 > ( x ), at ( x , y )); // evaluate the derivative d\u00b2u/dxdx double uxy = derivative ( f , wrt ( x , y ), at ( x , y )); // evaluate the derivative d\u00b2u/dxdy double uyx = derivative ( f , wrt ( y , x ), at ( x , y )); // evaluate the derivative d\u00b2u/dydx double uyy = derivative ( f , wrt < 2 > ( y ), at ( x , y )); // evaluate the derivative d\u00b2u/dydy cout << \"u = \" << u << endl ; // print the evaluated output u cout << \"ux = \" << ux << endl ; // print the evaluated derivative du/dx cout << \"uy = \" << uy << endl ; // print the evaluated derivative du/dy cout << \"uxx = \" << uxx << endl ; // print the evaluated derivative d\u00b2u/dxdx cout << \"uxy = \" << uxy << endl ; // print the evaluated derivative d\u00b2u/dxdy cout << \"uyx = \" << uyx << endl ; // print the evaluated derivative d\u00b2u/dydx cout << \"uyy = \" << uyy << endl ; // print the evaluated derivative d\u00b2u/dydy } Reverse mode \u00b6 Single-variable function \u00b6 // C++ includes #include <iostream> using namespace std ; // autodiff include #include <autodiff/reverse.hpp> using namespace autodiff ; // The single-variable function for which derivatives are needed var f ( var x ) { return 1 + x + x * x + 1 / x + log ( x ); } int main () { var x = 2.0 ; // the input variable x var u = f ( x ); // the output variable u auto [ ux ] = derivatives ( u , wrt ( x )); // evaluate the derivative of u with respect to x cout << \"u = \" << u << endl ; // print the evaluated output variable u cout << \"ux = \" << ux << endl ; // print the evaluated derivative ux } Multi-variable function \u00b6 // C++ includes #include <iostream> using namespace std ; // autodiff include #include <autodiff/reverse.hpp> using namespace autodiff ; // The multi-variable function for which derivatives are needed var f ( var x , var y , var z ) { return 1 + x + y + z + x * y + y * z + x * z + x * y * z + exp ( x / y + y / z ); } int main () { var x = 1.0 ; // the input variable x var y = 2.0 ; // the input variable y var z = 3.0 ; // the input variable z var u = f ( x , y , z ); // the output variable u auto [ ux , uy , uz ] = derivatives ( u , wrt ( x , y , z )); // evaluate the derivatives of u with respect to x, y, z cout << \"u = \" << u << endl ; // print the evaluated output u cout << \"ux = \" << ux << endl ; // print the evaluated derivative ux cout << \"uy = \" << uy << endl ; // print the evaluated derivative uy cout << \"uz = \" << uz << endl ; // print the evaluated derivative uz } Multi-variable function with parameters \u00b6 // C++ includes #include <iostream> using namespace std ; // autodiff include #include <autodiff/reverse.hpp> using namespace autodiff ; // A type defining parameters for a function of interest struct Params { var a ; var b ; var c ; }; // The function that depends on parameters for which derivatives are needed var f ( var x , const Params & params ) { return params . a * sin ( x ) + params . b * cos ( x ) + params . c * sin ( x ) * cos ( x ); } int main () { Params params ; // initialize the parameter variables params . a = 1.0 ; // the parameter a of type var, not double! params . b = 2.0 ; // the parameter b of type var, not double! params . c = 3.0 ; // the parameter c of type var, not double! var x = 0.5 ; // the input variable x var u = f ( x , params ); // the output variable u auto [ ux , ua , ub , uc ] = derivatives ( u , wrt ( x , params . a , params . b , params . c )); // evaluate the derivatives of u with respect to x and parameters a, b, c cout << \"u = \" << u << endl ; // print the evaluated output u cout << \"ux = \" << ux << endl ; // print the evaluated derivative du/dx cout << \"ua = \" << ua << endl ; // print the evaluated derivative du/da cout << \"ub = \" << ub << endl ; // print the evaluated derivative du/db cout << \"uc = \" << uc << endl ; // print the evaluated derivative du/dc } Gradient of a scalar function \u00b6 // C++ includes #include <iostream> using namespace std ; // Eigen includes #include <Eigen/Core> using namespace Eigen ; // autodiff include #include <autodiff/reverse.hpp> #include <autodiff/reverse/eigen.hpp> using namespace autodiff ; // The scalar function for which the gradient is needed var f ( const VectorXvar & x ) { return sqrt ( x . cwiseProduct ( x ). sum ()); // sqrt(sum([x(i) * x(i) for i = 1:5])) } int main () { VectorXvar x ( 5 ); // the input vector x with 5 variables x << 1 , 2 , 3 , 4 , 5 ; // x = [1, 2, 3, 4, 5] var y = f ( x ); // the output variable y VectorXd dydx = gradient ( y , x ); // evaluate the gradient vector dy/dx cout << \"y = \" << y << endl ; // print the evaluated output y cout << \"dy/dx = \\n \" << dydx << endl ; // print the evaluated gradient vector dy/dx } Hessian of a scalar function \u00b6 // C++ includes #include <iostream> using namespace std ; // Eigen includes #include <Eigen/Core> using namespace Eigen ; // autodiff include #include <autodiff/reverse.hpp> #include <autodiff/reverse/eigen.hpp> using namespace autodiff ; // The scalar function for which the gradient is needed var f ( const VectorXvar & x ) { return sqrt ( x . cwiseProduct ( x ). sum ()); // sqrt(sum([x(i) * x(i) for i = 1:5])) } int main () { VectorXvar x ( 5 ); // the input vector x with 5 variables x << 1 , 2 , 3 , 4 , 5 ; // x = [1, 2, 3, 4, 5] var u = f ( x ); // the output variable u VectorXd g ; // the gradient vector to be computed in method `hessian` MatrixXd H = hessian ( u , x , g ); // evaluate the Hessian matrix H and the gradient vector g of u cout << \"u = \" << u << endl ; // print the evaluated output variable u cout << \"g = \\n \" << g << endl ; // print the evaluated gradient vector of u cout << \"H = \\n \" << H << endl ; // print the evaluated Hessian matrix of u } Higher-order derivatives of a single-variable function \u00b6 // C++ includes #include <iostream> using namespace std ; // autodiff include #include <autodiff/reverse.hpp> using namespace autodiff ; int main () { var x = 0.5 ; // the input variable x var u = sin ( x ) * cos ( x ); // the output variable u auto [ ux ] = derivativesx ( u , wrt ( x )); // evaluate the first order derivatives of u auto [ uxx ] = derivativesx ( ux , wrt ( x )); // evaluate the second order derivatives of ux cout << \"u = \" << u << endl ; // print the evaluated output variable u cout << \"ux(autodiff) = \" << ux << endl ; // print the evaluated first order derivative ux cout << \"ux(exact) = \" << 1 - 2 * sin ( x ) * sin ( x ) << endl ; // print the exact first order derivative ux cout << \"uxx(autodiff) = \" << uxx << endl ; // print the evaluated second order derivative uxx cout << \"uxx(exact) = \" << -4 * cos ( x ) * sin ( x ) << endl ; // print the exact second order derivative uxx } /*=============================================================================== Output: ================================================================================= u = 0.420735 ux(autodiff) = 0.540302 ux(exact) = 0.540302 uxx(autodiff) = -1.68294 uxx(exact) = -1.68294 ===============================================================================*/ Higher-order derivatives of a multi-variable function \u00b6 // C++ includes #include <iostream> using namespace std ; // autodiff include #include <autodiff/reverse.hpp> using namespace autodiff ; int main () { var x = 1.0 ; // the input variable x var y = 0.5 ; // the input variable y var z = 2.0 ; // the input variable z var u = x * log ( y ) * exp ( z ); // the output variable u auto [ ux , uy , uz ] = derivativesx ( u , wrt ( x , y , z )); // evaluate the derivatives of u with respect to x, y, z. auto [ uxx , uxy , uxz ] = derivativesx ( ux , wrt ( x , y , z )); // evaluate the derivatives of ux with respect to x, y, z. auto [ uyx , uyy , uyz ] = derivativesx ( uy , wrt ( x , y , z )); // evaluate the derivatives of uy with respect to x, y, z. auto [ uzx , uzy , uzz ] = derivativesx ( uz , wrt ( x , y , z )); // evaluate the derivatives of uz with respect to x, y, z. cout << \"u = \" << u << endl ; // print the evaluated output variable u cout << \"ux = \" << ux << endl ; // print the evaluated first order derivative ux cout << \"uy = \" << uy << endl ; // print the evaluated first order derivative uy cout << \"uz = \" << uz << endl ; // print the evaluated first order derivative uz cout << \"uxx = \" << uxx << endl ; // print the evaluated second order derivative uxx cout << \"uxy = \" << uxy << endl ; // print the evaluated second order derivative uxy cout << \"uxz = \" << uxz << endl ; // print the evaluated second order derivative uxz cout << \"uyx = \" << uyx << endl ; // print the evaluated second order derivative uyx cout << \"uyy = \" << uyy << endl ; // print the evaluated second order derivative uyy cout << \"uyz = \" << uyz << endl ; // print the evaluated second order derivative uyz cout << \"uzx = \" << uzx << endl ; // print the evaluated second order derivative uzx cout << \"uzy = \" << uzy << endl ; // print the evaluated second order derivative uzy cout << \"uzz = \" << uzz << endl ; // print the evaluated second order derivative uzz } Integration with CMake-based projects \u00b6 Integrating autodiff in a CMake-based project is very simple as shown next. Let's assume our CMake-based project consists of two files: main.cpp and CMakeLists.txt , whose contents are shown below: main.cpp #include <iostream> using namespace std ; #include <autodiff/forward.hpp> using namespace autodiff ; dual f ( dual x ) { return 1 + x + x * x + 1 / x + log ( x ); } int main () { dual x = 1.0 ; dual u = f ( x ); double dudx = derivative ( f , wrt ( x ), at ( x )); cout << \"u = \" << u << endl ; cout << \"du/dx = \" << dudx << endl ; } CMakeLists.txt cmake_minimum_required ( VERSION 3.0 ) project ( app ) find_package ( autodiff ) add_executable ( app main.cpp ) target_link_libraries ( app autodiff::autodiff ) In the CMakeLists.txt file, note the use of the command: find_package ( autodiff ) to find the header files of the autodiff library, and the command: target_link_libraries ( app autodiff::autodiff ) to link the executable target app against the autodiff library ( autodiff::autodiff ) using CMake's modern target-based design. To build the application, do: mkdir build && cd build cmake .. -DCMAKE_PREFIX_PATH = /path/to/autodiff/install/dir make Attention If autodiff has been installed system-wide, then the CMake argument CMAKE_PREFIX_PATH should not be needed. Otherwise, you will need to specify where autodiff is installed in your machine. For example: cmake .. -DCMAKE_PREFIX_PATH = $HOME /local assuming directory $HOME/local is where autodiff was installed to, which should then contain the following directory: $HOME/local/include/autodiff/ where the autodiff header files are located. To execute the application, do: ./app","title":"Tutorials"},{"location":"tutorials/#tutorials","text":"","title":"Tutorials"},{"location":"tutorials/#forward-mode","text":"","title":"Forward mode"},{"location":"tutorials/#single-variable-function","text":"// C++ includes #include <iostream> using namespace std ; // autodiff include #include <autodiff/forward.hpp> using namespace autodiff ; // The single-variable function for which derivatives are needed dual f ( dual x ) { return 1 + x + x * x + 1 / x + log ( x ); } int main () { dual x = 2.0 ; // the input variable x dual u = f ( x ); // the output variable u double dudx = derivative ( f , wrt ( x ), at ( x )); // evaluate the derivative du/dx cout << \"u = \" << u << endl ; // print the evaluated output u cout << \"du/dx = \" << dudx << endl ; // print the evaluated derivative du/dx }","title":"Single-variable function"},{"location":"tutorials/#multi-variable-function","text":"// C++ includes #include <iostream> using namespace std ; // autodiff include #include <autodiff/forward.hpp> using namespace autodiff ; // The multi-variable function for which derivatives are needed dual f ( dual x , dual y , dual z ) { return 1 + x + y + z + x * y + y * z + x * z + x * y * z + exp ( x / y + y / z ); } int main () { dual x = 1.0 ; // the input variable x dual y = 2.0 ; // the input variable y dual z = 3.0 ; // the input variable z dual u = f ( x , y , z ); // the output scalar u = f(x, y, z) double dudx = derivative ( f , wrt ( x ), at ( x , y , z )); // evaluate du/dx double dudy = derivative ( f , wrt ( y ), at ( x , y , z )); // evaluate du/dy double dudz = derivative ( f , wrt ( z ), at ( x , y , z )); // evaluate du/dz cout << \"u = \" << u << endl ; // print the evaluated output u cout << \"du/dx = \" << dudx << endl ; // print the evaluated derivative du/dx cout << \"du/dy = \" << dudy << endl ; // print the evaluated derivative du/dy cout << \"du/dz = \" << dudz << endl ; // print the evaluated derivative du/dz }","title":"Multi-variable function"},{"location":"tutorials/#multi-variable-function-with-parameters","text":"// C++ includes #include <iostream> using namespace std ; // autodiff include #include <autodiff/forward.hpp> using namespace autodiff ; // A type defining parameters for a function of interest struct Params { dual a ; dual b ; dual c ; }; // The function that depends on parameters for which derivatives are needed dual f ( dual x , const Params & params ) { return params . a * sin ( x ) + params . b * cos ( x ) + params . c * sin ( x ) * cos ( x ); } int main () { Params params ; // initialize the parameter variables params . a = 1.0 ; // the parameter a of type dual, not double! params . b = 2.0 ; // the parameter b of type dual, not double! params . c = 3.0 ; // the parameter c of type dual, not double! dual x = 0.5 ; // the input variable x dual u = f ( x , params ); // the output variable u double dudx = derivative ( f , wrt ( x ), at ( x , params )); // evaluate the derivative du/dx double duda = derivative ( f , wrt ( params . a ), at ( x , params )); // evaluate the derivative du/da double dudb = derivative ( f , wrt ( params . b ), at ( x , params )); // evaluate the derivative du/db double dudc = derivative ( f , wrt ( params . c ), at ( x , params )); // evaluate the derivative du/dc cout << \"u = \" << u << endl ; // print the evaluated output u cout << \"du/dx = \" << dudx << endl ; // print the evaluated derivative du/dx cout << \"du/da = \" << duda << endl ; // print the evaluated derivative du/da cout << \"du/db = \" << dudb << endl ; // print the evaluated derivative du/db cout << \"du/dc = \" << dudc << endl ; // print the evaluated derivative du/dc }","title":"Multi-variable function with parameters"},{"location":"tutorials/#gradient-of-a-scalar-function","text":"// C++ includes #include <iostream> using namespace std ; // Eigen includes #include <Eigen/Core> using namespace Eigen ; // autodiff include #include <autodiff/forward.hpp> #include <autodiff/forward/eigen.hpp> using namespace autodiff ; // The scalar function for which the gradient is needed dual f ( const VectorXdual & x ) { return x . cwiseProduct ( x ). sum (); // sum([x(i) * x(i) for i = 1:5]) } int main () { VectorXdual x ( 5 ); // the input vector x with 5 variables x << 1 , 2 , 3 , 4 , 5 ; // x = [1, 2, 3, 4, 5] dual u ; // the output scalar u = f(x) evaluated together with gradient below VectorXd g = gradient ( f , wrt ( x ), at ( x ), u ); // evaluate the function value u and its gradient vector g = du/dx cout << \"u = \" << u << endl ; // print the evaluated output u cout << \"g = \\n \" << g << endl ; // print the evaluated gradient vector g = du/dx }","title":"Gradient of a scalar function"},{"location":"tutorials/#gradient-of-a-scalar-function-with-parameters","text":"// C++ includes #include <iostream> using namespace std ; // Eigen includes #include <Eigen/Core> using namespace Eigen ; // autodiff include #include <autodiff/forward.hpp> #include <autodiff/forward/eigen.hpp> using namespace autodiff ; // The scalar function for which the gradient is needed dual f ( const VectorXdual & x , const VectorXdual & p ) { return x . cwiseProduct ( x ). sum () * exp ( p . sum ()); // sum([x(i) * x(i) for i = 1:5]) * exp(sum(p)) } int main () { VectorXdual x ( 5 ); // the input vector x with 5 variables x << 1 , 2 , 3 , 4 , 5 ; // x = [1, 2, 3, 4, 5] VectorXdual p ( 3 ); // the input parameter vector p with 3 variables p << 1 , 2 , 3 ; // p = [1, 2, 3] dual u ; // the output scalar u = f(x, p) evaluated together with gradient below VectorXd gx = gradient ( f , wrt ( x ), at ( x , p ), u ); // evaluate the function value u and its gradient vector gx = du/dx VectorXd gp = gradient ( f , wrt ( p ), at ( x , p ), u ); // evaluate the function value u and its gradient vector gp = du/dp VectorXd gpx = gradient ( f , wrtpack ( p , x ), at ( x , p ), u ); // evaluate the function value u and its gradient vector gp = [du/dp, du/dx] cout << \"u = \" << u << endl ; // print the evaluated output u cout << \"gx = \\n \" << gx << endl ; // print the evaluated gradient vector gx = du/dx cout << \"gp = \\n \" << gp << endl ; // print the evaluated gradient vector gp = du/dp cout << \"gpx = \\n \" << gpx << endl ; // print the evaluated gradient vector gp = [du/dp, du/dx] }","title":"Gradient of a scalar function with parameters"},{"location":"tutorials/#jacobian-of-a-vector-function","text":"// C++ includes #include <iostream> using namespace std ; // Eigen includes #include <Eigen/Core> using namespace Eigen ; // autodiff include #include <autodiff/forward.hpp> #include <autodiff/forward/eigen.hpp> using namespace autodiff ; // The vector function for which the Jacobian is needed VectorXdual f ( const VectorXdual & x ) { return x * x . sum (); } int main () { VectorXdual x ( 5 ); // the input vector x with 5 variables x << 1 , 2 , 3 , 4 , 5 ; // x = [1, 2, 3, 4, 5] VectorXdual F ; // the output vector F = f(x) evaluated together with Jacobian matrix below MatrixXd J = jacobian ( f , wrt ( x ), at ( x ), F ); // evaluate the output vector F and the Jacobian matrix dF/dx cout << \"F = \\n \" << F << endl ; // print the evaluated output vector F cout << \"J = \\n \" << J << endl ; // print the evaluated Jacobian matrix dF/dx }","title":"Jacobian of a vector function"},{"location":"tutorials/#jacobian-of-a-vector-function-with-parameters","text":"// C++ includes #include <iostream> using namespace std ; // Eigen includes #include <Eigen/Core> using namespace Eigen ; // autodiff include #include <autodiff/forward.hpp> #include <autodiff/forward/eigen.hpp> using namespace autodiff ; // The vector function with parameters for which the Jacobian is needed VectorXdual f ( const VectorXdual & x , const VectorXdual & p ) { return x * exp ( p . sum ()); } int main () { VectorXdual x ( 5 ); // the input vector x with 5 variables x << 1 , 2 , 3 , 4 , 5 ; // x = [1, 2, 3, 4, 5] VectorXdual p ( 3 ); // the input parameter vector p with 3 variables p << 1 , 2 , 3 ; // p = [1, 2, 3] VectorXdual F ; // the output vector F = f(x, p) evaluated together with Jacobian below MatrixXd Jx = jacobian ( f , wrt ( x ), at ( x , p ), F ); // evaluate the function and the Jacobian matrix dF/dx MatrixXd Jp = jacobian ( f , wrt ( p ), at ( x , p ), F ); // evaluate the function and the Jacobian matrix dF/dp MatrixXd Jpx = jacobian ( f , wrtpack ( p , x ), at ( x , p ), F ); // evaluate the function and the Jacobian matrix [dF/dp, dF/dx] cout << \"F = \\n \" << F << endl ; // print the evaluated output vector F cout << \"Jx = \\n \" << Jx << endl ; // print the evaluated Jacobian matrix dF/dx cout << \"Jp = \\n \" << Jp << endl ; // print the evaluated Jacobian matrix dF/dp cout << \"Jpx = \\n \" << Jpx << endl ; // print the evaluated Jacobian matrix [dF/dp, dF/dx] }","title":"Jacobian of a vector function with parameters"},{"location":"tutorials/#higher-order-derivatives-of-a-multi-variable-function","text":"// C++ includes #include <iostream> using namespace std ; // autodiff include #include <autodiff/forward.hpp> using namespace autodiff ; // Define a 2nd order dual type using HigherOrderDual<N> construct. using dual2nd = HigherOrderDual < 2 > ; // The single-variable function for which derivatives are needed dual2nd f ( dual2nd x , dual2nd y ) { return 1 + x + y + x * y + y / x + log ( x / y ); } int main () { dual2nd x = 2.0 ; // the input variable x dual2nd y = 1.0 ; // the input variable y dual2nd u = f ( x , y ); // the output variable u dual ux = derivative ( f , wrt ( x ), at ( x , y )); // evaluate the derivative du/dx dual uy = derivative ( f , wrt ( y ), at ( x , y )); // evaluate the derivative du/dy double uxx = derivative ( f , wrt < 2 > ( x ), at ( x , y )); // evaluate the derivative d\u00b2u/dxdx double uxy = derivative ( f , wrt ( x , y ), at ( x , y )); // evaluate the derivative d\u00b2u/dxdy double uyx = derivative ( f , wrt ( y , x ), at ( x , y )); // evaluate the derivative d\u00b2u/dydx double uyy = derivative ( f , wrt < 2 > ( y ), at ( x , y )); // evaluate the derivative d\u00b2u/dydy cout << \"u = \" << u << endl ; // print the evaluated output u cout << \"ux = \" << ux << endl ; // print the evaluated derivative du/dx cout << \"uy = \" << uy << endl ; // print the evaluated derivative du/dy cout << \"uxx = \" << uxx << endl ; // print the evaluated derivative d\u00b2u/dxdx cout << \"uxy = \" << uxy << endl ; // print the evaluated derivative d\u00b2u/dxdy cout << \"uyx = \" << uyx << endl ; // print the evaluated derivative d\u00b2u/dydx cout << \"uyy = \" << uyy << endl ; // print the evaluated derivative d\u00b2u/dydy }","title":"Higher-order derivatives of a multi-variable function"},{"location":"tutorials/#reverse-mode","text":"","title":"Reverse mode"},{"location":"tutorials/#single-variable-function_1","text":"// C++ includes #include <iostream> using namespace std ; // autodiff include #include <autodiff/reverse.hpp> using namespace autodiff ; // The single-variable function for which derivatives are needed var f ( var x ) { return 1 + x + x * x + 1 / x + log ( x ); } int main () { var x = 2.0 ; // the input variable x var u = f ( x ); // the output variable u auto [ ux ] = derivatives ( u , wrt ( x )); // evaluate the derivative of u with respect to x cout << \"u = \" << u << endl ; // print the evaluated output variable u cout << \"ux = \" << ux << endl ; // print the evaluated derivative ux }","title":"Single-variable function"},{"location":"tutorials/#multi-variable-function_1","text":"// C++ includes #include <iostream> using namespace std ; // autodiff include #include <autodiff/reverse.hpp> using namespace autodiff ; // The multi-variable function for which derivatives are needed var f ( var x , var y , var z ) { return 1 + x + y + z + x * y + y * z + x * z + x * y * z + exp ( x / y + y / z ); } int main () { var x = 1.0 ; // the input variable x var y = 2.0 ; // the input variable y var z = 3.0 ; // the input variable z var u = f ( x , y , z ); // the output variable u auto [ ux , uy , uz ] = derivatives ( u , wrt ( x , y , z )); // evaluate the derivatives of u with respect to x, y, z cout << \"u = \" << u << endl ; // print the evaluated output u cout << \"ux = \" << ux << endl ; // print the evaluated derivative ux cout << \"uy = \" << uy << endl ; // print the evaluated derivative uy cout << \"uz = \" << uz << endl ; // print the evaluated derivative uz }","title":"Multi-variable function"},{"location":"tutorials/#multi-variable-function-with-parameters_1","text":"// C++ includes #include <iostream> using namespace std ; // autodiff include #include <autodiff/reverse.hpp> using namespace autodiff ; // A type defining parameters for a function of interest struct Params { var a ; var b ; var c ; }; // The function that depends on parameters for which derivatives are needed var f ( var x , const Params & params ) { return params . a * sin ( x ) + params . b * cos ( x ) + params . c * sin ( x ) * cos ( x ); } int main () { Params params ; // initialize the parameter variables params . a = 1.0 ; // the parameter a of type var, not double! params . b = 2.0 ; // the parameter b of type var, not double! params . c = 3.0 ; // the parameter c of type var, not double! var x = 0.5 ; // the input variable x var u = f ( x , params ); // the output variable u auto [ ux , ua , ub , uc ] = derivatives ( u , wrt ( x , params . a , params . b , params . c )); // evaluate the derivatives of u with respect to x and parameters a, b, c cout << \"u = \" << u << endl ; // print the evaluated output u cout << \"ux = \" << ux << endl ; // print the evaluated derivative du/dx cout << \"ua = \" << ua << endl ; // print the evaluated derivative du/da cout << \"ub = \" << ub << endl ; // print the evaluated derivative du/db cout << \"uc = \" << uc << endl ; // print the evaluated derivative du/dc }","title":"Multi-variable function with parameters"},{"location":"tutorials/#gradient-of-a-scalar-function_1","text":"// C++ includes #include <iostream> using namespace std ; // Eigen includes #include <Eigen/Core> using namespace Eigen ; // autodiff include #include <autodiff/reverse.hpp> #include <autodiff/reverse/eigen.hpp> using namespace autodiff ; // The scalar function for which the gradient is needed var f ( const VectorXvar & x ) { return sqrt ( x . cwiseProduct ( x ). sum ()); // sqrt(sum([x(i) * x(i) for i = 1:5])) } int main () { VectorXvar x ( 5 ); // the input vector x with 5 variables x << 1 , 2 , 3 , 4 , 5 ; // x = [1, 2, 3, 4, 5] var y = f ( x ); // the output variable y VectorXd dydx = gradient ( y , x ); // evaluate the gradient vector dy/dx cout << \"y = \" << y << endl ; // print the evaluated output y cout << \"dy/dx = \\n \" << dydx << endl ; // print the evaluated gradient vector dy/dx }","title":"Gradient of a scalar function"},{"location":"tutorials/#hessian-of-a-scalar-function","text":"// C++ includes #include <iostream> using namespace std ; // Eigen includes #include <Eigen/Core> using namespace Eigen ; // autodiff include #include <autodiff/reverse.hpp> #include <autodiff/reverse/eigen.hpp> using namespace autodiff ; // The scalar function for which the gradient is needed var f ( const VectorXvar & x ) { return sqrt ( x . cwiseProduct ( x ). sum ()); // sqrt(sum([x(i) * x(i) for i = 1:5])) } int main () { VectorXvar x ( 5 ); // the input vector x with 5 variables x << 1 , 2 , 3 , 4 , 5 ; // x = [1, 2, 3, 4, 5] var u = f ( x ); // the output variable u VectorXd g ; // the gradient vector to be computed in method `hessian` MatrixXd H = hessian ( u , x , g ); // evaluate the Hessian matrix H and the gradient vector g of u cout << \"u = \" << u << endl ; // print the evaluated output variable u cout << \"g = \\n \" << g << endl ; // print the evaluated gradient vector of u cout << \"H = \\n \" << H << endl ; // print the evaluated Hessian matrix of u }","title":"Hessian of a scalar function"},{"location":"tutorials/#higher-order-derivatives-of-a-single-variable-function","text":"// C++ includes #include <iostream> using namespace std ; // autodiff include #include <autodiff/reverse.hpp> using namespace autodiff ; int main () { var x = 0.5 ; // the input variable x var u = sin ( x ) * cos ( x ); // the output variable u auto [ ux ] = derivativesx ( u , wrt ( x )); // evaluate the first order derivatives of u auto [ uxx ] = derivativesx ( ux , wrt ( x )); // evaluate the second order derivatives of ux cout << \"u = \" << u << endl ; // print the evaluated output variable u cout << \"ux(autodiff) = \" << ux << endl ; // print the evaluated first order derivative ux cout << \"ux(exact) = \" << 1 - 2 * sin ( x ) * sin ( x ) << endl ; // print the exact first order derivative ux cout << \"uxx(autodiff) = \" << uxx << endl ; // print the evaluated second order derivative uxx cout << \"uxx(exact) = \" << -4 * cos ( x ) * sin ( x ) << endl ; // print the exact second order derivative uxx } /*=============================================================================== Output: ================================================================================= u = 0.420735 ux(autodiff) = 0.540302 ux(exact) = 0.540302 uxx(autodiff) = -1.68294 uxx(exact) = -1.68294 ===============================================================================*/","title":"Higher-order derivatives of a single-variable function"},{"location":"tutorials/#higher-order-derivatives-of-a-multi-variable-function_1","text":"// C++ includes #include <iostream> using namespace std ; // autodiff include #include <autodiff/reverse.hpp> using namespace autodiff ; int main () { var x = 1.0 ; // the input variable x var y = 0.5 ; // the input variable y var z = 2.0 ; // the input variable z var u = x * log ( y ) * exp ( z ); // the output variable u auto [ ux , uy , uz ] = derivativesx ( u , wrt ( x , y , z )); // evaluate the derivatives of u with respect to x, y, z. auto [ uxx , uxy , uxz ] = derivativesx ( ux , wrt ( x , y , z )); // evaluate the derivatives of ux with respect to x, y, z. auto [ uyx , uyy , uyz ] = derivativesx ( uy , wrt ( x , y , z )); // evaluate the derivatives of uy with respect to x, y, z. auto [ uzx , uzy , uzz ] = derivativesx ( uz , wrt ( x , y , z )); // evaluate the derivatives of uz with respect to x, y, z. cout << \"u = \" << u << endl ; // print the evaluated output variable u cout << \"ux = \" << ux << endl ; // print the evaluated first order derivative ux cout << \"uy = \" << uy << endl ; // print the evaluated first order derivative uy cout << \"uz = \" << uz << endl ; // print the evaluated first order derivative uz cout << \"uxx = \" << uxx << endl ; // print the evaluated second order derivative uxx cout << \"uxy = \" << uxy << endl ; // print the evaluated second order derivative uxy cout << \"uxz = \" << uxz << endl ; // print the evaluated second order derivative uxz cout << \"uyx = \" << uyx << endl ; // print the evaluated second order derivative uyx cout << \"uyy = \" << uyy << endl ; // print the evaluated second order derivative uyy cout << \"uyz = \" << uyz << endl ; // print the evaluated second order derivative uyz cout << \"uzx = \" << uzx << endl ; // print the evaluated second order derivative uzx cout << \"uzy = \" << uzy << endl ; // print the evaluated second order derivative uzy cout << \"uzz = \" << uzz << endl ; // print the evaluated second order derivative uzz }","title":"Higher-order derivatives of a multi-variable function"},{"location":"tutorials/#integration-with-cmake-based-projects","text":"Integrating autodiff in a CMake-based project is very simple as shown next. Let's assume our CMake-based project consists of two files: main.cpp and CMakeLists.txt , whose contents are shown below: main.cpp #include <iostream> using namespace std ; #include <autodiff/forward.hpp> using namespace autodiff ; dual f ( dual x ) { return 1 + x + x * x + 1 / x + log ( x ); } int main () { dual x = 1.0 ; dual u = f ( x ); double dudx = derivative ( f , wrt ( x ), at ( x )); cout << \"u = \" << u << endl ; cout << \"du/dx = \" << dudx << endl ; } CMakeLists.txt cmake_minimum_required ( VERSION 3.0 ) project ( app ) find_package ( autodiff ) add_executable ( app main.cpp ) target_link_libraries ( app autodiff::autodiff ) In the CMakeLists.txt file, note the use of the command: find_package ( autodiff ) to find the header files of the autodiff library, and the command: target_link_libraries ( app autodiff::autodiff ) to link the executable target app against the autodiff library ( autodiff::autodiff ) using CMake's modern target-based design. To build the application, do: mkdir build && cd build cmake .. -DCMAKE_PREFIX_PATH = /path/to/autodiff/install/dir make Attention If autodiff has been installed system-wide, then the CMake argument CMAKE_PREFIX_PATH should not be needed. Otherwise, you will need to specify where autodiff is installed in your machine. For example: cmake .. -DCMAKE_PREFIX_PATH = $HOME /local assuming directory $HOME/local is where autodiff was installed to, which should then contain the following directory: $HOME/local/include/autodiff/ where the autodiff header files are located. To execute the application, do: ./app","title":"Integration with CMake-based projects"}]}