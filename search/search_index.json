{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#autodiff","title":"autodiff","text":"<p>autodiff is a C++17 library that uses modern and advanced programming techniques to enable automatic computation of derivatives in an efficient, easy, and intuitive way.</p> <p>We welcome you to use autodiff and recommend to us any improvements you think are necessary. You may want to do so by chatting with us on our Gitter Community Channel and/or by making proposals by creating a GitHub issue.</p> <p>Attention</p> <p>There are breaking changes in autodiff v0.6! Please check the updated Tutorials page to learn how to correctly include the header files, the slightly changed API, and the new autodiff type <code>real</code> designed for efficient higher-order directional derivatives. This is in contrast to the <code>dual</code> type, which is designed for higher-order cross derivatives.</p>"},{"location":"#demonstration","title":"Demonstration","text":"<p>Consider the following function f(x, y, z):</p> <pre><code>double f(double x, double y, double z)\n{\n    return (x + y + z) * exp(x * y * z);\n}\n</code></pre> <p>which we use use to evaluate the variable u = f(x, y, z):</p> <pre><code>double x = 1.0;\ndouble y = 2.0;\ndouble z = 3.0;\ndouble u = f(x, y, z);\n</code></pre> <p>How can we minimally transform this code so that not only u, but also its derivatives \u2202u/\u2202x, \u2202u/\u2202y, and \u2202u/\u2202z, can be computed?</p> <p>The next two sections present how this can be achieved using two automatic differentiation algorithms implemented in autodiff: forward mode and reverse mode.</p>"},{"location":"#forward-mode","title":"Forward mode","text":"<p>In a forward mode automatic differentiation algorithm, both output variables and one or more of their derivatives are computed together. For example, the function evaluation f(x, y, z) can be transformed in a way that it will not only produce the value of u, the output variable, but also one or more of its derivatives (\u2202u/\u2202x,  \u2202u/\u2202y, \u2202u/\u2202z) with respect to the input variables (x, y, z).</p> <p>Enabling forward automatic differentiation for the calculation of derivatives using autodiff is relatively simple. For our previous function f, we only need to replace the floating-point type <code>double</code> with <code>autodiff::dual</code> for both input and output variables:</p> <pre><code>dual f(const dual&amp; x, const dual&amp; y, const dual&amp; z)\n{\n    return (x + y + z) * exp(x * y * z);\n}\n</code></pre> <p>We can now compute the derivatives \u2202u/\u2202x,  \u2202u/\u2202y, and \u2202u/\u2202z as follows:</p> <pre><code>dual x = 1.0;\ndual y = 2.0;\ndual z = 3.0;\ndual u = f(x, y, z);\n\ndouble ux = derivative(f, wrt(x), at(x, y, z));\ndouble uy = derivative(f, wrt(y), at(x, y, z));\ndouble uz = derivative(f, wrt(z), at(x, y, z));\n</code></pre> <p>The auxiliary function <code>autodiff::wrt</code>, an acronym for with respect to, is used to indicate which input variable (x, y, z) is the selected one to compute the partial derivative of f. The auxiliary function <code>autodiff::at</code> is used to indicate where (at which values of its parameters) the derivative of f is evaluated.</p>"},{"location":"#reverse-mode","title":"Reverse mode","text":"<p>In a reverse mode automatic differentiation algorithm, the output variable of a function is evaluated first. During this function evaluation, all mathematical operations between the input variables are \"recorded\" in an expression tree. By traversing this tree from top-level (output variable as the root node) to bottom-level (input variables as the leaf nodes), it is possible to compute the contribution of each branch on the derivatives of the output variable with respect to input variables.</p> <p></p> <p>Thus, a single pass in a reverse mode calculation computes all derivatives, in contrast with forward mode, which requires one pass for each input variable. Note, however, that it is possible to change the behavior of a forward pass so that many (perhaps even all) derivatives of an output variable are computed simultaneously (e.g., in a single forward pass, \u2202u/\u2202x,  \u2202u/\u2202y, and \u2202u/\u2202z are evaluated together with u, in contrast with three forward passes, each one computing the individual derivatives).</p> <p>Similar as before, we can use autodiff to enable reverse automatic differentiation for our function f by simply replacing type <code>double</code> with <code>autodiff::var</code> as follows:</p> <pre><code>var f(var x, var y, var z)\n{\n    return (x + y + z) * exp(x * y * z);\n}\n</code></pre> <p>The code below demonstrates how the derivatives \u2202u/\u2202x,  \u2202u/\u2202y, and \u2202u/\u2202z can be calculated:</p> <pre><code>var x = 1.0;\nvar y = 2.0;\nvar z = 3.0;\nvar u = f(x, y, z);\n\nauto [ux, uy, uz] = derivatives(u, wrt(x, y, z));\n</code></pre> <p>The function <code>autodiff::derivatives</code> will traverse the expression tree stored in variable <code>u</code> and compute all its derivatives with respect to the input variables (x, y, z).</p>"},{"location":"#get-in-touch","title":"Get in touch!","text":"<p>Contact us on Gitter or via a GitHub Discussion if you need support and assistance when using autodiff. If you would like to report a bug, then please create a new GitHub Issue.</p>"},{"location":"about/","title":"About","text":""},{"location":"about/#philosophy","title":"Philosophy","text":"<p>autodiff aims to be both efficient and easy to use C++ library for automatic differentiation. If you appreciate how it has been developed so far, and want to contribute, you are most welcome to join us in its development. And if you dislike it, please let us know how we can improve!</p> <p>You can contact us using our Gitter Community Channel or by creating a new GitHub Discussion. If you would like to report a bug, then please create a new GitHub Issue.</p>"},{"location":"about/#license","title":"License","text":"<p>MIT License</p> <p>Copyright \u00a9 2018\u20132021 Allan Leal</p> <p>Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:</p> <p>The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.</p> <p>THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.</p>"},{"location":"faq/","title":"FAQ","text":""},{"location":"faq/#why-should-i-consider-automatic-differentiation","title":"Why should I consider automatic differentiation?","text":"<p>The following are the reasons why you should consider automatic differentiation in your computational project:</p> <ul> <li>your functions are extremely complicated;</li> <li>manually implementing analytical derivatives is a tedious and error-prone   task; and</li> <li>computing derivatives using finite differences can be inaccurate and   inefficient.</li> </ul>"},{"location":"faq/#what-is-the-difference-between-reverse-and-forward-modes","title":"What is the difference between reverse and forward modes?","text":"<p>Here is a brief, practical, and qualitative discussion on the differences between these two automatic differentiation algorithms.</p> <p>In a forward mode algorithm, each function evaluation produces not only its output value but also its derivative. The evaluation of a vector function, for example, computes both the output vector as well as the derivative of this vector, in general, with respect to one of the input variables (e.g., \u2202u/\u2202x, \u2202u/\u2202y). However, the forward mode algorithm can also be used to compute directional derivatives. In this case, the derivative of the output vector with respect to a given direction vector is performed. We could say that, fundamentally, the forward mode always computes directional derivatives. The simplest case of a derivative with respect to a single input variable corresponds thus with a directional derivative whose direction vector is a unit vector along the input variable of interest (e.g., the unit vector along x, or along y, and so forth).</p> <p>In a reverse mode algorithm, each function evaluation produces a complete expression tree that contains the sequence of all mathematical operations between the input variables to produce the output variable (a scalar). Once this expression tree is constructed, it is then used to compute the derivatives of the scalar output variable with respect to all input variables.</p>"},{"location":"faq/#is-one-algorithm-always-faster-than-another","title":"Is one algorithm always faster than another?","text":"<p>Even though the reverse mode algorithm requires a single function evaluation and all derivatives are subsequently computed in a single pass, as the constructed expression tree is traversed, the forward algorithm can still be a much more efficient choice for your application, despite its multiple, repeated function evaluations, one for each input variable. This is because the implementation of the forward mode algorithm in autodiff uses template meta-programming techniques to avoid as much as possible temporary memory allocations and to optimize the calculations in certain cases. The reverse mode algorithm, on the other hand, requires the construction of an expression tree at runtime, and, for this, several dynamic memory allocations are needed, which can be costly. We plan to implement alternative versions of this algorithm, in which memory allocation could be done in advance to decrease the number of subsequent allocations. This, however, will require a slightly more complicated usage than it is already provided by the reverse mode algorithm implemented in autodiff.</p>"},{"location":"faq/#which-automatic-differentiation-algorithm-should-i-use","title":"Which automatic differentiation algorithm should I use?","text":"<p>Ideally, you should try both algorithms for your specific needs, benchmark them, and then make an informed decision about which one to use.</p> <p>If you're in a hurry, consider:</p> <p>forward mode: if you have a vector function, or a scalar function with not many input variables.</p> <p>reverse mode: if you have a scalar function with many (thousands or more) input variables.</p> <p>Have in mind this is a very simplistic rule, and you should definitely try both algorithms whenever possible, since the forward mode could still be faster than reverse mode even when many input variables are considered for a function of interest.</p>"},{"location":"faq/#how-do-i-cite-autodiff","title":"How do I cite autodiff?","text":"<p>We appreciate your intention of citing autodiff in your publications. Please use the following BibTeX reference entry:</p> <pre><code>@misc{autodiff,\n    author = {Leal, Allan M. M.},\n    title = {autodiff, a modern, fast and expressive {C++} library for automatic differentiation},\n    url = {https://autodiff.github.io},\n    howpublished = {\\texttt{https://autodiff.github.io}},\n    year = {2018}\n}\n</code></pre> <p>This should produce a formatted citation that looks more or less the following:</p> <p>Leal, A.M.M. (2018). autodiff, a modern, fast and expressive C++ library for automatic differentiation. https://autodiff.github.io</p> <p>Please ensure the website URL is displayed.</p>"},{"location":"installation/","title":"Installation","text":"<p>Installing autodiff is easy, since it is a header-only library. Follow the steps below.</p>"},{"location":"installation/#installation-using-conda","title":"Installation using Conda","text":"<p>If you have Anaconda or Miniconda installed (with Python 3.7+), you can install autodiff with a single command:</p> <pre><code>conda install conda-forge::autodiff\n</code></pre> <p>This will install autodiff in the conda environment that is active at the moment (e.g., the default conda environment is named <code>base</code>).</p>"},{"location":"installation/#development-environment","title":"Development Environment","text":"<p>Anaconda or Miniconda can be used to create a full development environment using conda-devenv.</p> <p>You can install conda-devenv and build the development environment defined in <code>environment.devenv.yml</code> as follows: <pre><code>conda install conda-devenv\nconda devenv\n</code></pre></p> <p>The above commands will produce a {{ conda }} environment called autodiff with all the dependencies installed. You can activate the development environment with <pre><code>conda activate autodiff\n</code></pre> and follow the instructions below to build autodiff from source.</p>"},{"location":"installation/#installation-using-cmake","title":"Installation using CMake","text":"<p>If you have <code>cmake</code> installed in your system, you can then not only install autodiff but also build its examples and tests. First, you'll need to download it by either git cloning its GitHub repository:</p> <pre><code>git clone https://github.com/autodiff/autodiff\n</code></pre> <p>or by clicking here to start the download of a zip file, which you should extract to a directory of your choice.</p> <p>Then, execute the following steps (assuming you are in the root of the source code directory of autodiff!):</p> <pre><code>mkdir .build &amp;&amp; cd .build\ncmake ..\ncmake --build . --target install\n</code></pre> <p>Attention</p> <p>We assume above that you are in the root of the source code directory, under <code>autodiff</code>! The build directory will be created at <code>autodiff/.build</code>. We use <code>.build</code> here instead of the more usual <code>build</code> because there is a file called <code>BUILD</code> that provides support to Bazel build system. In operating systems that treats file and directory names as case insensitive, you may not be able to create a <code>build</code> directory.</p> <p>The previous installation commands will require administrative rights in most systems. To install autodiff locally, use:</p> <pre><code>cmake .. -DCMAKE_INSTALL_PREFIX=/some/local/dir\n</code></pre>"},{"location":"installation/#build-using-bazel","title":"Build Using Bazel","text":"<p>bazel can be used as build system.</p> <p>Attention</p> <p>bazel support is part of the community effort. Therefore, it is not officially supported.</p> <p>Currently, running the unit tests and installing the library using bazel is not supported.</p>"},{"location":"installation/#build-and-run-examples","title":"Build and Run Examples","text":"<p>Build all examples using bazel:</p> <pre><code>bazel build //examples/forward:all\nbazel build //examples/reverse:all\n</code></pre> <p>Run all examples using bazel and display their output:</p> <pre><code>bazel test //examples/forward:all --test_output=all\nbazel test //examples/reverse:all --test_output=all\n</code></pre>"},{"location":"installation/#installation-by-copying","title":"Installation by copying","text":"<p>Assuming the git cloned repository or the extracted source code resides in a directory named <code>autodiff</code>, you can now copy the sub-directory <code>autodiff/autodiff</code> to somewhere in your project directory and directly use autodiff.</p>"},{"location":"installation/#installation-failed-what-do-i-do","title":"Installation failed. What do I do?","text":"<p>Discuss with us first your issue on our Gitter Community Channel. We may be able to respond more quickly there on how to sort out your issue. If bug fixes are indeed required, we'll kindly ask you to create a GitHub issue, in which you can provide more details about the issue and keep track of our progress on fixing it. You are also welcome to recommend us installation improvements in the Gitter channel.</p>"},{"location":"tutorials/","title":"Tutorials","text":"<p>We present here some examples demonstrating the use of autodiff for computing different types of derivatives. We welcome any contribution towards improving and expanding this list of examples. We would also love to hear your suggestions on how to better demonstrate the capabilities of autodiff.</p>"},{"location":"tutorials/#forward-mode","title":"Forward mode","text":""},{"location":"tutorials/#derivatives-of-a-single-variable-function","title":"Derivatives of a single-variable function","text":"<pre><code>// C++ includes\n#include &lt;iostream&gt;\n\n// autodiff include\n#include &lt;autodiff/forward/dual.hpp&gt;\nusing namespace autodiff;\n\n// The single-variable function for which derivatives are needed\ndual f(dual x)\n{\n    return 1 + x + x*x + 1/x + log(x);\n}\n\nint main()\n{\n    dual x = 2.0;                                 // the input variable x\n    dual u = f(x);                                // the output variable u\n\n    double dudx = derivative(f, wrt(x), at(x));   // evaluate the derivative du/dx\n\n    std::cout &lt;&lt; \"u = \" &lt;&lt; u &lt;&lt; std::endl;        // print the evaluated output u\n    std::cout &lt;&lt; \"du/dx = \" &lt;&lt; dudx &lt;&lt; std::endl; // print the evaluated derivative du/dx\n}\n</code></pre>"},{"location":"tutorials/#derivatives-of-a-single-variable-function-using-a-custom-scalar-complex","title":"Derivatives of a single-variable function using a custom scalar (complex)","text":"<pre><code>// C++ includes\n#include &lt;iostream&gt;\n#include &lt;complex&gt;\nusing namespace std;\n\n// autodiff include\n#include &lt;autodiff/forward/dual.hpp&gt;\nusing namespace autodiff;\n\n// Specialize isArithmetic for complex to make it compatible with dual\nnamespace autodiff::detail {\n\ntemplate&lt;typename T&gt;\nstruct ArithmeticTraits&lt;complex&lt;T&gt;&gt; : ArithmeticTraits&lt;T&gt; {};\n\n} // autodiff::detail\n\nusing cxdual = Dual&lt;complex&lt;double&gt;, complex&lt;double&gt;&gt;;\n\n// The single-variable function for which derivatives are needed\ncxdual f(cxdual x)\n{\n    return 1 + x + x*x + 1/x + log(x);\n}\n\nint main()\n{\n    cxdual x = 2.0;   // the input variable x\n    cxdual u = f(x);  // the output variable u\n\n    cxdual dudx = derivative(f, wrt(x), at(x));  // evaluate the derivative du/dx\n\n    cout &lt;&lt; \"u = \" &lt;&lt; u &lt;&lt; endl;         // print the evaluated output u\n    cout &lt;&lt; \"du/dx = \" &lt;&lt; dudx &lt;&lt; endl;  // print the evaluated derivative du/dx\n}\n</code></pre>"},{"location":"tutorials/#derivatives-of-a-multi-variable-function","title":"Derivatives of a multi-variable function","text":"<pre><code>// C++ includes\n#include &lt;iostream&gt;\n\n// autodiff include\n#include &lt;autodiff/forward/dual.hpp&gt;\nusing namespace autodiff;\n\n// The multi-variable function for which derivatives are needed\ndual f(dual x, dual y, dual z)\n{\n    return 1 + x + y + z + x*y + y*z + x*z + x*y*z + exp(x/y + y/z);\n}\n\nint main()\n{\n    dual x = 1.0;\n    dual y = 2.0;\n    dual z = 3.0;\n\n    dual u = f(x, y, z);\n\n    double dudx = derivative(f, wrt(x), at(x, y, z));\n    double dudy = derivative(f, wrt(y), at(x, y, z));\n    double dudz = derivative(f, wrt(z), at(x, y, z));\n\n    std::cout &lt;&lt; \"u = \" &lt;&lt; u &lt;&lt; std::endl;         // print the evaluated output u = f(x, y, z)\n    std::cout &lt;&lt; \"du/dx = \" &lt;&lt; dudx &lt;&lt; std::endl;  // print the evaluated derivative du/dx\n    std::cout &lt;&lt; \"du/dy = \" &lt;&lt; dudy &lt;&lt; std::endl;  // print the evaluated derivative du/dy\n    std::cout &lt;&lt; \"du/dz = \" &lt;&lt; dudz &lt;&lt; std::endl;  // print the evaluated derivative du/dz\n}\n</code></pre>"},{"location":"tutorials/#derivatives-of-a-multi-variable-function-with-parameters","title":"Derivatives of a multi-variable function with parameters","text":"<pre><code>// C++ includes\n#include &lt;iostream&gt;\n\n// autodiff include\n#include &lt;autodiff/forward/dual.hpp&gt;\nusing namespace autodiff;\n\n// A type defining parameters for a function of interest\nstruct Params\n{\n    dual a;\n    dual b;\n    dual c;\n};\n\n// The function that depends on parameters for which derivatives are needed\ndual f(dual x, const Params&amp; params)\n{\n    return params.a * sin(x) + params.b * cos(x) + params.c * sin(x)*cos(x);\n}\n\nint main()\n{\n    Params params;   // initialize the parameter variables\n    params.a = 1.0;  // the parameter a of type dual, not double!\n    params.b = 2.0;  // the parameter b of type dual, not double!\n    params.c = 3.0;  // the parameter c of type dual, not double!\n\n    dual x = 0.5;  // the input variable x\n\n    dual u = f(x, params);  // the output variable u\n\n    double dudx = derivative(f, wrt(x), at(x, params));        // evaluate the derivative du/dx\n    double duda = derivative(f, wrt(params.a), at(x, params)); // evaluate the derivative du/da\n    double dudb = derivative(f, wrt(params.b), at(x, params)); // evaluate the derivative du/db\n    double dudc = derivative(f, wrt(params.c), at(x, params)); // evaluate the derivative du/dc\n\n    std::cout &lt;&lt; \"u = \" &lt;&lt; u &lt;&lt; std::endl;         // print the evaluated output u\n    std::cout &lt;&lt; \"du/dx = \" &lt;&lt; dudx &lt;&lt; std::endl;  // print the evaluated derivative du/dx\n    std::cout &lt;&lt; \"du/da = \" &lt;&lt; duda &lt;&lt; std::endl;  // print the evaluated derivative du/da\n    std::cout &lt;&lt; \"du/db = \" &lt;&lt; dudb &lt;&lt; std::endl;  // print the evaluated derivative du/db\n    std::cout &lt;&lt; \"du/dc = \" &lt;&lt; dudc &lt;&lt; std::endl;  // print the evaluated derivative du/dc\n}\n\n/*-------------------------------------------------------------------------------------------------\n=== Note ===\n---------------------------------------------------------------------------------------------------\nThis example would also work if real was used instead of dual. Should you\nneed higher-order cross derivatives, however, e.g.,:\n\n    double d2udxda = derivative(f, wrt(x, params.a), at(x, params));\n\nthen higher-order dual types are the right choicesince real types are\noptimally designed for higher-order directional derivatives.\n-------------------------------------------------------------------------------------------------*/\n</code></pre>"},{"location":"tutorials/#derivatives-of-a-multi-variable-function-that-also-relies-on-analytical-derivatives","title":"Derivatives of a multi-variable function that also relies on analytical derivatives","text":"<pre><code>// C++ includes\n#include &lt;iostream&gt;\n\n// autodiff include\n#include &lt;autodiff/forward/dual.hpp&gt;\nusing namespace autodiff;\n\n// Define functions A, Ax, Ay using double; analytical derivatives are available.\ndouble  A(double x, double y) { return x*y; }\ndouble Ax(double x, double y) { return x; }\ndouble Ay(double x, double y) { return y; }\n\n// Define functions B, Bx, By using double; analytical derivatives are available.\ndouble  B(double x, double y) { return x + y; }\ndouble Bx(double x, double y) { return 1.0; }\ndouble By(double x, double y) { return 1.0; }\n\n// Wrap A into Adual function so that it can be used within autodiff-enabled codes.\ndual Adual(dual const&amp; x, dual const&amp; y)\n{\n    dual res = A(x.val, y.val);\n\n    if(x.grad != 0.0)\n        res.grad += x.grad * Ax(x.val, y.val);\n\n    if(y.grad != 0.0)\n        res.grad += y.grad * Ay(x.val, y.val);\n\n    return res;\n}\n\n// Wrap B into Bdual function so that it can be used within autodiff-enabled codes.\ndual Bdual(dual const&amp; x, dual const&amp; y)\n{\n    dual res = B(x.val, y.val);\n\n    if(x.grad != 0.0)\n        res.grad += x.grad * Bx(x.val, y.val);\n\n    if(y.grad != 0.0)\n        res.grad += y.grad * By(x.val, y.val);\n\n    return res;\n}\n\n// Define autodiff-enabled C function that relies on Adual and Bdual\ndual C(dual const&amp; x, dual const&amp; y)\n{\n    const auto A = Adual(x, y);\n    const auto B = Bdual(x, y);\n    return A*A + B;\n}\n\nint main()\n{\n    dual x = 1.0;\n    dual y = 2.0;\n\n    auto C0 = C(x, y);\n\n    // Compute derivatives of C with respect to x and y using autodiff!\n    auto Cx = derivative(C, wrt(x), at(x, y));\n    auto Cy = derivative(C, wrt(y), at(x, y));\n\n    // Compute expected analytical derivatives of C with respect to x and y\n    auto x0 = x.val;\n    auto y0 = y.val;\n    auto expectedCx = 2.0*A(x0, y0)*Ax(x0, y0) + Bx(x0, y0);\n    auto expectedCy = 2.0*A(x0, y0)*Ay(x0, y0) + By(x0, y0);\n\n    std::cout &lt;&lt; \"C0 = \" &lt;&lt; C0 &lt;&lt; \"\\n\";\n\n    std::cout &lt;&lt; \"Cx(computed) = \" &lt;&lt; Cx &lt;&lt; \"\\n\";\n    std::cout &lt;&lt; \"Cx(expected) = \" &lt;&lt; expectedCx &lt;&lt; \"\\n\";\n\n    std::cout &lt;&lt; \"Cy(computed) = \" &lt;&lt; Cy &lt;&lt; \"\\n\";\n    std::cout &lt;&lt; \"Cy(expected) = \" &lt;&lt; expectedCy &lt;&lt; \"\\n\";\n}\n\n// Output:\n// C0 = 7\n// Cx(computed) = 5\n// Cx(expected) = 5\n// Cy(computed) = 9\n// Cy(expected) = 9\n</code></pre>"},{"location":"tutorials/#gradient-vector-of-a-scalar-function","title":"Gradient vector of a scalar function","text":"<pre><code>// C++ includes\n#include &lt;iostream&gt;\n\n// autodiff include\n#include &lt;autodiff/forward/real.hpp&gt;\n#include &lt;autodiff/forward/real/eigen.hpp&gt;\nusing namespace autodiff;\n\n// The scalar function for which the gradient is needed\nreal f(const ArrayXreal&amp; x)\n{\n    return (x * x.exp()).sum(); // sum([xi * exp(xi) for i = 1:5])\n}\n\nint main()\n{\n    using Eigen::VectorXd;\n\n    ArrayXreal x(5);                            // the input array x with 5 variables\n    x &lt;&lt; 1, 2, 3, 4, 5;                         // x = [1, 2, 3, 4, 5]\n\n    real u;                                     // the output scalar u = f(x) evaluated together with gradient below\n\n    VectorXd g = gradient(f, wrt(x), at(x), u); // evaluate the function value u and its gradient vector g = du/dx\n\n    std::cout &lt;&lt; \"u = \" &lt;&lt; u &lt;&lt; std::endl;      // print the evaluated output u\n    std::cout &lt;&lt; \"g = \\n\" &lt;&lt; g &lt;&lt; std::endl;    // print the evaluated gradient vector g = du/dx\n}\n</code></pre>"},{"location":"tutorials/#gradient-vector-of-a-scalar-function-with-parameters","title":"Gradient vector of a scalar function with parameters","text":"<pre><code>// C++ includes\n#include &lt;iostream&gt;\n\n// autodiff include\n#include &lt;autodiff/forward/real.hpp&gt;\n#include &lt;autodiff/forward/real/eigen.hpp&gt;\nusing namespace autodiff;\n\n// The scalar function for which the gradient is needed\nreal f(const ArrayXreal&amp; x, const ArrayXreal&amp; p, const real&amp; q)\n{\n    return (x * x).sum() * p.sum() * exp(q); // sum([xi * xi for i = 1:5]) * sum(p) * exp(q)\n}\n\nint main()\n{\n    using Eigen::VectorXd;\n\n    ArrayXreal x(5);    // the input vector x with 5 variables\n    x &lt;&lt; 1, 2, 3, 4, 5; // x = [1, 2, 3, 4, 5]\n\n    ArrayXreal p(3);    // the input parameter vector p with 3 variables\n    p &lt;&lt; 1, 2, 3;       // p = [1, 2, 3]\n\n    real q = -2;        // the input parameter q as a single variable\n\n    real u;             // the output scalar u = f(x, p, q) evaluated together with gradient below\n\n    VectorXd gx   = gradient(f, wrt(x), at(x, p, q), u);       // evaluate the function value u and its gradient vector gx = du/dx\n    VectorXd gp   = gradient(f, wrt(p), at(x, p, q), u);       // evaluate the function value u and its gradient vector gp = du/dp\n    VectorXd gq   = gradient(f, wrt(q), at(x, p, q), u);       // evaluate the function value u and its gradient vector gq = du/dq\n    VectorXd gqpx = gradient(f, wrt(q, p, x), at(x, p, q), u); // evaluate the function value u and its gradient vector gqpx = [du/dq, du/dp, du/dx]\n\n    std::cout &lt;&lt; \"u = \" &lt;&lt; u &lt;&lt; std::endl;       // print the evaluated output u\n    std::cout &lt;&lt; \"gx = \\n\" &lt;&lt; gx &lt;&lt; std::endl;   // print the evaluated gradient vector gx = du/dx\n    std::cout &lt;&lt; \"gp = \\n\" &lt;&lt; gp &lt;&lt; std::endl;   // print the evaluated gradient vector gp = du/dp\n    std::cout &lt;&lt; \"gq = \\n\" &lt;&lt; gq &lt;&lt; std::endl;   // print the evaluated gradient vector gq = du/dq\n    std::cout &lt;&lt; \"gqpx = \\n\" &lt;&lt; gqpx &lt;&lt; std::endl; // print the evaluated gradient vector gqpx = [du/dq, du/dp, du/dx]\n}\n\n/*-------------------------------------------------------------------------------------------------\n=== Note ===\n---------------------------------------------------------------------------------------------------\nThis example would also work if dual was used instead. However, if gradient,\nJacobian, and directional derivatives are all you need, then real types are your\nbest option. You want to use dual types when evaluating higher-order cross\nderivatives, which is not supported for real types.\n-------------------------------------------------------------------------------------------------*/\n</code></pre>"},{"location":"tutorials/#jacobian-matrix-of-a-vector-function","title":"Jacobian matrix of a vector function","text":"<pre><code>// C++ includes\n#include &lt;iostream&gt;\n\n// autodiff include\n#include &lt;autodiff/forward/real.hpp&gt;\n#include &lt;autodiff/forward/real/eigen.hpp&gt;\nusing namespace autodiff;\n\n// The vector function for which the Jacobian is needed\nVectorXreal f(const VectorXreal&amp; x)\n{\n    return x * x.sum();\n}\n\nint main()\n{\n    using Eigen::MatrixXd;\n\n    VectorXreal x(5);                           // the input vector x with 5 variables\n    x &lt;&lt; 1, 2, 3, 4, 5;                         // x = [1, 2, 3, 4, 5]\n\n    VectorXreal F;                              // the output vector F = f(x) evaluated together with Jacobian matrix below\n\n    MatrixXd J = jacobian(f, wrt(x), at(x), F); // evaluate the output vector F and the Jacobian matrix dF/dx\n\n    std::cout &lt;&lt; \"F = \\n\" &lt;&lt; F &lt;&lt; std::endl;    // print the evaluated output vector F\n    std::cout &lt;&lt; \"J = \\n\" &lt;&lt; J &lt;&lt; std::endl;    // print the evaluated Jacobian matrix dF/dx\n}\n</code></pre>"},{"location":"tutorials/#jacobian-matrix-of-a-vector-function-with-parameters","title":"Jacobian matrix of a vector function with parameters","text":"<pre><code>// C++ includes\n#include &lt;iostream&gt;\n\n// autodiff include\n#include &lt;autodiff/forward/real.hpp&gt;\n#include &lt;autodiff/forward/real/eigen.hpp&gt;\nusing namespace autodiff;\n\n// The vector function with parameters for which the Jacobian is needed\nVectorXreal f(const VectorXreal&amp; x, const VectorXreal&amp; p, const real&amp; q)\n{\n    return x * p.sum() * exp(q);\n}\n\nint main()\n{\n    using Eigen::MatrixXd;\n\n    VectorXreal x(5);    // the input vector x with 5 variables\n    x &lt;&lt; 1, 2, 3, 4, 5;  // x = [1, 2, 3, 4, 5]\n\n    VectorXreal p(3);    // the input parameter vector p with 3 variables\n    p &lt;&lt; 1, 2, 3;        // p = [1, 2, 3]\n\n    real q = -2;         // the input parameter q as a single variable\n\n    VectorXreal F;       // the output vector F = f(x, p, q) evaluated together with Jacobian below\n\n    MatrixXd Jx   = jacobian(f, wrt(x), at(x, p, q), F);       // evaluate the function and the Jacobian matrix Jx = dF/dx\n    MatrixXd Jp   = jacobian(f, wrt(p), at(x, p, q), F);       // evaluate the function and the Jacobian matrix Jp = dF/dp\n    MatrixXd Jq   = jacobian(f, wrt(q), at(x, p, q), F);       // evaluate the function and the Jacobian matrix Jq = dF/dq\n    MatrixXd Jqpx = jacobian(f, wrt(q, p, x), at(x, p, q), F); // evaluate the function and the Jacobian matrix Jqpx = [dF/dq, dF/dp, dF/dx]\n\n    std::cout &lt;&lt; \"F = \\n\" &lt;&lt; F &lt;&lt; std::endl;     // print the evaluated output vector F\n    std::cout &lt;&lt; \"Jx = \\n\" &lt;&lt; Jx &lt;&lt; std::endl;   // print the evaluated Jacobian matrix dF/dx\n    std::cout &lt;&lt; \"Jp = \\n\" &lt;&lt; Jp &lt;&lt; std::endl;   // print the evaluated Jacobian matrix dF/dp\n    std::cout &lt;&lt; \"Jq = \\n\" &lt;&lt; Jq &lt;&lt; std::endl;   // print the evaluated Jacobian matrix dF/dq\n    std::cout &lt;&lt; \"Jqpx = \\n\" &lt;&lt; Jqpx &lt;&lt; std::endl; // print the evaluated Jacobian matrix [dF/dq, dF/dp, dF/dx]\n}\n</code></pre>"},{"location":"tutorials/#jacobian-matrix-of-a-vector-function-using-memory-maps","title":"Jacobian matrix of a vector function using memory maps","text":"<pre><code>// C++ includes\n#include &lt;iostream&gt;\n\n// autodiff include\n#include &lt;autodiff/forward/real.hpp&gt;\n#include &lt;autodiff/forward/real/eigen.hpp&gt;\nusing namespace autodiff;\n\n// The vector function for which the Jacobian is needed\nVectorXreal f(const VectorXreal&amp; x)\n{\n    return x * x.sum();\n}\n\nint main()\n{\n    using Eigen::Map;\n    using Eigen::MatrixXd;\n\n    VectorXreal x(5);                           // the input vector x with 5 variables\n    x &lt;&lt; 1, 2, 3, 4, 5;                         // x = [1, 2, 3, 4, 5]\n    double y[25];                               // the output Jacobian as a flat array\n\n    VectorXreal F;                              // the output vector F = f(x) evaluated together with Jacobian matrix below\n    Map&lt;MatrixXd&gt; J(y, 5, 5);                   // the output Jacobian dF/dx mapped onto the flat array\n\n    jacobian(f, wrt(x), at(x), F, J);           // evaluate the output vector F and the Jacobian matrix dF/dx\n\n    std::cout &lt;&lt; \"F = \\n\" &lt;&lt; F &lt;&lt; std::endl;    // print the evaluated output vector F\n    std::cout &lt;&lt; \"J = \\n\" &lt;&lt; J &lt;&lt; std::endl;    // print the evaluated Jacobian matrix dF/dx\n    std::cout &lt;&lt; \"y = \";                        // print the flat array\n    for(int i = 0 ; i &lt; 25 ; ++i)\n        std::cout &lt;&lt; y[i] &lt;&lt; \" \";\n    std::cout &lt;&lt; std::endl;\n}\n</code></pre>"},{"location":"tutorials/#higher-order-cross-derivatives-of-a-scalar-function","title":"Higher-order cross derivatives of a scalar function","text":"<pre><code>// C++ includes\n#include &lt;iostream&gt;\n\n// autodiff include\n#include &lt;autodiff/forward/dual.hpp&gt;\nusing namespace autodiff;\n\n// The multi-variable function for which higher-order derivatives are needed (up to 4th order)\ndual4th f(dual4th x, dual4th y, dual4th z)\n{\n    return 1 + x + y + z + x*y + y*z + x*z + x*y*z + exp(x/y + y/z);\n}\n\nint main()\n{\n    dual4th x = 1.0;\n    dual4th y = 2.0;\n    dual4th z = 3.0;\n\n    auto [u0, ux, uxy, uxyx, uxyxz] = derivatives(f, wrt(x, y, x, z), at(x, y, z));\n\n    std::cout &lt;&lt; \"u0 = \" &lt;&lt; u0 &lt;&lt; std::endl;       // print the evaluated value of u = f(x, y, z)\n    std::cout &lt;&lt; \"ux = \" &lt;&lt; ux &lt;&lt; std::endl;       // print the evaluated derivative du/dx\n    std::cout &lt;&lt; \"uxy = \" &lt;&lt; uxy &lt;&lt; std::endl;     // print the evaluated derivative d\u00b2u/dxdy\n    std::cout &lt;&lt; \"uxyx = \" &lt;&lt; uxyx &lt;&lt; std::endl;   // print the evaluated derivative d\u00b3u/dx\u00b2dy\n    std::cout &lt;&lt; \"uxyxz = \" &lt;&lt; uxyxz &lt;&lt; std::endl; // print the evaluated derivative d\u2074u/dx\u00b2dydz\n}\n\n/*-------------------------------------------------------------------------------------------------\n=== Note ===\n---------------------------------------------------------------------------------------------------\nIn most cases, dual can be replaced by real, as commented in other examples.\nHowever, computing higher-order cross derivatives has definitely to be done\nusing higher-order dual types (e.g., dual3rd, dual4th)! This is because real\ntypes (e.g., real2nd, real3rd, real4th) are optimally designed for computing\nhigher-order directional derivatives.\n-------------------------------------------------------------------------------------------------*/\n</code></pre>"},{"location":"tutorials/#higher-order-directional-derivatives-of-a-scalar-function","title":"Higher-order directional derivatives of a scalar function","text":"<pre><code>// C++ includes\n#include &lt;iostream&gt;\n\n// autodiff include\n#include &lt;autodiff/forward/real.hpp&gt;\nusing namespace autodiff;\n\n// The multi-variable function for which higher-order derivatives are needed (up to 4th order)\nreal4th f(real4th x, real4th y, real4th z)\n{\n    return sin(x) * cos(y) * exp(z);\n}\n\nint main()\n{\n    real4th x = 1.0;\n    real4th y = 2.0;\n    real4th z = 3.0;\n\n    auto dfdv = derivatives(f, along(1.0, 1.0, 2.0), at(x, y, z)); // the directional derivatives of f along direction v = (1, 1, 2) at (x, y, z) = (1, 2, 3)\n\n    std::cout &lt;&lt; \"dfdv[0] = \" &lt;&lt; dfdv[0] &lt;&lt; std::endl; // print the evaluated 0th order directional derivative of f along v (equivalent to f(x, y, z))\n    std::cout &lt;&lt; \"dfdv[1] = \" &lt;&lt; dfdv[1] &lt;&lt; std::endl; // print the evaluated 1st order directional derivative of f along v\n    std::cout &lt;&lt; \"dfdv[2] = \" &lt;&lt; dfdv[2] &lt;&lt; std::endl; // print the evaluated 2nd order directional derivative of f along v\n    std::cout &lt;&lt; \"dfdv[3] = \" &lt;&lt; dfdv[3] &lt;&lt; std::endl; // print the evaluated 3rd order directional derivative of f along v\n    std::cout &lt;&lt; \"dfdv[4] = \" &lt;&lt; dfdv[4] &lt;&lt; std::endl; // print the evaluated 4th order directional derivative of f along v\n}\n\n/*-------------------------------------------------------------------------------------------------\n=== Note ===\n---------------------------------------------------------------------------------------------------\nThis example would also work if dual was used instead of real. However, real\ntypes are your best option for directional derivatives, as they were optimally\ndesigned for this kind of derivatives.\n-------------------------------------------------------------------------------------------------*/\n</code></pre>"},{"location":"tutorials/#higher-order-directional-derivatives-of-a-vector-function","title":"Higher-order directional derivatives of a vector function","text":"<pre><code>// C++ includes\n#include &lt;iostream&gt;\n\n// autodiff include\n#include &lt;autodiff/forward/real.hpp&gt;\n#include &lt;autodiff/forward/real/eigen.hpp&gt;\nusing namespace autodiff;\n\n// The vector function for which higher-order directional derivatives are needed (up to 4th order).\nArrayXreal4th f(const ArrayXreal4th&amp; x, real4th p)\n{\n    return p * x.log();\n}\n\nint main()\n{\n    using Eigen::ArrayXd;\n\n    ArrayXreal4th x(5); // the input vector x\n    x &lt;&lt; 1.0, 2.0, 3.0, 4.0, 5.0;\n\n    real4th p = 7.0; // the input parameter p = 1\n\n    ArrayXd vx(5); // the direction vx in the direction vector v = (vx, vp)\n    vx &lt;&lt; 1.0, 1.0, 1.0, 1.0, 1.0;\n\n    double vp = 1.0; // the direction vp in the direction vector v = (vx, vp)\n\n    auto dfdv = derivatives(f, along(vx, vp), at(x, p)); // the directional derivatives of f along direction v = (vx, vp) at (x, p)\n\n    std::cout &lt;&lt; std::scientific &lt;&lt; std::showpos;\n    std::cout &lt;&lt; \"Directional derivatives of f along v = (vx, vp) up to 4th order:\" &lt;&lt; std::endl;\n    std::cout &lt;&lt; \"dfdv[0] = \" &lt;&lt; dfdv[0].transpose() &lt;&lt; std::endl; // print the evaluated 0th order directional derivative of f along v (equivalent to f(x, p))\n    std::cout &lt;&lt; \"dfdv[1] = \" &lt;&lt; dfdv[1].transpose() &lt;&lt; std::endl; // print the evaluated 1st order directional derivative of f along v\n    std::cout &lt;&lt; \"dfdv[2] = \" &lt;&lt; dfdv[2].transpose() &lt;&lt; std::endl; // print the evaluated 2nd order directional derivative of f along v\n    std::cout &lt;&lt; \"dfdv[3] = \" &lt;&lt; dfdv[3].transpose() &lt;&lt; std::endl; // print the evaluated 3rd order directional derivative of f along v\n    std::cout &lt;&lt; \"dfdv[4] = \" &lt;&lt; dfdv[4].transpose() &lt;&lt; std::endl; // print the evaluated 4th order directional derivative of f along v\n    std::cout &lt;&lt; std::endl;\n\n    double t = 0.1; // the step length along direction v = (vx, vp) used to compute 4th order Taylor estimate of f\n\n    ArrayXreal4th u = f(x + t * vx, p + t * vp); // start from (x, p), walk a step length t = 0.1 along direction v = (vx, vp) and evaluate f at this current point\n\n    ArrayXd utaylor = dfdv[0] + t*dfdv[1] + (t*t/2.0)*dfdv[2] + (t*t*t/6.0)*dfdv[3] + (t*t*t*t/24.0)*dfdv[4]; // evaluate the 4th order Taylor estimate of f along direction v = (vx, vp) at a step length of t = 0.1\n\n    std::cout &lt;&lt; \"Comparison between exact evaluation and 4th order Taylor estimate:\" &lt;&lt; std::endl;\n    std::cout &lt;&lt; \"u(exact)  = \" &lt;&lt; u.transpose() &lt;&lt; std::endl;\n    std::cout &lt;&lt; \"u(taylor) = \" &lt;&lt; utaylor.transpose() &lt;&lt; std::endl;\n}\n\n/*-------------------------------------------------------------------------------------------------\n=== Output ===\n---------------------------------------------------------------------------------------------------\nDirectional derivatives of f along v = (vx, vp) up to 4th order:\ndfdv[0] = +0.000000e+00 +4.852030e+00 +7.690286e+00 +9.704061e+00 +1.126607e+01\ndfdv[1] = +7.000000e+00 +4.193147e+00 +3.431946e+00 +3.136294e+00 +3.009438e+00\ndfdv[2] = -5.000000e+00 -7.500000e-01 -1.111111e-01 +6.250000e-02 +1.200000e-01\ndfdv[3] = +1.100000e+01 +1.000000e+00 +1.851852e-01 +3.125000e-02 -8.000000e-03\ndfdv[4] = -3.400000e+01 -1.625000e+00 -2.222222e-01 -3.906250e-02 -3.200000e-03\n\nComparison between exact evaluation and 4th order Taylor estimate:\nu(exact)  = +6.767023e-01 +5.267755e+00 +8.032955e+00 +1.001801e+01 +1.156761e+01\nu(taylor) = +6.766917e-01 +5.267755e+00 +8.032955e+00 +1.001801e+01 +1.156761e+01\n-------------------------------------------------------------------------------------------------*/\n\n/*-------------------------------------------------------------------------------------------------\n=== Note ===\n---------------------------------------------------------------------------------------------------\nThis example would also work if dual was used instead of real. However, real\ntypes are your best option for directional derivatives, as they were optimally\ndesigned for this kind of derivatives.\n-------------------------------------------------------------------------------------------------*/\n</code></pre>"},{"location":"tutorials/#taylor-series-of-a-scalar-function-along-a-direction","title":"Taylor series of a scalar function along a direction","text":"<pre><code>// C++ includes\n#include &lt;iostream&gt;\n\n// autodiff include\n#include &lt;autodiff/forward/real.hpp&gt;\n#include &lt;autodiff/forward/real/eigen.hpp&gt;\nusing namespace autodiff;\n\n// The scalar function for which a 4th order directional Taylor series will be computed.\nreal4th f(const real4th&amp; x, const real4th&amp; y, const real4th&amp; z)\n{\n    return sin(x * y) * cos(x * z) * exp(z);\n}\n\nint main()\n{\n    real4th x = 1.0;                                       // the input vector x\n    real4th y = 2.0;                                       // the input vector y\n    real4th z = 3.0;                                       // the input vector z\n\n    auto g = taylorseries(f, along(1, 1, 2), at(x, y, z)); // the function g(t) as a 4th order Taylor approximation of f(x + t, y + t, z + 2t)\n\n    double t = 0.1;                                        // the step length used to evaluate g(t), the Taylor approximation of f(x + t, y + t, z + 2t)\n\n    real4th u = f(x + t, y + t, z + 2*t);                  // the exact value of f(x + t, y + t, z + 2t)\n\n    double utaylor = g(t);                                 // the 4th order Taylor estimate of f(x + t, y + t, z + 2t)\n\n    std::cout &lt;&lt; std::fixed;\n    std::cout &lt;&lt; \"Comparison between exact evaluation and 4th order Taylor estimate of f(x + t, y + t, z + 2t):\" &lt;&lt; std::endl;\n    std::cout &lt;&lt; \"u(exact)  = \" &lt;&lt; u &lt;&lt; std::endl;\n    std::cout &lt;&lt; \"u(taylor) = \" &lt;&lt; utaylor &lt;&lt; std::endl;\n}\n\n/*-------------------------------------------------------------------------------------------------\n=== Output ===\n---------------------------------------------------------------------------------------------------\nComparison between exact evaluation and 4th order Taylor estimate of f(x + t, y + t, z + 2t):\nu(exact)  = -16.847071\nu(taylor) = -16.793986\n-------------------------------------------------------------------------------------------------*/\n</code></pre>"},{"location":"tutorials/#taylor-series-of-a-vector-function-along-a-direction","title":"Taylor series of a vector function along a direction","text":"<pre><code>// C++ includes\n#include &lt;iostream&gt;\n\n// autodiff include\n#include &lt;autodiff/forward/real.hpp&gt;\n#include &lt;autodiff/forward/real/eigen.hpp&gt;\nusing namespace autodiff;\n\n// The vector function for which a 4th order directional Taylor series will be computed.\nArrayXreal4th f(const ArrayXreal4th&amp; x)\n{\n    return x.sin() / x;\n}\n\nint main()\n{\n    using Eigen::ArrayXd;\n\n    ArrayXreal4th x(5);                        // the input vector x\n    x &lt;&lt; 1.0, 2.0, 3.0, 4.0, 5.0;\n\n    ArrayXd v(5);                              // the direction vector v\n    v &lt;&lt; 1.0, 1.0, 1.0, 1.0, 1.0;\n\n    auto g = taylorseries(f, along(v), at(x)); // the function g(t) as a 4th order Taylor approximation of f(x + t\u00b7v)\n\n    double t = 0.1;                            // the step length used to evaluate g(t), the Taylor approximation of f(x + t\u00b7v)\n\n    ArrayXreal4th u = f(x + t * v);            // the exact value of f(x + t\u00b7v)\n\n    ArrayXd utaylor = g(t);                    // the 4th order Taylor estimate of f(x + t\u00b7v)\n\n    std::cout &lt;&lt; std::fixed;\n    std::cout &lt;&lt; \"Comparison between exact evaluation and 4th order Taylor estimate of f(x + t\u00b7v):\" &lt;&lt; std::endl;\n    std::cout &lt;&lt; \"u(exact)  = \" &lt;&lt; u.transpose() &lt;&lt; std::endl;\n    std::cout &lt;&lt; \"u(taylor) = \" &lt;&lt; utaylor.transpose() &lt;&lt; std::endl;\n}\n\n/*-------------------------------------------------------------------------------------------------\n=== Output ===\n---------------------------------------------------------------------------------------------------\nComparison between exact evaluation and 4th order Taylor estimate of f(x + t\u00b7v):\nu(exact)  =  0.810189  0.411052  0.013413 -0.199580 -0.181532\nu(taylor) =  0.810189  0.411052  0.013413 -0.199580 -0.181532\n-------------------------------------------------------------------------------------------------*/\n</code></pre>"},{"location":"tutorials/#reverse-mode","title":"Reverse mode","text":""},{"location":"tutorials/#single-variable-function","title":"Single-variable function","text":"<pre><code>// C++ includes\n#include &lt;iostream&gt;\nusing namespace std;\n\n// autodiff include\n#include &lt;autodiff/reverse/var.hpp&gt;\nusing namespace autodiff;\n\n// The single-variable function for which derivatives are needed\nvar f(var x)\n{\n    return 1 + x + x*x + 1/x + log(x);\n}\n\nint main()\n{\n    var x = 2.0;   // the input variable x\n    var u = f(x);  // the output variable u\n\n    auto [ux] = derivatives(u, wrt(x)); // evaluate the derivative of u with respect to x\n\n    cout &lt;&lt; \"u = \" &lt;&lt; u &lt;&lt; endl;  // print the evaluated output variable u\n    cout &lt;&lt; \"ux = \" &lt;&lt; ux &lt;&lt; endl;  // print the evaluated derivative ux\n}\n</code></pre>"},{"location":"tutorials/#multi-variable-function","title":"Multi-variable function","text":"<pre><code>// C++ includes\n#include &lt;iostream&gt;\nusing namespace std;\n\n// autodiff include\n#include &lt;autodiff/reverse/var.hpp&gt;\nusing namespace autodiff;\n\n// The multi-variable function for which derivatives are needed\nvar f(var x, var y, var z)\n{\n    return 1 + x + y + z + x*y + y*z + x*z + x*y*z + exp(x/y + y/z);\n}\n\nint main()\n{\n    var x = 1.0;         // the input variable x\n    var y = 2.0;         // the input variable y\n    var z = 3.0;         // the input variable z\n    var u = f(x, y, z);  // the output variable u\n\n    auto [ux, uy, uz] = derivatives(u, wrt(x, y, z)); // evaluate the derivatives of u with respect to x, y, z\n\n    cout &lt;&lt; \"u = \" &lt;&lt; u &lt;&lt; endl;    // print the evaluated output u\n    cout &lt;&lt; \"ux = \" &lt;&lt; ux &lt;&lt; endl;  // print the evaluated derivative ux\n    cout &lt;&lt; \"uy = \" &lt;&lt; uy &lt;&lt; endl;  // print the evaluated derivative uy\n    cout &lt;&lt; \"uz = \" &lt;&lt; uz &lt;&lt; endl;  // print the evaluated derivative uz\n}\n</code></pre>"},{"location":"tutorials/#multi-variable-function-with-conditional","title":"Multi-variable function with conditional","text":"<pre><code>// C++ includes\n#include &lt;iostream&gt;\nusing namespace std;\n\n// autodiff include\n#include &lt;autodiff/reverse/var.hpp&gt;\nusing namespace autodiff;\n\n// A two-variable piecewise function for which derivatives are needed\nvar f(var x, var y) { return condition(x &lt; y, x * y, x * x); }\n\nint main()\n{\n    var x = 1.0;   // the input variable x\n    var y = 2.0;   // the input variable y\n    var u = f(x, y);  // the output variable u\n    auto [ux, uy] = derivatives(u, wrt(x, y)); // evaluate the derivatives of u\n\n    cout &lt;&lt; \"x = \" &lt;&lt; x &lt;&lt; \", y = \" &lt;&lt; y &lt;&lt; endl;\n    cout &lt;&lt; \"u = \" &lt;&lt; u &lt;&lt; endl;  // x = 1, y = 2, so x &lt; y, so x * y = 2\n    cout &lt;&lt; \"ux = \" &lt;&lt; ux &lt;&lt; endl;  // d/dx x * y = y = 2\n    cout &lt;&lt; \"uy = \" &lt;&lt; uy &lt;&lt; endl;  // d/dy x * y = x = 1\n\n    x.update(3.0); // Change the value of x in the expression tree\n    u.update(); // Update the expression tree in a sweep\n    auto [ux2, uy2] = derivatives(u, wrt(x, y)); // re-evaluate the derivatives\n\n    cout &lt;&lt; \"x = \" &lt;&lt; x &lt;&lt; \", y = \" &lt;&lt; y &lt;&lt; endl;\n    cout &lt;&lt; \"u = \" &lt;&lt; u &lt;&lt; endl;  // Now x &gt; y, so x * x = 9\n    cout &lt;&lt; \"ux = \" &lt;&lt; ux2 &lt;&lt; endl;  // d/dx x * x = 2x = 6\n    cout &lt;&lt; \"uy = \" &lt;&lt; uy2 &lt;&lt; endl;  // d/dy x * x = 0\n\n    // condition-associated functions\n    cout &lt;&lt; \"min(x, y) = \" &lt;&lt; min(x, y) &lt;&lt; endl;\n    cout &lt;&lt; \"max(x, y) = \" &lt;&lt; max(x, y) &lt;&lt; endl;\n    cout &lt;&lt; \"sgn(x) = \" &lt;&lt; sgn(x) &lt;&lt; endl;\n}\n</code></pre>"},{"location":"tutorials/#multi-variable-function-with-parameters","title":"Multi-variable function with parameters","text":"<pre><code>// C++ includes\n#include &lt;iostream&gt;\nusing namespace std;\n\n// autodiff include\n#include &lt;autodiff/reverse/var.hpp&gt;\nusing namespace autodiff;\n\n// A type defining parameters for a function of interest\nstruct Params\n{\n    var a;\n    var b;\n    var c;\n};\n\n// The function that depends on parameters for which derivatives are needed\nvar f(var x, const Params&amp; params)\n{\n    return params.a * sin(x) + params.b * cos(x) + params.c * sin(x)*cos(x);\n}\n\nint main()\n{\n    Params params;   // initialize the parameter variables\n    params.a = 1.0;  // the parameter a of type var, not double!\n    params.b = 2.0;  // the parameter b of type var, not double!\n    params.c = 3.0;  // the parameter c of type var, not double!\n\n    var x = 0.5;  // the input variable x\n    var u = f(x, params);  // the output variable u\n\n    auto [ux, ua, ub, uc] = derivatives(u, wrt(x, params.a, params.b, params.c)); // evaluate the derivatives of u with respect to x and parameters a, b, c\n\n    cout &lt;&lt; \"u = \" &lt;&lt; u &lt;&lt; endl;    // print the evaluated output u\n    cout &lt;&lt; \"ux = \" &lt;&lt; ux &lt;&lt; endl;  // print the evaluated derivative du/dx\n    cout &lt;&lt; \"ua = \" &lt;&lt; ua &lt;&lt; endl;  // print the evaluated derivative du/da\n    cout &lt;&lt; \"ub = \" &lt;&lt; ub &lt;&lt; endl;  // print the evaluated derivative du/db\n    cout &lt;&lt; \"uc = \" &lt;&lt; uc &lt;&lt; endl;  // print the evaluated derivative du/dc\n}\n</code></pre>"},{"location":"tutorials/#gradient-of-a-scalar-function","title":"Gradient of a scalar function","text":"<pre><code>// C++ includes\n#include &lt;iostream&gt;\n\n// autodiff include\n#include &lt;autodiff/reverse/var.hpp&gt;\n#include &lt;autodiff/reverse/var/eigen.hpp&gt;\nusing namespace autodiff;\n\n// The scalar function for which the gradient is needed\nvar f(const ArrayXvar&amp; x)\n{\n    return sqrt((x * x).sum()); // sqrt(sum([xi * xi for i = 1:5]))\n}\n\nint main()\n{\n    using Eigen::VectorXd;\n\n    VectorXvar x(5);                       // the input vector x with 5 variables\n    x &lt;&lt; 1, 2, 3, 4, 5;                    // x = [1, 2, 3, 4, 5]\n\n    var y = f(x);                          // the output variable y\n\n    VectorXd dydx = gradient(y, x);        // evaluate the gradient vector dy/dx\n\n    std::cout &lt;&lt; \"y = \" &lt;&lt; y &lt;&lt; std::endl;           // print the evaluated output y\n    std::cout &lt;&lt; \"dy/dx = \\n\" &lt;&lt; dydx &lt;&lt; std::endl;  // print the evaluated gradient vector dy/dx\n}\n</code></pre>"},{"location":"tutorials/#hessian-of-a-scalar-function","title":"Hessian of a scalar function","text":"<pre><code>// C++ includes\n#include &lt;iostream&gt;\nusing namespace std;\n\n// autodiff include\n#include &lt;autodiff/reverse/var.hpp&gt;\n#include &lt;autodiff/reverse/var/eigen.hpp&gt;\nusing namespace autodiff;\n\n// The scalar function for which the gradient is needed\nvar f(const ArrayXvar&amp; x)\n{\n    return sqrt((x * x).sum()); // sqrt(sum([xi * xi for i = 1:5]))\n}\n\nint main()\n{\n    VectorXvar x(5);     // the input vector x with 5 variables\n    x &lt;&lt; 1, 2, 3, 4, 5;  // x = [1, 2, 3, 4, 5]\n\n    var u = f(x);  // the output variable u\n\n    Eigen::VectorXd g;  // the gradient vector to be computed in method `hessian`\n    Eigen::MatrixXd H = hessian(u, x, g);  // evaluate the Hessian matrix H and the gradient vector g of u\n\n    cout &lt;&lt; \"u = \" &lt;&lt; u &lt;&lt; endl;    // print the evaluated output variable u\n    cout &lt;&lt; \"g = \\n\" &lt;&lt; g &lt;&lt; endl;  // print the evaluated gradient vector of u\n    cout &lt;&lt; \"H = \\n\" &lt;&lt; H &lt;&lt; endl;  // print the evaluated Hessian matrix of u\n}\n</code></pre>"},{"location":"tutorials/#higher-order-derivatives-of-a-single-variable-function","title":"Higher-order derivatives of a single-variable function","text":"<pre><code>// C++ includes\n#include &lt;iostream&gt;\nusing namespace std;\n\n// autodiff include\n#include &lt;autodiff/reverse/var.hpp&gt;\nusing namespace autodiff;\n\nint main()\n{\n    var x = 0.5;  // the input variable x\n    var u = sin(x) * cos(x);  // the output variable u\n\n    auto [ux] = derivativesx(u, wrt(x));  // evaluate the first order derivatives of u\n    auto [uxx] = derivativesx(ux, wrt(x));  // evaluate the second order derivatives of ux\n\n    cout &lt;&lt; \"u = \" &lt;&lt; u &lt;&lt; endl;  // print the evaluated output variable u\n    cout &lt;&lt; \"ux(autodiff) = \" &lt;&lt; ux &lt;&lt; endl;  // print the evaluated first order derivative ux\n    cout &lt;&lt; \"ux(exact) = \" &lt;&lt; 1 - 2*sin(x)*sin(x) &lt;&lt; endl;  // print the exact first order derivative ux\n    cout &lt;&lt; \"uxx(autodiff) = \" &lt;&lt; uxx &lt;&lt; endl;  // print the evaluated second order derivative uxx\n    cout &lt;&lt; \"uxx(exact) = \" &lt;&lt; -4*cos(x)*sin(x) &lt;&lt; endl;  // print the exact second order derivative uxx\n}\n\n/*===============================================================================\nOutput:\n=================================================================================\nu = 0.420735\nux(autodiff) = 0.540302\nux(exact) = 0.540302\nuxx(autodiff) = -1.68294\nuxx(exact) = -1.68294\n===============================================================================*/\n</code></pre>"},{"location":"tutorials/#higher-order-derivatives-of-a-multi-variable-function","title":"Higher-order derivatives of a multi-variable function","text":"<pre><code>// C++ includes\n#include &lt;iostream&gt;\nusing namespace std;\n\n// autodiff include\n#include &lt;autodiff/reverse/var.hpp&gt;\nusing namespace autodiff;\n\nint main()\n{\n    var x = 1.0;  // the input variable x\n    var y = 0.5;  // the input variable y\n    var z = 2.0;  // the input variable z\n\n    var u = x * log(y) * exp(z);  // the output variable u\n\n    auto [ux, uy, uz] = derivativesx(u, wrt(x, y, z));  // evaluate the derivatives of u with respect to x, y, z.\n\n    auto [uxx, uxy, uxz] = derivativesx(ux, wrt(x, y, z)); // evaluate the derivatives of ux with respect to x, y, z.\n    auto [uyx, uyy, uyz] = derivativesx(uy, wrt(x, y, z)); // evaluate the derivatives of uy with respect to x, y, z.\n    auto [uzx, uzy, uzz] = derivativesx(uz, wrt(x, y, z)); // evaluate the derivatives of uz with respect to x, y, z.\n\n    cout &lt;&lt; \"u = \" &lt;&lt; u &lt;&lt; endl;  // print the evaluated output variable u\n\n    cout &lt;&lt; \"ux = \" &lt;&lt; ux &lt;&lt; endl;  // print the evaluated first order derivative ux\n    cout &lt;&lt; \"uy = \" &lt;&lt; uy &lt;&lt; endl;  // print the evaluated first order derivative uy\n    cout &lt;&lt; \"uz = \" &lt;&lt; uz &lt;&lt; endl;  // print the evaluated first order derivative uz\n\n    cout &lt;&lt; \"uxx = \" &lt;&lt; uxx &lt;&lt; endl;  // print the evaluated second order derivative uxx\n    cout &lt;&lt; \"uxy = \" &lt;&lt; uxy &lt;&lt; endl;  // print the evaluated second order derivative uxy\n    cout &lt;&lt; \"uxz = \" &lt;&lt; uxz &lt;&lt; endl;  // print the evaluated second order derivative uxz\n\n    cout &lt;&lt; \"uyx = \" &lt;&lt; uyx &lt;&lt; endl;  // print the evaluated second order derivative uyx\n    cout &lt;&lt; \"uyy = \" &lt;&lt; uyy &lt;&lt; endl;  // print the evaluated second order derivative uyy\n    cout &lt;&lt; \"uyz = \" &lt;&lt; uyz &lt;&lt; endl;  // print the evaluated second order derivative uyz\n\n    cout &lt;&lt; \"uzx = \" &lt;&lt; uzx &lt;&lt; endl;  // print the evaluated second order derivative uzx\n    cout &lt;&lt; \"uzy = \" &lt;&lt; uzy &lt;&lt; endl;  // print the evaluated second order derivative uzy\n    cout &lt;&lt; \"uzz = \" &lt;&lt; uzz &lt;&lt; endl;  // print the evaluated second order derivative uzz\n}\n</code></pre>"},{"location":"tutorials/#integration-with-cmake-based-projects","title":"Integration with CMake-based projects","text":"<p>Integrating autodiff in a CMake-based project is very simple as shown next.</p> <p>Let's assume our CMake-based project consists of two files: <code>main.cpp</code> and <code>CMakeLists.txt</code>, whose contents are shown below:</p> <p>main.cpp <pre><code>#include &lt;iostream&gt;\n\n#include &lt;autodiff/forward/dual.hpp&gt;\nusing namespace autodiff;\n\ndual f(dual x)\n{\n    return 1 + x + x*x + 1/x + log(x);\n}\n\nint main()\n{\n    dual x = 1.0;\n    dual u = f(x);\n    double dudx = derivative(f, wrt(x), at(x));\n\n    std::cout &lt;&lt; \"u = \" &lt;&lt; u &lt;&lt; std::endl;\n    std::cout &lt;&lt; \"du/dx = \" &lt;&lt; dudx &lt;&lt; std::endl;\n}\n</code></pre></p> <p>CMakeLists.txt</p> <pre><code>cmake_minimum_required(VERSION 3.16)\n\nproject(app)\n\nfind_package(autodiff)\n\nadd_executable(app main.cpp)\n\ntarget_link_libraries(app autodiff::autodiff)\n</code></pre> <p>In the <code>CMakeLists.txt</code> file, note the use of the command:</p> <pre><code>find_package(autodiff)\n</code></pre> <p>to find the header files of the autodiff library, and the command:</p> <p><pre><code>target_link_libraries(app autodiff::autodiff)\n</code></pre> to link the executable target <code>app</code> against the autodiff library (<code>autodiff::autodiff</code>) using CMake's modern target-based design.</p> <p>To build the application, do:</p> <pre><code>mkdir build &amp;&amp; cd build\ncmake .. -DCMAKE_PREFIX_PATH=/path/to/autodiff/install/dir\nmake\n</code></pre> <p>Attention</p> <p>If autodiff has been installed system-wide, then the CMake argument <code>CMAKE_PREFIX_PATH</code> should not be needed. Otherwise, you will need to specify where autodiff is installed in your machine. For example:</p> <pre><code>cmake .. -DCMAKE_PREFIX_PATH=$HOME/local\n</code></pre> <p>assuming directory <code>$HOME/local</code> is where autodiff was installed to, which should then contain the following directory:</p> <pre><code>$HOME/local/include/autodiff/\n</code></pre> <p>where the autodiff header files are located.</p> <p>To execute the application, do:</p> <pre><code>./app\n</code></pre>"}]}